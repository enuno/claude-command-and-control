{
  "title": "Configure the part size to be 10MB. 5MB is the minimum part size, except for the last part",
  "content": "partsize = 10 * 1024 * 1024\n\ndef upload_file(worker_endpoint, filename, partsize):\n    url = f\"{worker_endpoint}{filename}\"\n\n# Create the multipart upload\n    uploadId = requests.post(url, params={\"action\": \"mpu-create\"}).json()[\"uploadId\"]\n\npart_count = math.ceil(os.stat(filename).st_size / partsize)\n    # Create an executor for up to 25 concurrent uploads.\n    executor = concurrent.futures.ThreadPoolExecutor(25)\n    # Submit a task to the executor to upload each part\n    futures = [\n        executor.submit(upload_part, filename, partsize, url, uploadId, index)\n        for index in range(part_count)\n    ]\n    concurrent.futures.wait(futures)\n    # get the parts from the futures\n    uploaded_parts = [future.result() for future in futures]\n\n# complete the multipart upload\n    response = requests.post(\n        url,\n        params={\"action\": \"mpu-complete\", \"uploadId\": uploadId},\n        json={\"parts\": uploaded_parts},\n    )\n    if response.status_code == 200:\n        print(\"ðŸŽ‰ successfully completed multipart upload\")\n    else:\n        print(response.text)\n\ndef upload_part(filename, partsize, url, uploadId, index):\n    # Open the file in rb mode, which treats it as raw bytes rather than attempting to parse utf-8\n    with open(filename, \"rb\") as file:\n        file.seek(partsize * index)\n        part = file.read(partsize)\n\n# Retry policy for when uploading a part fails\n    s = requests.Session()\n    retries = Retry(total=3, status_forcelist=[400, 500, 502, 503, 504])\n    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n\nreturn s.put(\n        url,\n        params={\n            \"action\": \"mpu-uploadpart\",\n            \"uploadId\": uploadId,\n            \"partNumber\": str(index + 1),\n        },\n        data=part,\n    ).json()\n\nupload_file(worker_endpoint, filename, partsize)\npy\nimport pyarrow as pa\nfrom pyiceberg.catalog.rest import RestCatalog\nfrom pyiceberg.exceptions import NamespaceAlreadyExistsError",
  "code_samples": [
    {
      "code": "## State management\n\nThe stateful nature of multipart uploads does not easily map to the usage model of Workers, which are inherently stateless. In a normal multipart upload, the multipart upload is usually performed in one continuous execution of the client application. This is different from multipart uploads in a Worker, which will often be completed over multiple invocations of that Worker. This makes state management more challenging.\n\nTo overcome this, the state associated with a multipart upload, namely the `uploadId` and which parts have been uploaded, needs to be kept track of somewhere outside of the Worker.\n\nIn the example Worker and Python application described in this guide, the state of the multipart upload is tracked in the client application which sends requests to the Worker, with the necessary state contained in each request. Keeping track of the multipart state in the client application enables maximal flexibility and allows for parallel and unordered uploads of each part.\n\nWhen keeping track of this state in the client is impossible, alternative designs can be considered. For example, you could track the `uploadId` and which parts have been uploaded in a Durable Object or other database.\n\n</page>\n\n<page>\n---\ntitle: PyIceberg Â· Cloudflare R2 docs\ndescription: Below is an example of using PyIceberg to connect to R2 Data Catalog.\nlastUpdated: 2025-04-09T22:46:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-catalog/config-examples/pyiceberg/\n  md: https://developers.cloudflare.com/r2/data-catalog/config-examples/pyiceberg/index.md\n---\n\nBelow is an example of using [PyIceberg](https://py.iceberg.apache.org/) to connect to R2 Data Catalog.\n\n## Prerequisites\n\n* Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages).\n* [Create an R2 bucket](https://developers.cloudflare.com/r2/buckets/create-buckets/) and [enable the data catalog](https://developers.cloudflare.com/r2/data-catalog/manage-catalogs/#enable-r2-data-catalog-on-a-bucket).\n* [Create an R2 API token](https://developers.cloudflare.com/r2/api/tokens/) with both [R2 and data catalog permissions](https://developers.cloudflare.com/r2/api/tokens/#permissions).\n* Install the [PyIceberg](https://py.iceberg.apache.org/#installation) and [PyArrow](https://arrow.apache.org/docs/python/install.html) libraries.\n\n## Example usage",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "State management",
      "id": "state-management"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "Example usage",
      "id": "example-usage"
    }
  ],
  "url": "llms-txt#configure-the-part-size-to-be-10mb.-5mb-is-the-minimum-part-size,-except-for-the-last-part",
  "links": []
}