{
  "title": "Make sure to replace the placeholder with your shared secret",
  "content": "curl -XPOST \"https://YOUR_WORKER.YOUR_ACCOUNT.workers.dev\" --data '{\"messages\": [{\"msg\":\"hello world\"}]}'\nsh\n{\"success\":true}\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"my-worker\",\n    \"queues\": {\n      \"producers\": [\n        {\n          \"queue\": \"my-queue\",\n          \"binding\": \"ERROR_QUEUE\"\n        }\n      ],\n      \"consumers\": [\n        {\n          \"queue\": \"my-queue\",\n          \"max_batch_size\": 100,\n          \"max_batch_timeout\": 30\n        }\n      ]\n    },\n    \"r2_buckets\": [\n      {\n        \"bucket_name\": \"my-bucket\",\n        \"binding\": \"ERROR_BUCKET\"\n      }\n    ]\n  }\n  toml\n  name = \"my-worker\"\n\n[[queues.producers]]\n    queue = \"my-queue\"\n    binding = \"ERROR_QUEUE\"\n\n[[queues.consumers]]\n    queue = \"my-queue\"\n    max_batch_size = 100\n    max_batch_timeout = 30\n\n[[r2_buckets]]\n    bucket_name = \"my-bucket\"\n    binding = \"ERROR_BUCKET\"\n  ts\ntype Environment = {\n  readonly ERROR_QUEUE: Queue<Error>;\n  readonly ERROR_BUCKET: R2Bucket;\n};\n\nexport default {\n  async fetch(req, env): Promise<Response> {\n    try {\n      return doRequest(req);\n    } catch (error) {\n      await env.ERROR_QUEUE.send(error);\n      return new Response(error.message, { status: 500 });\n    }\n  },\n  async queue(batch, env): Promise<void> {\n    let file = '';\n    for (const message of batch.messages) {\n      const error = message.body;\n      file += error.stack || error.message || String(error);\n      file += '\\r\\n';\n    }\n    await env.ERROR_BUCKET.put(`errors/${Date.now()}.log`, file);\n  },\n} satisfies ExportedHandler<Environment, Error>;\n\nfunction doRequest(request: Request): Promise<Response> {\n  if (Math.random() > 0.5) {\n    return new Response('Success!');\n  }\n  throw new Error('Failed!');\n}\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"my-worker\",\n    \"queues\": {\n      \"producers\": [\n        {\n          \"queue\": \"my-queue\",\n          \"binding\": \"YOUR_QUEUE\"\n        }\n      ]\n    },\n    \"durable_objects\": {\n      \"bindings\": [\n        {\n          \"name\": \"YOUR_DO_CLASS\",\n          \"class_name\": \"YourDurableObject\"\n        }\n      ]\n    },\n    \"migrations\": [\n      {\n        \"tag\": \"v1\",\n        \"new_sqlite_classes\": [\n          \"YourDurableObject\"\n        ]\n      }\n    ]\n  }\n  toml\n  name = \"my-worker\"\n\n[[queues.producers]]\n    queue = \"my-queue\"\n    binding = \"YOUR_QUEUE\"\n\n[durable_objects]\n  bindings = [\n    { name = \"YOUR_DO_CLASS\", class_name = \"YourDurableObject\" }\n  ]\n\n[[migrations]]\n  tag = \"v1\"\n  new_sqlite_classes = [\"YourDurableObject\"]\n  ts\ninterface Env {\n  YOUR_QUEUE: Queue;\n  YOUR_DO_CLASS: DurableObjectNamespace;\n}\n\nexport default {\n  async fetch(req, env): Promise<Response> {\n    // Assume each Durable Object is mapped to a userId in a query parameter\n    // In a production application, this will be a userId defined by your application\n    // that you validate (and/or authenticate) first.\n    let url = new URL(req.url)\n    let userIdParam = url.searchParams.get(\"userId\")\n\nif (userIdParam) {\n      // Get a stub that allows you to call that Durable Object\n      let durableObjectStub = env.YOUR_DO_CLASS.getByName(userIdParam);\n\n// Pass the request to that Durable Object and await the response\n      // This invokes the constructor once on your Durable Object class (defined further down)\n      // on the first initialization, and the fetch method on each request.\n      // We pass the original Request to the Durable Object's fetch method\n      let response = await durableObjectStub.fetch(req);\n\n// This would return \"wrote to queue\", but you could return any response.\n      return response;\n    }\n    return new Response(\"userId must be provided\", { status: 400 });\n  },\n} satisfies ExportedHandler<Env>;\n\nexport class YourDurableObject implements DurableObject {\n  constructor(private state: DurableObjectState, private env: Env) {}\n\nasync fetch(req: Request): Promise<Response> {\n    // Error handling elided for brevity.\n    // Publish to your queue\n    await this.env.YOUR_QUEUE.send({\n      id: this.state.id.toString() // Write the ID of the Durable Object to your queue\n      // Write any other properties to your queue\n    });\n\nreturn new Response(\"wrote to queue\")\n  }\ngraphql\nquery QueueBacklog(\n  $accountTag: string!\n  $queueId: string!\n  $datetimeStart: Time!\n  $datetimeEnd: Time!\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountTag }) {\n      queueBacklogAdaptiveGroups(\n        limit: 10000\n        filter: {\n          queueId: $queueId\n          datetime_geq: $datetimeStart\n          datetime_leq: $datetimeEnd\n        }\n      ) {\n        avg {\n          messages\n          bytes\n        }\n      }\n    }\n  }\n}\ngraphql\nquery QueueConcurrencyByHour(\n  $accountTag: string!\n  $queueId: string!\n  $datetimeStart: Time!\n  $datetimeEnd: Time!\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountTag }) {\n      queueConsumerMetricsAdaptiveGroups(\n        limit: 10000\n        filter: {\n          queueId: $queueId\n          datetime_geq: $datetimeStart\n          datetime_leq: $datetimeEnd\n        }\n        orderBy: [datetimeHour_DESC]\n      ) {\n        avg {\n          concurrency\n        }\n        dimensions {\n          datetimeHour\n        }\n      }\n    }\n  }\n}\ngraphql\nquery QueueMessageOperationsByMinute(\n  $accountTag: string!\n  $queueId: string!\n  $datetimeStart: Date!\n  $datetimeEnd: Date!\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountTag }) {\n      queueMessageOperationsAdaptiveGroups(\n        limit: 10000\n        filter: {\n          queueId: $queueId\n          datetime_geq: $datetimeStart\n          datetime_leq: $datetimeEnd\n        }\n        orderBy: [datetimeMinute_DESC]\n      ) {\n        count\n        sum {\n          bytes\n        }\n        dimensions {\n          datetimeMinute\n        }\n      }\n    }\n  }\n}\njsonc\n  {\n    // ...rest of your configuration...\n    \"limits\": {\n      \"cpu_ms\": 300000, // 300,000 milliseconds = 5 minutes\n    },\n    // ...rest of your configuration...\n  }\n  toml\n  [limits]\n  cpu_ms = 300_000\n  txt\n((Number of Messages * 3) - 1,000,000) / 1,000,000  * $0.40\nts\ntype Environment = {\n  readonly MY_FIRST_QUEUE: Queue;\n};\n\nexport default {\n  async fetch(req, env, context): Promise<Response> {\n    let message = {\n      url: req.url,\n      method: req.method,\n      headers: Object.fromEntries(req.headers),\n    };\n\nawait env.MY_FIRST_QUEUE.send(message); // This will throw an exception if the send fails for any reason\n  },\n} satisfies ExportedHandler<Environment>;\nts\ntype Environment = {\n  readonly MY_FIRST_QUEUE: Queue;\n};\n\nexport default {\n  async fetch(req, env): Promise<Response> {\n    let message = {\n      url: req.url,\n      method: req.method,\n      headers: Object.fromEntries(req.headers),\n    };\n    try {\n      await env.MY_FIRST_QUEUE.send(message, { contentType: \"json\" }); // \"json\" is the default\n    } catch (e) {\n      // Catch cases where send fails, including due to a mismatched content type\n      console.log(e)\n      return Response.json({\"msg\": e}, { status: 500 })\n    }\n  },\n} satisfies ExportedHandler<Environment>;\nts\n    try {\n      // This will throw an exception (error) if you write to pass a non-string to the queue, such as a\n      // native JavaScript object or ArrayBuffer.\n      await env.MY_FIRST_QUEUE.send(\"hello there\", { contentType: \"text\" }); // explicitly set 'text'\n    } catch (e) {\n      console.log(e)\n      return Response.json({\"msg\": e}, { status: 500 })\nts\nexport default {\n  async queue(batch: MessageBatch<Error>, env: Environment): Promise<void> {\n    // Do something with messages in the batch\n    // i.e. write to R2 storage, D1 database, or POST to an external API\n    // You can also iterate over each message in the batch by looping over batch.messages\n  },\n};\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"queues\": {\n      \"consumers\": [\n        {\n          \"queue\": \"<your-queue-name>\",\n          \"max_batch_size\": 100,\n          \"max_batch_timeout\": 30\n        }\n      ]\n    }\n  }\n  toml\n  [[queues.consumers]]\n    queue = \"<your-queue-name>\"\n    max_batch_size = 100 # optional\n    max_batch_timeout = 30 # optional\n  ts\nexport default {\n  async queue(batch: MessageBatch<Error>, env: Environment): Promise<void> {\n    // MessageBatch has a `queue` property we can switch on\n    switch (batch.queue) {\n      case 'log-queue':\n        // Write the batch to R2\n        break;\n      case 'debug-queue':\n        // Write the message to the console or to another queue\n        break;\n      case 'email-reset':\n        // Trigger a password reset email via an external API\n        break;\n      default:\n      // Handle messages we haven't mentioned explicitly (write a log, push to a DLQ)\n    }\n  },\n};\nsh\n  npx wrangler queues list\n  sh\n  pnpm wrangler queues list\n  sh\n  yarn wrangler queues list\n  sh\n  npx wrangler queues create [NAME]\n  sh\n  pnpm wrangler queues create [NAME]\n  sh\n  yarn wrangler queues create [NAME]\n  sh\n  npx wrangler queues update [NAME]\n  sh\n  pnpm wrangler queues update [NAME]\n  sh\n  yarn wrangler queues update [NAME]\n  sh\n  npx wrangler queues delete [NAME]\n  sh\n  pnpm wrangler queues delete [NAME]\n  sh\n  yarn wrangler queues delete [NAME]\n  sh\n  npx wrangler queues info [NAME]\n  sh\n  pnpm wrangler queues info [NAME]\n  sh\n  yarn wrangler queues info [NAME]\n  sh\n  npx wrangler queues consumer add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  pnpm wrangler queues consumer add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  yarn wrangler queues consumer add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  npx wrangler queues consumer remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  pnpm wrangler queues consumer remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  yarn wrangler queues consumer remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  npx wrangler queues consumer http add [QUEUE-NAME]\n  sh\n  pnpm wrangler queues consumer http add [QUEUE-NAME]\n  sh\n  yarn wrangler queues consumer http add [QUEUE-NAME]\n  sh\n  npx wrangler queues consumer http remove [QUEUE-NAME]\n  sh\n  pnpm wrangler queues consumer http remove [QUEUE-NAME]\n  sh\n  yarn wrangler queues consumer http remove [QUEUE-NAME]\n  sh\n  npx wrangler queues consumer worker add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  pnpm wrangler queues consumer worker add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  yarn wrangler queues consumer worker add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  npx wrangler queues consumer worker remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  pnpm wrangler queues consumer worker remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  yarn wrangler queues consumer worker remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  npx wrangler queues pause-delivery [NAME]\n  sh\n  pnpm wrangler queues pause-delivery [NAME]\n  sh\n  yarn wrangler queues pause-delivery [NAME]\n  sh\n  npx wrangler queues resume-delivery [NAME]\n  sh\n  pnpm wrangler queues resume-delivery [NAME]\n  sh\n  yarn wrangler queues resume-delivery [NAME]\n  sh\n  npx wrangler queues purge [NAME]\n  sh\n  pnpm wrangler queues purge [NAME]\n  sh\n  yarn wrangler queues purge [NAME]\n  sh\n  npx wrangler queues subscription create [QUEUE]\n  sh\n  pnpm wrangler queues subscription create [QUEUE]\n  sh\n  yarn wrangler queues subscription create [QUEUE]\n  sh\n  npx wrangler queues subscription list [QUEUE]\n  sh\n  pnpm wrangler queues subscription list [QUEUE]\n  sh\n  yarn wrangler queues subscription list [QUEUE]\n  sh\n  npx wrangler queues subscription get [QUEUE]\n  sh\n  pnpm wrangler queues subscription get [QUEUE]\n  sh\n  yarn wrangler queues subscription get [QUEUE]\n  sh\n  npx wrangler queues subscription delete [QUEUE]\n  sh\n  pnpm wrangler queues subscription delete [QUEUE]\n  sh\n  yarn wrangler queues subscription delete [QUEUE]\n  sh\n  npx wrangler queues subscription update [QUEUE]\n  sh\n  pnpm wrangler queues subscription update [QUEUE]\n  sh\n  yarn wrangler queues subscription update [QUEUE]\n  sh\n  npm create cloudflare@latest -- resend-rate-limit-queue\n  sh\n  yarn create cloudflare resend-rate-limit-queue\n  sh\n  pnpm create cloudflare@latest resend-rate-limit-queue\n  sh\ncd resend-rate-limit-queue\nsh\nnpx wrangler queues create rate-limit-queue\nsh\nCreating queue rate-limit-queue.\nCreated queue rate-limit-queue.\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"queues\": {\n      \"producers\": [\n        {\n          \"binding\": \"EMAIL_QUEUE\",\n          \"queue\": \"rate-limit-queue\"\n        }\n      ],\n      \"consumers\": [\n        {\n          \"queue\": \"rate-limit-queue\",\n          \"max_batch_size\": 2,\n          \"max_batch_timeout\": 10,\n          \"max_retries\": 3\n        }\n      ]\n    }\n  }\n  toml\n  [[queues.producers]]\n  binding = \"EMAIL_QUEUE\"\n  queue = \"rate-limit-queue\"\n\n[[queues.consumers]]\n  queue = \"rate-limit-queue\"\n  max_batch_size = 2\n  max_batch_timeout = 10\n  max_retries = 3\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"resend-rate-limit-queue\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-09-09\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"queues\": {\n      \"producers\": [\n        {\n          \"binding\": \"EMAIL_QUEUE\",\n          \"queue\": \"rate-limit-queue\"\n        }\n      ],\n      \"consumers\": [\n        {\n          \"queue\": \"rate-limit-queue\",\n          \"max_batch_size\": 2,\n          \"max_batch_timeout\": 10,\n          \"max_retries\": 3\n        }\n      ]\n    }\n  }\n  toml\n  #:schema node_modules/wrangler/config-schema.json\n  name = \"resend-rate-limit-queue\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-09-09\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n[[queues.producers]]\n  binding = \"EMAIL_QUEUE\"\n  queue = \"rate-limit-queue\"\n\n[[queues.consumers]]\n  queue = \"rate-limit-queue\"\n  max_batch_size = 2\n  max_batch_timeout = 10\n  max_retries = 3\n  ts\ninterface Env {\n  EMAIL_QUEUE: Queue<any>;\n}\nts\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    try {\n      await env.EMAIL_QUEUE.send(\n        { email: await req.text() },\n        { delaySeconds: 1 },\n      );\n      return new Response(\"Success!\");\n    } catch (e) {\n      return new Response(\"Error!\", { status: 500 });\n    }\n  },\n};\nts\ninterface Message {\n  email: string;\n}\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    try {\n      await env.EMAIL_QUEUE.send(\n        { email: await req.text() },\n        { delaySeconds: 1 },\n      );\n      return new Response(\"Success!\");\n    } catch (e) {\n      return new Response(\"Error!\", { status: 500 });\n    }\n  },\n  async queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n    for (const message of batch.messages) {\n      try {\n        console.log(message.body.email);\n        // After configuring Resend, you can send email\n        message.ack();\n      } catch (e) {\n        console.error(e);\n        message.retry({ delaySeconds: 5 });\n      }\n    }\n  },\n};\nsh\nnpm run dev\nsh\ncurl -X POST -d \"test@example.com\" http://localhost:8787/\nsh\n[wrangler:inf] POST / 200 OK (2ms)\nQueueMessage {\n  attempts: 1,\n  body: { email: 'test@example.com' },\n  timestamp: 2024-09-12T13:48:07.236Z,\n  id: '72a25ff18dd441f5acb6086b9ce87c8c'\n}\ntxt\nRESEND_API_KEY='your-resend-api-key'\nts\ninterface Env {\n  EMAIL_QUEUE: Queue<any>;\n  RESEND_API_KEY: string;\n}\nsh\n  npm i resend\n  sh\n  yarn add resend\n  sh\n  pnpm add resend\n  ts\nimport { Resend } from \"resend\";\n\ninterface Message {\n  email: string;\n}\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    try {\n      await env.EMAIL_QUEUE.send(\n        { email: await req.text() },\n        { delaySeconds: 1 },\n      );\n      return new Response(\"Success!\");\n    } catch (e) {\n      return new Response(\"Error!\", { status: 500 });\n    }\n  },\n  async queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n    // Initialize Resend\n    const resend = new Resend(env.RESEND_API_KEY);\n    for (const message of batch.messages) {\n      try {\n        console.log(message.body.email);\n        // send email\n        const sendEmail = await resend.emails.send({\n          from: \"onboarding@resend.dev\",\n          to: [message.body.email],\n          subject: \"Hello World\",\n          html: \"<strong>Sending an email from Worker!</strong>\",\n        });\n\n// check if the email failed\n        if (sendEmail.error) {\n          console.error(sendEmail.error);\n          message.retry({ delaySeconds: 5 });\n        } else {\n          // if success, ack the message\n          message.ack();\n        }\n        message.ack();\n      } catch (e) {\n        console.error(e);\n        message.retry({ delaySeconds: 5 });\n      }\n    }\n  },\n};\nts\nimport { Resend } from \"resend\";\n\ninterface Message {\n  email: string;\n}\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    try {\n      await env.EMAIL_QUEUE.send(\n        { email: await req.text() },\n        { delaySeconds: 1 },\n      );\n      return new Response(\"Success!\");\n    } catch (e) {\n      return new Response(\"Error!\", { status: 500 });\n    }\n  },\n  async queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n    // Initialize Resend\n    const resend = new Resend(env.RESEND_API_KEY);\n    for (const message of batch.messages) {\n      try {\n        // send email\n        const sendEmail = await resend.emails.send({\n          from: \"onboarding@resend.dev\",\n          to: [message.body.email],\n          subject: \"Hello World\",\n          html: \"<strong>Sending an email from Worker!</strong>\",\n        });\n\n// check if the email failed\n        if (sendEmail.error) {\n          console.error(sendEmail.error);\n          message.retry({ delaySeconds: 5 });\n        } else {\n          // if success, ack the message\n          message.ack();\n        }\n      } catch (e) {\n        console.error(e);\n        message.retry({ delaySeconds: 5 });\n      }\n    }\n  },\n};\nsh\nnpm run dev\nsh\ncurl -X POST -d \"delivered@resend.dev\" http://localhost:8787/\nsh\nnpx wrangler deploy\nsh\nnpx wrangler secret put RESEND_API_KEY\nbash\ncurl -X POST -d \"delivered@resend.dev\" <YOUR_WORKER_URL>\nsh\n  npm create cloudflare@latest -- queues-web-crawler\n  sh\n  yarn create cloudflare queues-web-crawler\n  sh\n  pnpm create cloudflare@latest queues-web-crawler\n  sh\ncd queues-web-crawler\nsh\n  npx wrangler kv namespace create crawler_links\n  sh\n  yarn wrangler kv namespace create crawler_links\n  sh\n  pnpm wrangler kv namespace create crawler_links\n  sh\n  npx wrangler kv namespace create crawler_screenshots\n  sh\n  yarn wrangler kv namespace create crawler_screenshots\n  sh\n  pnpm wrangler kv namespace create crawler_screenshots\n  sh\nüåÄ Creating namespace with title \"web-crawler-crawler-links\"\n‚ú® Success!\nAdd the following to your configuration file in your kv_namespaces array:\n[[kv_namespaces]]\nbinding = \"crawler_links\"\nid = \"<GENERATED_NAMESPACE_ID>\"\n\nüåÄ Creating namespace with title \"web-crawler-crawler-screenshots\"\n‚ú® Success!\nAdd the following to your configuration file in your kv_namespaces array:\n[[kv_namespaces]]\nbinding = \"crawler_screenshots\"\nid = \"<GENERATED_NAMESPACE_ID>\"\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"kv_namespaces\": [\n      {\n        \"binding\": \"CRAWLER_SCREENSHOTS_KV\",\n        \"id\": \"<GENERATED_NAMESPACE_ID>\"\n      },\n      {\n        \"binding\": \"CRAWLER_LINKS_KV\",\n        \"id\": \"<GENERATED_NAMESPACE_ID>\"\n      }\n    ]\n  }\n  toml\n  kv_namespaces = [\n    { binding = \"CRAWLER_SCREENSHOTS_KV\", id = \"<GENERATED_NAMESPACE_ID>\" },\n    { binding = \"CRAWLER_LINKS_KV\", id = \"<GENERATED_NAMESPACE_ID>\" }\n  ]\n  sh\n  npm i -D @cloudflare/puppeteer\n  sh\n  yarn add -D @cloudflare/puppeteer\n  sh\n  pnpm add -D @cloudflare/puppeteer\n  sh\n  npm i robots-parser\n  sh\n  yarn add robots-parser\n  sh\n  pnpm add robots-parser\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"browser\": {\n      \"binding\": \"CRAWLER_BROWSER\"\n    }\n  }\n  toml\n  browser = { binding = \"CRAWLER_BROWSER\" }\n  sh\n  npx wrangler queues create queues-web-crawler\n  sh\n  yarn wrangler queues create queues-web-crawler\n  sh\n  pnpm wrangler queues create queues-web-crawler\n  txt\nCreating queue queues-web-crawler.\nCreated queue queues-web-crawler.\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"queues\": {\n      \"consumers\": [\n        {\n          \"queue\": \"queues-web-crawler\",\n          \"max_batch_timeout\": 60\n        }\n      ],\n      \"producers\": [\n        {\n          \"queue\": \"queues-web-crawler\",\n          \"binding\": \"CRAWLER_QUEUE\"\n        }\n      ]\n    }\n  }\n  toml\n  [[queues.consumers]]\n  queue = \"queues-web-crawler\"\n  max_batch_timeout = 60\n\n[[queues.producers]]\n  queue = \"queues-web-crawler\"\n  binding = \"CRAWLER_QUEUE\"\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"web-crawler\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-07-25\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"kv_namespaces\": [\n      {\n        \"binding\": \"CRAWLER_SCREENSHOTS_KV\",\n        \"id\": \"<GENERATED_NAMESPACE_ID>\"\n      },\n      {\n        \"binding\": \"CRAWLER_LINKS_KV\",\n        \"id\": \"<GENERATED_NAMESPACE_ID>\"\n      }\n    ],\n    \"browser\": {\n      \"binding\": \"CRAWLER_BROWSER\"\n    },\n    \"queues\": {\n      \"consumers\": [\n        {\n          \"queue\": \"queues-web-crawler\",\n          \"max_batch_timeout\": 60\n        }\n      ],\n      \"producers\": [\n        {\n          \"queue\": \"queues-web-crawler\",\n          \"binding\": \"CRAWLER_QUEUE\"\n        }\n      ]\n    }\n  }\n  toml\n  #:schema node_modules/wrangler/config-schema.json\n  name = \"web-crawler\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-07-25\"\n  compatibility_flags = [\"nodejs_compat\"]\n\nkv_namespaces = [\n    { binding = \"CRAWLER_SCREENSHOTS_KV\", id = \"<GENERATED_NAMESPACE_ID>\" },\n    { binding = \"CRAWLER_LINKS_KV\", id = \"<GENERATED_NAMESPACE_ID>\" }\n  ]\n\nbrowser = { binding = \"CRAWLER_BROWSER\" }\n\n[[queues.consumers]]\n  queue = \"queues-web-crawler\"\n  max_batch_timeout = 60\n\n[[queues.producers]]\n  queue = \"queues-web-crawler\"\n  binding = \"CRAWLER_QUEUE\"\n  ts\nimport { BrowserWorker } from \"@cloudflare/puppeteer\";\n\nexport interface Env {\n  CRAWLER_QUEUE: Queue<any>;\n  CRAWLER_SCREENSHOTS_KV: KVNamespace;\n  CRAWLER_LINKS_KV: KVNamespace;\n  CRAWLER_BROWSER: BrowserWorker;\n}\nts\ntype Message = {\n  url: string;\n};\n\nexport interface Env {\n  CRAWLER_QUEUE: Queue<Message>;\n  // ... etc.\n}\n\nexport default {\n  async fetch(req, env): Promise<Response> {\n    await env.CRAWLER_QUEUE.send({ url: await req.text() });\n    return new Response(\"Success!\");\n  },\n} satisfies ExportedHandler<Env>;\nts\nimport puppeteer from \"@cloudflare/puppeteer\";\nimport robotsParser from \"robots-parser\";\n\nasync queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n  let browser: puppeteer.Browser | null = null;\n  try {\n    browser = await puppeteer.launch(env.CRAWLER_BROWSER);\n  } catch {\n    batch.retryAll();\n  return;\n  }\n\nfor (const message of batch.messages) {\n    const { url } = message.body;\n\nlet isAllowed = true;\n    try {\n      const robotsTextPath = new URL(url).origin + \"/robots.txt\";\n      const response = await fetch(robotsTextPath);\n\nconst robots = robotsParser(robotsTextPath, await response.text());\n      isAllowed = robots.isAllowed(url) ?? true; // respect robots.txt!\n    } catch {}\n\nif (!isAllowed) {\n      message.ack();\n      continue;\n    }\n\n// TODO: crawl!\n    message.ack();\n  }\n\nawait browser.close();\n},\nts\ntype Result = {\n  numCloudflareLinks: number;\n  screenshot: ArrayBuffer;\n};\n\nconst crawlPage = async (url: string): Promise<Result> => {\n  const page = await (browser as puppeteer.Browser).newPage();\n\nawait page.goto(url, {\n    waitUntil: \"load\",\n  });\n\nconst numCloudflareLinks = await page.$$eval(\"a\", (links) => {\n    links = links.filter((link) => {\n      try {\n        return new URL(link.href).hostname.includes(\"cloudflare.com\");\n      } catch {\n        return false;\n      }\n    });\n    return links.length;\n  });\n\nawait page.setViewport({\n    width: 1920,\n    height: 1080,\n    deviceScaleFactor: 1,\n  });\n\nreturn {\n    numCloudflareLinks,\n    screenshot: ((await page.screenshot({ fullPage: true })) as Buffer).buffer,\n  };\n};\nts\n// const numCloudflareLinks = await page.$$eval(\"a\", (links) => { ...\n\nawait page.$$eval(\"a\", async (links) => {\n  const urls: MessageSendRequest<Message>[] = links.map((link) => {\n    return {\n      body: {\n        url: link.href,\n      },\n    };\n  });\n  try {\n    await env.CRAWLER_QUEUE.sendBatch(urls);\n  } catch {} // do nothing, likely hit subrequest limit\n});\n\n// await page.setViewport({ ...\nts\n// in the `queue` handler:\n// ...\nif (!isAllowed) {\n  message.ack();\n  continue;\n}\n\ntry {\n  const { numCloudflareLinks, screenshot } = await crawlPage(url);\n  const timestamp = new Date().getTime();\n  const resultKey = `${encodeURIComponent(url)}-${timestamp}`;\n  await env.CRAWLER_LINKS_KV.put(resultKey, numCloudflareLinks.toString(), {\n    metadata: { date: timestamp },\n  });\n  await env.CRAWLER_SCREENSHOTS_KV.put(resultKey, screenshot, {\n    metadata: { date: timestamp },\n  });\n  message.ack();\n} catch {\n  message.retry();\n}\n\n// ...\nts\ntype KeyMetadata = {\n  date: number;\n};\n\n// in the `queue` handler:\n// ...\nfor (const message of batch.messages) {\n  const sameUrlCrawls = await env.CRAWLER_LINKS_KV.list({\n    prefix: `${encodeURIComponent(url)}`,\n  });\n\nlet shouldSkip = false;\n  for (const key of sameUrlCrawls.keys) {\n    if (timestamp - (key.metadata as KeyMetadata)?.date < 60 * 60 * 1000) {\n      // if crawled in last hour, skip\n      message.ack();\n      shouldSkip = true;\n      break;\n    }\n  }\n  if (shouldSkip) {\n    continue;\n  }\n\nlet isAllowed = true;\n  // ...\nts\nimport puppeteer, { BrowserWorker } from \"@cloudflare/puppeteer\";\nimport robotsParser from \"robots-parser\";\n\ntype Message = {\n  url: string;\n};\n\nexport interface Env {\n  CRAWLER_QUEUE: Queue<Message>;\n  CRAWLER_SCREENSHOTS_KV: KVNamespace;\n  CRAWLER_LINKS_KV: KVNamespace;\n  CRAWLER_BROWSER: BrowserWorker;\n}\n\ntype Result = {\n  numCloudflareLinks: number;\n  screenshot: ArrayBuffer;\n};\n\ntype KeyMetadata = {\n  date: number;\n};\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    // util endpoint for testing purposes\n    await env.CRAWLER_QUEUE.send({ url: await req.text() });\n    return new Response(\"Success!\");\n  },\n  async queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n    const crawlPage = async (url: string): Promise<Result> => {\n      const page = await (browser as puppeteer.Browser).newPage();\n\nawait page.goto(url, {\n        waitUntil: \"load\",\n      });\n\nconst numCloudflareLinks = await page.$$eval(\"a\", (links) => {\n        links = links.filter((link) => {\n          try {\n            return new URL(link.href).hostname.includes(\"cloudflare.com\");\n          } catch {\n            return false;\n          }\n        });\n        return links.length;\n      });\n\n// to crawl recursively - uncomment this!\n      /*await page.$$eval(\"a\", async (links) => {\n        const urls: MessageSendRequest<Message>[] = links.map((link) => {\n          return {\n            body: {\n              url: link.href,\n            },\n          };\n        });\n        try {\n          await env.CRAWLER_QUEUE.sendBatch(urls);\n        } catch {} // do nothing, might've hit subrequest limit\n      });*/\n\nawait page.setViewport({\n        width: 1920,\n        height: 1080,\n        deviceScaleFactor: 1,\n      });\n\nreturn {\n        numCloudflareLinks,\n        screenshot: ((await page.screenshot({ fullPage: true })) as Buffer)\n          .buffer,\n      };\n    };\n\nlet browser: puppeteer.Browser | null = null;\n    try {\n      browser = await puppeteer.launch(env.CRAWLER_BROWSER);\n    } catch {\n      batch.retryAll();\n      return;\n    }\n\nfor (const message of batch.messages) {\n      const { url } = message.body;\n      const timestamp = new Date().getTime();\n      const resultKey = `${encodeURIComponent(url)}-${timestamp}`;\n\nconst sameUrlCrawls = await env.CRAWLER_LINKS_KV.list({\n        prefix: `${encodeURIComponent(url)}`,\n      });\n\nlet shouldSkip = false;\n      for (const key of sameUrlCrawls.keys) {\n        if (timestamp - (key.metadata as KeyMetadata)?.date < 60 * 60 * 1000) {\n          // if crawled in last hour, skip\n          message.ack();\n          shouldSkip = true;\n          break;\n        }\n      }\n      if (shouldSkip) {\n        continue;\n      }\n\nlet isAllowed = true;\n      try {\n        const robotsTextPath = new URL(url).origin + \"/robots.txt\";\n        const response = await fetch(robotsTextPath);\n\nconst robots = robotsParser(robotsTextPath, await response.text());\n        isAllowed = robots.isAllowed(url) ?? true; // respect robots.txt!\n      } catch {}\n\nif (!isAllowed) {\n        message.ack();\n        continue;\n      }\n\ntry {\n        const { numCloudflareLinks, screenshot } = await crawlPage(url);\n        await env.CRAWLER_LINKS_KV.put(\n          resultKey,\n          numCloudflareLinks.toString(),\n          { metadata: { date: timestamp } },\n        );\n        await env.CRAWLER_SCREENSHOTS_KV.put(resultKey, screenshot, {\n          metadata: { date: timestamp },\n        });\n        message.ack();\n      } catch {\n        message.retry();\n      }\n    }\n\nawait browser.close();\n  },\n};\nsh\n  npx wrangler deploy\n  sh\n  yarn wrangler deploy\n  sh\n  pnpm wrangler deploy\n  bash\ncurl <YOUR_WORKER_URL> \\\n  -H \"Content-Type: application/json\" \\\n  -d 'https://developers.cloudflare.com/queues/tutorials/web-crawler-with-browser-rendering/'\nsql\n-- Valid\nSELECT timestamp, user_id, status FROM my_table;\nSELECT * FROM my_table;\n\n-- Invalid\nSELECT user_id AS uid, timestamp AS ts FROM my_table;\nSELECT COUNT(*) FROM events FROM FROM my_table;\nSELECT json_field.property FROM my_table;\nSELECT 1 AS synthetic_column FROM my_table;\nsql\n-- Valid\nSELECT department, COUNT(*) FROM sales GROUP BY department;\nSELECT region, AVG(amount) FROM sales GROUP BY region;\nSELECT category, MIN(price), MAX(price) FROM products GROUP BY category;\nSELECT SUM(quantity) FROM sales GROUP BY department ORDER BY SUM(amount) DESC;\n\n-- Invalid\nSELECT COUNT(*) AS total FROM sales GROUP BY department; -- No aliases\nSELECT COUNT(department) FROM sales; -- Must use COUNT(*)\nSELECT COUNT(DISTINCT region) FROM sales; -- No DISTINCT support\nsql\nSELECT region, COUNT(*) FROM sales GROUP BY region;\nSELECT dept, category, COUNT(*) FROM sales GROUP BY dept, category;\nSELECT region, COUNT(*) FROM sales WHERE status = 'completed' GROUP BY region;\nSELECT dept, COUNT(*) FROM sales GROUP BY dept ORDER BY COUNT(*) DESC LIMIT 10;\nSELECT is_active, SUM(amount) FROM sales GROUP BY is_active;\nSELECT dept, SUM(amount) FROM sales GROUP BY dept ORDER BY SUM(amount) DESC;\nsql\nSELECT region, COUNT(*) FROM sales GROUP BY region HAVING COUNT(*) > 1000;\nSELECT dept, SUM(amount) FROM sales GROUP BY dept HAVING SUM(amount) > 100000; -- HAVING with SUM\nSELECT region, COUNT(*) FROM sales GROUP BY region HAVING COUNT(*) > 100 AND COUNT(*) < 1000;\nsql\n--Valid\nSELECT * FROM http_requests;\n\n--Invalid\nSELECT * FROM table1, table2;\nSELECT * FROM table1 JOIN table2 ON table1.id = table2.id;\nSELECT * FROM (SELECT * FROM events WHERE status = 200);\nsql\n--Valid\nSELECT * FROM events WHERE timestamp BETWEEN '2024-01-01' AND '2024-01-02';\nSELECT * FROM logs WHERE status = 200 AND user_type = 'premium';\nSELECT * FROM requests WHERE (method = 'GET' OR method = 'POST') AND response_time < 1000;\n\n--Invalid\nSELECT * FROM logs WHERE tags[0] = 'error'; -- Array filtering\nSELECT * FROM requests WHERE metadata.user_id = '123'; -- JSON field filtering\nSELECT * FROM events WHERE col_a = col_b; -- Column comparison\nSELECT * FROM logs WHERE response_time + latency > 5000; -- Arithmetic\nsql\n-- Valid\nSELECT * FROM table_name WHERE ... ORDER BY partitionKey;\nSELECT * FROM table_name WHERE ... ORDER BY partitionKey DESC;\nSELECT dept, COUNT(*) FROM table_name GROUP BY dept ORDER BY COUNT(*) DESC;\n\n-- Invalid\nSELECT * FROM table_name GROUP BY dept ORDER BY nonPartitionKey DESC --ORDER BY a non-grouped column\nsql\n-- Valid\nSELECT * FROM events LIMIT 100\nSELECT * FROM logs WHERE ... LIMIT 10000\n\n-- Invalid\nSELECT * FROM events LIMIT 100, 50; -- Pagination\nSELECT * FROM logs LIMIT COUNT(*); / 2 -- Functions\nSELECT * FROM events LIMIT 10 * 10; -- Arithmetic\nbash\nexport WRANGLER_R2_SQL_AUTH_TOKEN= #paste your token here\nbash\nnpx wrangler login\nbash\n  npx wrangler r2 bucket create fraud-pipeline\n  bash\n  npx wrangler r2 bucket catalog enable fraud-pipeline\n  bash\nexport WAREHOUSE= #Paste your warehouse here\nbash\n  npx wrangler r2 bucket catalog compaction enable fraud-pipeline --token $WRANGLER_R2_SQL_AUTH_TOKEN\n  json\n  {\n    \"fields\": [\n      { \"name\": \"transaction_id\", \"type\": \"string\", \"required\": true },\n      { \"name\": \"user_id\", \"type\": \"int64\", \"required\": true },\n      { \"name\": \"amount\", \"type\": \"float64\", \"required\": false },\n      { \"name\": \"transaction_timestamp\", \"type\": \"string\", \"required\": false },\n      { \"name\": \"location\", \"type\": \"string\", \"required\": false },\n      { \"name\": \"merchant_category\", \"type\": \"string\", \"required\": false },\n      { \"name\": \"is_fraud\", \"type\": \"bool\", \"required\": false }\n    ]\n  }\n  bash\n  npx wrangler pipelines streams create raw_events_stream \\\n    --schema-file raw_transactions_schema.json \\\n    --http-enabled true \\\n    --http-auth false\n  bash\n  # The http ingest endpoint from the output (see example below)\n  export STREAM_ENDPOINT= #the http ingest endpoint from the output (see example below)\n  sh\n  üåÄ Creating stream 'raw_events_stream'...\n  ‚ú® Successfully created stream 'raw_events_stream' with id 'stream_id'.\n\nCreation Summary:\n  General:\n    Name:  raw_events_stream\n\nHTTP Ingest:\n    Enabled:         Yes\n    Authentication:  Yes\n    Endpoint:        https://stream_id.ingest.cloudflare.com\n    CORS Origins:    None\n\nInput Schema:\n  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n  ‚îÇ Field Name            ‚îÇ Type   ‚îÇ Unit/Items ‚îÇ Required ‚îÇ\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n  ‚îÇ transaction_id        ‚îÇ string ‚îÇ            ‚îÇ Yes      ‚îÇ\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n  ‚îÇ user_id               ‚îÇ int64  ‚îÇ            ‚îÇ Yes      ‚îÇ\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n  ‚îÇ amount                ‚îÇfloat64 ‚îÇ            ‚îÇ No       ‚îÇ\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n  ‚îÇ transaction_timestamp ‚îÇ string ‚îÇ            ‚îÇ No       ‚îÇ\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n  ‚îÇ location              ‚îÇ string ‚îÇ            ‚îÇ No       ‚îÇ\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n  ‚îÇ merchant_category     ‚îÇ string ‚îÇ            ‚îÇ No       ‚îÇ\n  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n  ‚îÇ is_fraud              ‚îÇ bool   ‚îÇ            ‚îÇ No       ‚îÇ\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n  bash\n  npx wrangler pipelines sinks create raw_events_sink \\\n    --type \"r2-data-catalog\" \\\n    --bucket \"fraud-pipeline\" \\\n    --roll-interval 30 \\\n    --namespace \"fraud_detection\" \\\n    --table \"transactions\" \\\n    --catalog-token $WRANGLER_R2_SQL_AUTH_TOKEN\n  bash\n  npx wrangler pipelines create raw_events_pipeline \\\n    --sql \"INSERT INTO raw_events_sink SELECT * FROM raw_events_stream\"\n  json\n       {\n         \"fields\": [\n           { \"name\": \"transaction_id\", \"type\": \"string\", \"required\": true },\n           { \"name\": \"user_id\", \"type\": \"int64\", \"required\": true },\n           { \"name\": \"amount\", \"type\": \"float64\", \"required\": false },\n           {\n             \"name\": \"transaction_timestamp\",\n             \"type\": \"string\",\n             \"required\": false\n           },\n           { \"name\": \"location\", \"type\": \"string\", \"required\": false },\n           { \"name\": \"merchant_category\", \"type\": \"string\", \"required\": false },\n           { \"name\": \"is_fraud\", \"type\": \"bool\", \"required\": false }\n         ]\n       }\n       sql\n       INSERT INTO raw_events_sink SELECT * FROM raw_events_stream;\n       python\nimport requests\nimport json\nimport uuid\nimport random\nimport time\nimport os\nfrom datetime import datetime, timezone, timedelta",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "This will issue a HTTP POST request, and if successful, return a HTTP 200 with a `success: true` response body.\n\n* If you receive a HTTP 400, this is because you attempted to send malformed JSON to your queue.\n* If you receive a HTTP 500, this is because the message was not written to your Queue successfully.\n\nYou can use [`wrangler tail`](https://developers.cloudflare.com/workers/observability/logs/real-time-logs/) to debug the output of `console.log`.\n\n</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Queues & R2 ¬∑ Cloudflare Queues docs\ndescription: Example of how to use Queues to batch data and store it in an R2 bucket.\nlastUpdated: 2025-01-29T12:28:42.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/examples/send-errors-to-r2/\n  md: https://developers.cloudflare.com/queues/examples/send-errors-to-r2/index.md\n---\n\nThe following Worker will catch JavaScript errors and send them to a queue. The same Worker will receive those errors in batches and store them to a log file in an R2 bucket.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Sending messages from the dashboard ¬∑ Cloudflare\n  Queues docs\ndescription: Use the dashboard to send messages to a queue.\nlastUpdated: 2025-09-04T11:41:09.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/examples/send-messages-from-dash/\n  md: https://developers.cloudflare.com/queues/examples/send-messages-from-dash/index.md\n---\n\nSending messages from the dashboard allows you to debug Queues or queue consumers without a producer Worker.\n\nTo send messages from the dashboard:\n\n1. In the Cloudflare dashboard, go to the **Queues** page.\n\n   [Go to **Queues**](https://dash.cloudflare.com/?to=/:account/workers/queues)\n\n2. Select the queue to send a message to.\n\n3. Select the **Messages** tab.\n\n4. Select **Send**.\n\n5. Choose your message **Content Type**: *Text* or *JSON*.\n\n6. Enter your message. Alternatively, drag a file over the textbox to upload a file as a message.\n\n7. Select **Send**.\n\nYour message will be sent to the queue.\n\nRefer to the [Get Started guide](https://developers.cloudflare.com/queues/get-started/) to learn how to send messages to a queue from a Worker.\n\n</page>\n\n<page>\n---\ntitle: Serverless ETL pipelines ¬∑ Cloudflare Queues docs\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/examples/serverless-etl/\n  md: https://developers.cloudflare.com/queues/examples/serverless-etl/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Queues - Use Queues and Durable Objects ¬∑ Cloudflare Queues docs\ndescription: Publish to a queue from within a Durable Object.\nlastUpdated: 2025-08-21T12:34:40.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/examples/use-queues-with-durable-objects/\n  md: https://developers.cloudflare.com/queues/examples/use-queues-with-durable-objects/index.md\n---\n\nThe following example shows you how to write a Worker script to publish to [Cloudflare Queues](https://developers.cloudflare.com/queues/) from within a [Durable Object](https://developers.cloudflare.com/durable-objects/).\n\nPrerequisites:\n\n* A [queue created](https://developers.cloudflare.com/queues/get-started/#3-create-a-queue) via the Cloudflare dashboard or the [wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/).\n* A [configured **producer** binding](https://developers.cloudflare.com/queues/configuration/configure-queues/#producer-worker-configuration) in the Cloudflare dashboard or Wrangler file.\n* A [Durable Object namespace binding](https://developers.cloudflare.com/workers/wrangler/configuration/#durable-objects).\n\nConfigure your Wrangler file as follows:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "The following Worker script:\n\n1. Creates a Durable Object stub, or retrieves an existing one based on a userId.\n2. Passes request data to the Durable Object.\n3. Publishes to a queue from within the Durable Object.\n\nThe `constructor()` in the Durable Object makes your `Environment` available (in scope) on `this.env` to the [`fetch()` handler](https://developers.cloudflare.com/durable-objects/best-practices/create-durable-object-stubs-and-send-requests/) in the Durable Object.",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Metrics ¬∑ Cloudflare Queues docs\ndescription: Queues expose metrics which allow you to measure the queue backlog,\n  consumer concurrency, and message operations.\nlastUpdated: 2025-05-14T00:02:06.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/observability/metrics/\n  md: https://developers.cloudflare.com/queues/observability/metrics/index.md\n---\n\nQueues expose metrics which allow you to measure the queue backlog, consumer concurrency, and message operations.\n\nThe metrics displayed in the [Cloudflare dashboard](https://dash.cloudflare.com/) are queried from Cloudflare‚Äôs [GraphQL Analytics API](https://developers.cloudflare.com/analytics/graphql-api/). You can access the metrics [programmatically](#query-via-the-graphql-api) via GraphQL or HTTP client.\n\n## Metrics\n\n### Backlog\n\nQueues export the below metrics within the `queuesBacklogAdaptiveGroups` dataset.\n\n| Metric | GraphQL Field Name | Description |\n| - | - | - |\n| Backlog bytes | `bytes` | Average size of the backlog, in bytes |\n| Backlog messages | `messages` | Average size of the backlog, in number of messages |\n\nThe `queuesBacklogAdaptiveGroups` dataset provides the following dimensions for filtering and grouping queries:\n\n* `queueID` - ID of the queue\n* `datetime` - Timestamp for when the message was sent\n* `date` - Timestamp for when the message was sent, truncated to the start of a day\n* `datetimeHour` - Timestamp for when the message was sent, truncated to the start of an hour\n* `datetimeMinute` - Timestamp for when the message was sent, truncated to the start of a minute\n\n### Consumer concurrency\n\nQueues export the below metrics within the `queueConsumerMetricsAdaptiveGroups` dataset.\n\n| Metric | GraphQL Field Name | Description |\n| - | - | - |\n| Avg. Consumer Concurrency | `concurrency` | Average number of concurrent consumers over the period |\n\nThe `queueConsumerMetricsAdaptiveGroups` dataset provides the following dimensions for filtering and grouping queries:\n\n* `queueID` - ID of the queue\n* `datetime` - Timestamp for the consumer metrics\n* `date` - Timestamp for the consumer metrics, truncated to the start of a day\n* `datetimeHour` - Timestamp for the consumer metrics, truncated to the start of an hour\n* `datetimeMinute` - Timestamp for the consumer metrics, truncated to the start of a minute\n\n### Message operations\n\nQueues export the below metrics within the `queueMessageOperationsAdaptiveGroups` dataset.\n\n| Metric | GraphQL Field Name | Description |\n| - | - | - |\n| Total billable operations | `billableOperations` | Sum of billable operations (writes, reads, and deletes) over the time period |\n| Total Bytes | `bytes` | Sum of bytes read, written, and deleted from the queue |\n| Lag | `lagTime` | Average lag time in milliseconds between when the message was written and the operation to consume the message. |\n| Retries | `retryCount` | Average number of retries per message |\n| Message Size | `messageSize` | Maximum message size over the specified period |\n\nThe `queueMessageOperationsAdaptiveGroups` dataset provides the following dimensions for filtering and grouping queries:\n\n* `queueID` - ID of the queue\n* `actionType` - The type of message operation. Can be `WriteMessage`, `ReadMessage` or `DeleteMessage`\n* `consumerType` - The queue consumer type. Can be `worker` or `http`. Only applicable for `ReadMessage` and `DeleteMessage` action types\n* `outcome` - The outcome of the mesage operation. Only applicable for `DeleteMessage` action types. Can be `success`, `dlq` or `fail`.\n* `datetime` - Timestamp for the message operation\n* `date` - Timestamp for the message operation, truncated to the start of a day\n* `datetimeHour` - Timestamp for the message operation, truncated to the start of an hour\n* `datetimeMinute` - Timestamp for the message operation, truncated to the start of a minute\n\n## Example GraphQL Queries\n\n### Get average queue backlog over time period",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAiucAhAhgYwNYBsD2BzACgCgYYASdNHEAOwBcAVFPALhgGc6IBLGvAQhLlQYcAEkAJm048+g0mQko6YOtwC2YAMp0UEOmwYaw88kpVrNAURpSYRzYICUMAN5CAbtzAB3SG6FSSmp6dgIAM24sFQg2Vxhg2kZmNgo0KiSmPBgAXxd3UkKYEWR0bHwAQSUABzUPMABxCGpqsMCimCwNbgMYAEYABiGB9qLI6Mg40Y6SsElU2clpovNVYwB9PDBgVNXLbV19ZcK9jaxt3eU161tjnOn845QPbIKOos12dmYwdmPSABGUBUf3epHu7whhSh9xyQA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBHWAUzaoBMsQAlAUQAKAGXz8KAdSrIAEtTqNOYRK0QBLALasAyojAAnRDwBMABmMBWALQBGY1YDMp5MeOZjATkwOA7AC0GECUVdS1+eG5sM0tbewcbZFMAFncvXwCAXyA)\n\n### Get average consumer concurrency by hour",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAiucBhA9gOwMYghMmoBCUAEitgBQBQMMAJAIYYZloAuAKvQOYBcMAzqwgBLNFwCE1OqDDgAkgBM+gkWMk1aC+qzCthAWzABlVvQis+7A2HV0tOvYYCiaJTCuHJAShgBvKQBuwmAA7pB+UjSMzCBs-OQAZsIANjoQfL4w0Swc3HwMTDmcXDAAvj7+NFUwMsjo-CCGEACyuiIY-ACCWgAOegFgAOIQZD3xkdUwyQbCFjAAjAAMy4sT1UmpkBlrk7Vgivl7ijvV9rrWAPpcYMD5Z47GpuYnVfeXyTd32ufOri+lLxQEAUkCIfAA2m9DKRsBcACJOIxIAC6OwqL3oARKlUm1WYmGwuHw-xeCmsaH4wnqEVxp2+DxhEBJuIB1VZZUopSAA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBHWAUzaoBMsQAlAUQAKAGXz8KAdSrIAEtTqNOYRK0QBLALasAyojAAnRDwBMABmMBWALQBGY1YDMp5MeOZjATkwOA7AC0GECUVdS1+eG5sM0tbewcbZFMAFncvXwCAXyA)\n\n### Get message operations by minute",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAiucBZMBnVBDA5mA8gB0gwBcBLAewDtUAhKJUykYsACgCgYYASDAYz7kQlYgBVsALhipiERlgCEnHqDDgAkgBMpMuZUXLumkmDIBbMAGViGCMSkARE0q5GT5sAFFK2mE5ZKAJQwAN7KAG6kYADukKHKXPyCwsSorABmpAA2LBBSITBJQiLiWFK8AsVi2DAAvsFhXE0wqshomDgERGRUqACCxvhk4WAA4hBC+GkJzTBZpGak9jAAjAAMG2szzZk5kPnbs61gWuXHWofNxiweAPo4wOXXpgtWNnaXTc93WWCPPN9Xt5NJ9ap9yBBNJA6FIANqAiwMJgsW4OTyWADCAF1Dg1PskRJ9UCAzPFZrMAEZQFioUGfTSvagUahk8lfdyvJHMMB08lg5r8ursWpAA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBHWAUzaoBMsQAlAUQAKAGXz8KAdSrIAEtTqNOYRK0QBLALasAyojAAnRDwBMABmMBWALQBGY1YDMphiCUr1W-vG7Yzl2-YONiAAvkA)\n\n</page>\n\n<page>\n---\ntitle: Audit Logs ¬∑ Cloudflare Queues docs\ndescription: Audit logs provide a comprehensive summary of changes made within\n  your Cloudflare account, including those made to Queues. This functionality is\n  always enabled.\nlastUpdated: 2025-09-04T16:11:18.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/audit-logs/\n  md: https://developers.cloudflare.com/queues/platform/audit-logs/index.md\n---\n\n[Audit logs](https://developers.cloudflare.com/fundamentals/account/account-security/review-audit-logs/) provide a comprehensive summary of changes made within your Cloudflare account, including those made to Queues. This functionality is always enabled.\n\n## Viewing audit logs\n\nTo view audit logs for your Queue in the Cloudflare dashboard, go to the **Audit logs** page.\n\n[Go to **Audit logs**](https://dash.cloudflare.com/?to=/:account/audit-log)\n\nFor more information on how to access and use audit logs, refer to [Review audit logs](https://developers.cloudflare.com/fundamentals/account/account-security/review-audit-logs/).\n\n## Logged operations\n\nThe following configuration actions are logged:\n\n| Operation | Description |\n| - | - |\n| CreateQueue | Creation of a new queue. |\n| DeleteQueue | Deletion of an existing queue. |\n| UpdateQueue | Updating the configuration of a queue. |\n| AttachConsumer | Attaching a consumer, including HTTP pull consumers, to the Queue. |\n| RemoveConsumer | Removing a consumer, including HTTP pull consumers, from the Queue. |\n| UpdateConsumerSettings | Changing Queues consumer settings. |\n\n</page>\n\n<page>\n---\ntitle: Changelog ¬∑ Cloudflare Queues docs\ndescription: Subscribe to RSS\nlastUpdated: 2025-02-13T19:35:19.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/changelog/\n  md: https://developers.cloudflare.com/queues/platform/changelog/index.md\n---\n\n[Subscribe to RSS](https://developers.cloudflare.com/queues/platform/changelog/index.xml)\n\n## 2025-04-17\n\n**Improved limits for pull consumers**\n\n[Queues Pull Consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/) can now pull and acknowledge up to 5,000 messages per second per queue. Previously, pull consumers were rate limited to 1200 requests / 5 minutes, aggregated across all queues.\n\nRefer to the [documentation on pull consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/) to learn how to setup a pull consumer, acknowledge / retry messages, and setup multiple consumers.\n\n## 2025-03-27\n\n**Pause delivery and purge queues**\n\nQueues now supports the ability to pause delivery and/or delete messages from a queue, allowing you to better manage queue backlogs.\n\nMessage delivery from a Queue to consumers can be paused / resumed. Queues continue to receive messages while paused.\n\nQueues can be purged to permanently delete all messages currently stored in a Queue. This operation is useful while testing a new application, if a queue producer was misconfigured and is sending bad messages.\n\nRefer to the [documentation on Pause & Purge](https://developers.cloudflare.com/queues/configuration/pause-purge/) to learn how to use both operations.\n\n## 2025-02-14\n\n**Customize message retention period**\n\nYou can now customize a queue's message retention period, from a minimum of 60 seconds to a maximum of 14 days. Previously, it was fixed to the default of 4 days.\n\nRefer to the [Queues confiuguration documentation](https://developers.cloudflare.com/queues/configuration/configure-queues/#queue-configuration) to learn more.\n\n## 2024-09-26\n\n**Queues is GA, with higher throughput & consumer concurrency**\n\nQueues is now generally available.\n\nThe per-queue message throughput has increased from 400 to 5,000 messages per second. This applies to new and existing queues.\n\nMaximum concurrent consumers has increased from 20 to 250. This applies to new and existing queues. Queues with no explicit limit will automatically scale to the new maximum. Review the [consumer concurrency documentation](https://developers.cloudflare.com/queues/configuration/consumer-concurrency) to learn more.\n\n## 2024-03-26\n\n**Delay messages published to a queue**\n\nMessages published to a queue and/or marked for retry from a queue consumer can now be explicitly delayed. Delaying messages allows you to defer tasks until later, and/or respond to backpressure when consuming from a queue.\n\nRefer to [Batching and Retries](https://developers.cloudflare.com/queues/configuration/batching-retries/) to learn how to delay messages written to a queue.\n\n## 2024-03-25\n\n**Support for pull-based consumers**\n\nQueues now supports [pull-based consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/). A pull-based consumer allows you to pull from a queue over HTTP from any environment and/or programming language outside of Cloudflare Workers. A pull-based consumer can be useful when your message consumption rate is limited by upstream infrastructure or long-running tasks.\n\nReview the [documentation on pull-based consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/) to configure HTTP-based pull.\n\n## 2024-03-18\n\n**Default content type now set to JSON**\n\nThe default [content type](https://developers.cloudflare.com/queues/configuration/javascript-apis/#queuescontenttype) for messages published to a queue is now `json`, which improves compatibility with the upcoming pull-based queues.\n\nAny Workers created on or after the [compatibility date](https://developers.cloudflare.com/workers/configuration/compatibility-flags/#queues-send-messages-in-json-format) of `2024-03-18`, or that explicitly set the `queues_json_messages` compatibility flag, will use the new default behaviour. Existing Workers with a compatibility date prior will continue to use `v8` as the default content type for published messages.\n\n## 2024-02-24\n\n**Explicit retries no longer impact consumer concurrency/scaling.**\n\nCalling `retry()` or `retryAll()` on a message or message batch will no longer have an impact on how Queues scales [consumer concurrency](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/).\n\nPreviously, using [explicit retries](https://developers.cloudflare.com/queues/configuration/batching-retries/#explicit-acknowledgement-and-retries) via `retry()` or `retryAll()` would count as an error and could result in Queues scaling down the number of concurrent consumers.\n\n## 2023-10-07\n\n**More queues per account - up to 10,000**\n\nDevelopers building on Queues can now create up to 10,000 queues per account, enabling easier per-user, per-job and sharding use-cases.\n\nRefer to [Limits](https://developers.cloudflare.com/queues/platform/limits) to learn more about Queues' current limits.\n\n## 2023-10-05\n\n**Higher consumer concurrency limits**\n\n[Queue consumers](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/) can now scale to 20 concurrent invocations (per queue), up from 10. This allows you to scale out and process higher throughput queues more quickly.\n\nQueues with [no explicit limit specified](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/#limit-concurrency) will automatically scale to the new maximum.\n\nThis limit will continue to grow during the Queues beta.\n\n## 2023-03-28\n\n**Consumer concurrency (enabled)**\n\nQueue consumers will now [automatically scale up](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/) based on the number of messages being written to the queue. To control or limit concurrency, you can explicitly define a [`max_concurrency`](https://developers.cloudflare.com/queues/configuration/configure-queues/#consumer) for your consumer.\n\n## 2023-03-15\n\n**Consumer concurrency (upcoming)**\n\nQueue consumers will soon automatically scale up concurrently as a queues' backlog grows in order to keep overall message processing latency down. Concurrency will be enabled on all existing queues by 2023-03-28.\n\n**To opt-out, or to configure a fixed maximum concurrency**, set `max_concurrency = 1` in your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/) or via [the queues dashboard](https://dash.cloudflare.com/?to=/:account/queues).\n\n**To opt-in, you do not need to take any action**: your consumer will begin to scale out as needed to keep up with your message backlog. It will scale back down as the backlog shrinks, and/or if a consumer starts to generate a higher rate of errors. To learn more about how consumers scale, refer to the [consumer concurrency](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/) documentation.\n\n## 2023-03-02\n\n**Explicit acknowledgement (new feature)**\n\nYou can now [acknowledge individual messages with a batch](https://developers.cloudflare.com/queues/configuration/batching-retries/#explicit-acknowledgement-and-retries) by calling `.ack()` on a message.\n\nThis allows you to mark a message as delivered as you process it within a batch, and avoids the entire batch from being redelivered if your consumer throws an error during batch processing. This can be particularly useful when you are calling external APIs, writing messages to a database, or otherwise performing non-idempotent actions on individual messages within a batch.\n\n## 2023-03-01\n\n**Higher per-queue throughput**\n\nThe per-queue throughput limit has now been [raised to 400 messages per second](https://developers.cloudflare.com/queues/platform/limits/).\n\n## 2022-12-13\n\n**sendBatch support**\n\nThe JavaScript API for Queue producers now includes a `sendBatch` method which supports sending up to 100 messages at a time.\n\n## 2022-12-12\n\n**Increased per-account limits**\n\nQueues now allows developers to create up to 100 queues per account, up from the initial beta limit of 10 per account. This limit will continue to increase over time.\n\n</page>\n\n<page>\n---\ntitle: Limits ¬∑ Cloudflare Queues docs\ndescription: 1 1 KB is measured as 1000 bytes. Messages can include up to ~100\n  bytes of internal metadata that counts towards total message limits.\nlastUpdated: 2025-04-16T19:11:49.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/limits/\n  md: https://developers.cloudflare.com/queues/platform/limits/index.md\n---\n\n| Feature | Limit |\n| - | - |\n| Queues | 10,000 per account |\n| Message size | 128 KB 1 |\n| Message retries | 100 |\n| Maximum consumer batch size | 100 messages |\n| Maximum messages per `sendBatch` call | 100 (or 256KB in total) |\n| Maximum Batch wait time | 60 seconds |\n| Per-queue message throughput | 5,000 messages per second 2 |\n| Message retention period 3 | 4 days (default). [Configurable to 14 days](https://developers.cloudflare.com/queues/configuration/configure-queues/#queue-configuration) |\n| Per-queue backlog size 4 | 25GB |\n| Concurrent consumer invocations | 250 push-based only |\n| Consumer duration (wall clock time) | 15 minutes 5 |\n| [Consumer CPU time](https://developers.cloudflare.com/workers/platform/limits/#cpu-time) | 30 seconds (default). [Configurable to 5 minutes](https://developers.cloudflare.com/queues/platform/limits/#increasing-queue-consumer-worker-cpu-limits) |\n| `visibilityTimeout` (pull-based queues) | 12 hours |\n| `delaySeconds` (when sending or retrying) | 12 hours |\n| Requests to the Queues API (excluding pull consumer operations)6 | [1200 requests / 5 mins](https://developers.cloudflare.com/fundamentals/api/reference/limits/) |\n\n1 1 KB is measured as 1000 bytes. Messages can include up to \\~100 bytes of internal metadata that counts towards total message limits.\n\n2 Exceeding the maximum message throughput will cause the `send()` and `sendBatch()` methods to throw an exception with a `Too Many Requests` error until your producer falls below the limit.\n\n3 Messages in a queue that reach the maximum message retention are deleted from the queue. Queues does not delete messages in the same queue that have not reached this limit.\n\n4 Individual queues that reach this limit will receive a `Storage Limit Exceeded` error when calling `send()` or `sendBatch()` on the queue.\n\n5 Refer to [Workers limits](https://developers.cloudflare.com/workers/platform/limits/#cpu-time).\n\n6 [Pull Consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers) allow you to consume messages from a queue over HTTP. Pulls, acknowledgements, and retries over HTTP are not subject to the API rate limit.\n\nNeed a higher limit?\n\nTo request an adjustment to a limit, complete the [Limit Increase Request Form](https://forms.gle/ukpeZVLWLnKeixDu7). If the limit can be increased, Cloudflare will contact you with next steps.\n\n### Increasing Queue Consumer Worker CPU Limits\n\n[Queue consumer Workers](https://developers.cloudflare.com/queues/reference/how-queues-works/#consumers) are Worker scripts, and share the same [per invocation CPU limits](https://developers.cloudflare.com/workers/platform/limits/#worker-limits) as any Workers do. Note that CPU time is active processing time: not time spent waiting on network requests, storage calls, or other general I/O.\n\nBy default, the maximum CPU time per consumer Worker invocation is set to 30 seconds, but can be increased by setting `limits.cpu_ms` in your Wrangler configuration:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "To learn more about CPU time and limits, [review the Workers documentation](https://developers.cloudflare.com/workers/platform/limits/#cpu-time).\n\n</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Pricing ¬∑ Cloudflare Queues docs\ndescription: Cloudflare Queues charges for the total number of operations\n  against each of your queues during a given month.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/pricing/\n  md: https://developers.cloudflare.com/queues/platform/pricing/index.md\n---\n\nNote\n\nCloudflare Queues requires the [Workers Paid plan](https://developers.cloudflare.com/workers/platform/pricing/#workers) to use, but does not increase your monthly subscription cost.\n\nCloudflare Queues charges for the total number of operations against each of your queues during a given month.\n\n* An operation is counted for each 64 KB of data that is written, read, or deleted.\n* Messages larger than 64 KB are charged as if they were multiple messages: for example, a 65 KB message and a 127 KB message would both incur two operation charges when written, read, or deleted.\n* A KB is defined as 1,000 bytes, and each message includes approximately 100 bytes of internal metadata.\n* Operations are per message, not per batch. A batch of 10 messages (the default batch size), if processed, would incur 10x write, 10x read, and 10x delete operations: one for each message in the batch.\n* There are no data transfer (egress) or throughput (bandwidth) charges.\n\n| | Workers Paid |\n| - | - |\n| Standard operations | 1,000,000 operations/month included + $0.40/million operations |\n\nIn most cases, it takes 3 operations to deliver a message: 1 write, 1 read, and 1 delete. Therefore, you can use the following formula to estimate your monthly bill:",
      "language": "unknown"
    },
    {
      "code": "Additionally:\n\n* Each retry incurs a read operation. A batch of 10 messages that is retried would incur 10 operations for each retry.\n* Messages that reach the maximum retries and that are written to a [Dead Letter Queue](https://developers.cloudflare.com/queues/configuration/batching-retries/) incur a write operation for each 64 KB chunk. A message that was retried 3 times (the default), fails delivery on the fourth time and is written to a Dead Letter Queue would incur five (5) read operations.\n* Messages that are written to a queue, but that reach the maximum persistence duration (or \"expire\") before they are read, incur only a write and delete operation per 64 KB chunk.\n\n## Examples\n\nIf an application writes, reads and deletes (consumes) one million messages a day (in a 30 day month), and each message is less than 64 KB in size, the estimated bill for the month would be:\n\n| | Total Usage | Free Usage | Billed Usage | Price |\n| - | - | - | - | - |\n| Standard operations | 3 \\* 30 \\* 1,000,000 | 1,000,000 | 89,000,000 | $35.60 |\n| | (write, read, delete) | | | |\n| **TOTAL** | | | | **$35.60** |\n\nAn application that writes, reads and deletes (consumes) 100 million \\~127 KB messages (each message counts as two 64 KB chunks) per month would have an estimated bill resembling the following:\n\n| | Total Usage | Free Usage | Billed Usage | Price |\n| - | - | - | - | - |\n| Standard operations | 2 \\* 3 \\* 100 \\* 1,000,000 | 1,000,000 | 599,000,000 | $239.60 |\n| | (2x ops for > 64KB messages) | | | |\n| **TOTAL** | | | | **$239.60** |\n\n</page>\n\n<page>\n---\ntitle: Choose a data or storage product ¬∑ Cloudflare Queues docs\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/storage-options/\n  md: https://developers.cloudflare.com/queues/platform/storage-options/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Delivery guarantees ¬∑ Cloudflare Queues docs\ndescription: Delivery guarantees define how strongly a messaging system enforces\n  the delivery of messages it processes.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/reference/delivery-guarantees/\n  md: https://developers.cloudflare.com/queues/reference/delivery-guarantees/index.md\n---\n\nDelivery guarantees define how strongly a messaging system enforces the delivery of messages it processes.\n\nAs you make stronger guarantees about message delivery, the system needs to perform more checks and acknowledgments to ensure that messages are delivered, or maintain state to ensure a message is only delivered the specified number of times. This increases the latency of the system and reduces the overall throughput of the system. Each message may require an additional internal acknowledgements, and an equivalent number of additional roundtrips, before it can be considered delivered.\n\n* **Queues provides *at least once* delivery by default** in order to optimize for reliability.\n* This means that messages are guaranteed to be delivered at least once, and in rare occasions, may be delivered more than once.\n* For the majority of applications, this is the right balance between not losing any messages and minimizing end-to-end latency, as exactly once delivery incurs additional overheads in any messaging system.\n\nIn cases where processing the same message more than once would introduce unintended behaviour, generating a unique ID when writing the message to the queue and using that as the primary key on database inserts and/or as an idempotency key to de-duplicate the message after processing. For example, using this idempotency key as the ID in an upstream email API or payment API will allow those services to reject the duplicate on your behalf, without you having to carry additional state in your application.\n\n</page>\n\n<page>\n---\ntitle: How Queues Works ¬∑ Cloudflare Queues docs\ndescription: Cloudflare Queues is a flexible messaging queue that allows you to\n  queue messages for asynchronous processing. Message queues are great at\n  decoupling components of applications, like the checkout and order fulfillment\n  services for an e-commerce site. Decoupled services are easier to reason\n  about, deploy, and implement, allowing you to ship features that delight your\n  customers without worrying about synchronizing complex deployments. Queues\n  also allow you to batch and buffer calls to downstream services and APIs.\nlastUpdated: 2025-02-12T13:41:31.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/reference/how-queues-works/\n  md: https://developers.cloudflare.com/queues/reference/how-queues-works/index.md\n---\n\nCloudflare Queues is a flexible messaging queue that allows you to queue messages for asynchronous processing. Message queues are great at decoupling components of applications, like the checkout and order fulfillment services for an e-commerce site. Decoupled services are easier to reason about, deploy, and implement, allowing you to ship features that delight your customers without worrying about synchronizing complex deployments. Queues also allow you to batch and buffer calls to downstream services and APIs.\n\nThere are four major concepts to understand with Queues:\n\n1. [Queues](#what-is-a-queue)\n2. [Producers](#producers)\n3. [Consumers](#consumers)\n4. [Messages](#messages)\n\n## What is a queue\n\nA queue is a buffer or list that automatically scales as messages are written to it, and allows a consumer Worker to pull messages from that same queue.\n\nQueues are designed to be reliable, and messages written to a queue should never be lost once the write succeeds. Similarly, messages are not deleted from a queue until the [consumer](#consumers) has successfully consumed the message.\n\nQueues does not guarantee that messages will be delivered to a consumer in the same order in which they are published.\n\nDevelopers can create multiple queues. Creating multiple queues can be useful to:\n\n* Separate different use-cases and processing requirements: for example, a logging queue vs. a password reset queue.\n* Horizontally scale your overall throughput (messages per second) by using multiple queues to scale out.\n* Configure different batching strategies for each consumer connected to a queue.\n\nFor most applications, a single producer Worker per queue, with a single consumer Worker consuming messages from that queue allows you to logically separate the processing for each of your queues.\n\n## Producers\n\nA producer is the term for a client that is publishing or producing messages on to a queue. A producer is configured by [binding](https://developers.cloudflare.com/workers/runtime-apis/bindings/) a queue to a Worker and writing messages to the queue by calling that binding.\n\nFor example, if we bound a queue named `my-first-queue` to a binding of `MY_FIRST_QUEUE`, messages can be written to the queue by calling `send()` on the binding:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nYou can also use [`context.waitUntil()`](https://developers.cloudflare.com/workers/runtime-apis/context/#waituntil) to send the message without blocking the response.\n\nNote that because `waitUntil()` is non-blocking, any errors raised from the `send()` or `sendBatch()` methods on a queue will be implicitly ignored.\n\nA queue can have multiple producer Workers. For example, you may have multiple producer Workers writing events or logs to a shared queue based on incoming HTTP requests from users. There is no limit to the total number of producer Workers that can write to a single queue.\n\nAdditionally, multiple queues can be bound to a single Worker. That single Worker can decide which queue to write to (or write to multiple) based on any logic you define in your code.\n\n### Content types\n\nMessages published to a queue can be published in different formats, depending on what interoperability is needed with your consumer. The default content type is `json`, which means that any object that can be passed to `JSON.stringify()` will be accepted.\n\nTo explicitly set the content type or specify an alternative content type, pass the `contentType` option to the `send()` method of your queue:",
      "language": "unknown"
    },
    {
      "code": "To only accept simple strings when writing to a queue, set `{ contentType: \"text\" }` instead:",
      "language": "unknown"
    },
    {
      "code": "The [`QueuesContentType`](https://developers.cloudflare.com/queues/configuration/javascript-apis/#queuescontenttype) API documentation describes how each format is serialized to a queue.\n\n## Consumers\n\nQueues supports two types of consumer:\n\n1. A [consumer Worker](https://developers.cloudflare.com/queues/configuration/configure-queues/), which is push-based: the Worker is invoked when the queue has messages to deliver.\n2. A [HTTP pull consumer](https://developers.cloudflare.com/queues/configuration/pull-consumers/), which is pull-based: the consumer calls the queue endpoint over HTTP to receive and then acknowledge messages.\n\nA queue can only have one type of consumer configured.\n\n### Create a consumer Worker\n\nA consumer is the term for a client that is subscribing to or *consuming* messages from a queue. In its most basic form, a consumer is defined by creating a `queue` handler in a Worker:",
      "language": "unknown"
    },
    {
      "code": "You then connect that consumer to a queue with `wrangler queues consumer <queue-name> <worker-script-name>` or by defining a `[[queues.consumers]]` configuration in your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/) manually:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Importantly, each queue can only have one active consumer. This allows Cloudflare Queues to achieve at least once delivery and minimize the risk of duplicate messages beyond that.\n\nBest practice\n\nConfigure a single consumer per queue. This both logically separates your queues, and ensures that errors (failures) in processing messages from one queue do not impact your other queues.\n\nNotably, you can use the same consumer with multiple queues. The queue handler that defines your consumer Worker will be invoked by the queues it is connected to.\n\n* The `MessageBatch` that is passed to your `queue` handler includes a `queue` property with the name of the queue the batch was read from.\n* This can reduce the amount of code you need to write, and allow you to process messages based on the name of your queues.\n\nFor example, a consumer configured to consume messages from multiple queues would resemble the following:",
      "language": "unknown"
    },
    {
      "code": "### Remove a consumer\n\nTo remove a queue from your project, run `wrangler queues consumer remove <queue-name> <script-name>` and then remove the desired queue below the `[[queues.consumers]]` in Wrangler file.\n\n### Pull consumers\n\nA queue can have a HTTP-based consumer that pulls from the queue, instead of messages being pushed to a Worker.\n\nThis consumer can be any HTTP-speaking service that can communicate over the Internet. Review the [pull consumer guide](https://developers.cloudflare.com/queues/configuration/pull-consumers/) to learn how to configure a pull-based consumer for a queue.\n\n## Messages\n\nA message is the object you are producing to and consuming from a queue.\n\nAny JSON serializable object can be published to a queue. For most developers, this means either simple strings or JSON objects. You can explicitly [set the content type](#content-types) when sending a message.\n\nMessages themselves can be [batched when delivered to a consumer](https://developers.cloudflare.com/queues/configuration/batching-retries/). By default, messages within a batch are treated as all or nothing when determining retries. If the last message in a batch fails to be processed, the entire batch will be retried. You can also choose to [explicitly acknowledge](https://developers.cloudflare.com/queues/configuration/batching-retries/) messages as they are successfully processed, and/or mark individual messages to be retried.\n\n</page>\n\n<page>\n---\ntitle: Wrangler commands ¬∑ Cloudflare Queues docs\ndescription: Queues Wrangler commands use REST APIs to interact with the control\n  plane. This page lists the Wrangler commands for Queues.\nlastUpdated: 2025-11-14T16:29:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/reference/wrangler-commands/\n  md: https://developers.cloudflare.com/queues/reference/wrangler-commands/index.md\n---\n\nQueues Wrangler commands use REST APIs to interact with the control plane. This page lists the Wrangler commands for Queues.\n\n## `queues list`\n\nList queues\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `--page` number\n\n  Page number for pagination\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues create`\n\nCreate a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\n- `--delivery-delay-secs` number default: 0\n\n  How long a published message should be delayed for, in seconds. Must be between 0 and 42300\n\n- `--message-retention-period-secs` number default: 345600\n\n  How long to retain a message in the queue, in seconds. Must be between 60 and 1209600\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues update`\n\nUpdate a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\n- `--delivery-delay-secs` number\n\n  How long a published message should be delayed for, in seconds. Must be between 0 and 42300\n\n- `--message-retention-period-secs` number\n\n  How long to retain a message in the queue, in seconds. Must be between 60 and 1209600\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues delete`\n\nDelete a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues info`\n\nGet queue information\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer add`\n\nAdd a Queue Worker Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue to configure\n\n- `[SCRIPT-NAME]` string required\n\n  Name of the consumer script\n\n- `--batch-size` number\n\n  Maximum number of messages per batch\n\n- `--batch-timeout` number\n\n  Maximum number of seconds to wait to fill a batch with messages\n\n- `--message-retries` number\n\n  Maximum number of retries for each message\n\n- `--dead-letter-queue` string\n\n  Queue to send messages that failed to be consumed\n\n- `--max-concurrency` number\n\n  The maximum number of concurrent consumer Worker invocations. Must be a positive integer\n\n- `--retry-delay-secs` number\n\n  The number of seconds to wait before retrying a message\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer remove`\n\nRemove a Queue Worker Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue to configure\n\n- `[SCRIPT-NAME]` string required\n\n  Name of the consumer script\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer http add`\n\nAdd a Queue HTTP Pull Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue for the consumer\n\n- `--batch-size` number\n\n  Maximum number of messages per batch\n\n- `--message-retries` number\n\n  Maximum number of retries for each message\n\n- `--dead-letter-queue` string\n\n  Queue to send messages that failed to be consumed\n\n- `--visibility-timeout-secs` number\n\n  The number of seconds a message will wait for an acknowledgement before being returned to the queue.\n\n- `--retry-delay-secs` number\n\n  The number of seconds to wait before retrying a message\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer http remove`\n\nRemove a Queue HTTP Pull Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue for the consumer\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer worker add`\n\nAdd a Queue Worker Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue to configure\n\n- `[SCRIPT-NAME]` string required\n\n  Name of the consumer script\n\n- `--batch-size` number\n\n  Maximum number of messages per batch\n\n- `--batch-timeout` number\n\n  Maximum number of seconds to wait to fill a batch with messages\n\n- `--message-retries` number\n\n  Maximum number of retries for each message\n\n- `--dead-letter-queue` string\n\n  Queue to send messages that failed to be consumed\n\n- `--max-concurrency` number\n\n  The maximum number of concurrent consumer Worker invocations. Must be a positive integer\n\n- `--retry-delay-secs` number\n\n  The number of seconds to wait before retrying a message\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer worker remove`\n\nRemove a Queue Worker Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue to configure\n\n- `[SCRIPT-NAME]` string required\n\n  Name of the consumer script\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues pause-delivery`\n\nPause message delivery for a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues resume-delivery`\n\nResume message delivery for a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues purge`\n\nPurge messages from a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\n- `--force` boolean\n\n  Skip the confirmation dialog and forcefully purge the Queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription create`\n\nCreate a new event subscription for a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue to create the subscription for\n\n- `--source` string required\n\n  The event source type\n\n- `--events` string required\n\n  Comma-separated list of event types to subscribe to\n\n- `--name` string\n\n  Name for the subscription (auto-generated if not provided)\n\n- `--enabled` boolean default: true\n\n  Whether the subscription should be active\n\n- `--model-name` string\n\n  Workers AI model name (required for workersAi.model source)\n\n- `--worker-name` string\n\n  Worker name (required for workersBuilds.worker source)\n\n- `--workflow-name` string\n\n  Workflow name (required for workflows.workflow source)\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription list`\n\nList event subscriptions for a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue to list subscriptions for\n\n- `--page` number default: 1\n\n  Page number for pagination\n\n- `--per-page` number default: 20\n\n  Number of subscriptions per page\n\n- `--json` boolean default: false\n\n  Output in JSON format\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription get`\n\nGet details about a specific event subscription\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue\n\n- `--id` string required\n\n  The ID of the subscription to retrieve\n\n- `--json` boolean default: false\n\n  Output in JSON format\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription delete`\n\nDelete an event subscription from a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue\n\n- `--id` string required\n\n  The ID of the subscription to delete\n\n- `--force` boolean alias: --y default: false\n\n  Skip confirmation\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription update`\n\nUpdate an existing event subscription\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue\n\n- `--id` string required\n\n  The ID of the subscription to update\n\n- `--name` string\n\n  New name for the subscription\n\n- `--events` string\n\n  Comma-separated list of event types to subscribe to\n\n- `--enabled` boolean\n\n  Whether the subscription should be active\n\n- `--json` boolean default: false\n\n  Output in JSON format\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Queues & Rate Limits ¬∑ Cloudflare Queues docs\ndescription: Example of how to use Queues to handle rate limits of external APIs.\nlastUpdated: 2025-10-13T13:40:40.000Z\nchatbotDeprioritize: false\ntags: TypeScript\nsource_url:\n  html: https://developers.cloudflare.com/queues/tutorials/handle-rate-limits/\n  md: https://developers.cloudflare.com/queues/tutorials/handle-rate-limits/index.md\n---\n\nThis tutorial explains how to use Queues to handle rate limits of external APIs by building an application that sends email notifications using [Resend](https://www.resend.com/). However, you can use this pattern to handle rate limits of any external API.\n\nResend is a service that allows you to send emails from your application via an API. Resend has a default [rate limit](https://resend.com/docs/api-reference/introduction#rate-limit) of two requests per second. You will use Queues to handle the rate limit of Resend.\n\n## Prerequisites\n\n1. Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages).\n2. Install [`Node.js`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).\n\nNode.js version manager\n\nUse a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm) to avoid permission issues and change Node.js versions. [Wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/), discussed later in this guide, requires a Node version of `16.17.0` or later.\n\n1. Sign up for [Resend](https://resend.com/) and generate an API key by following the guide on the [Resend documentation](https://resend.com/docs/dashboard/api-keys/introduction).\n\n2. Additionally, you will need access to Cloudflare Queues.\n\nQueues are included in the monthly subscription cost of your Workers Paid plan, and charges based on operations against your queues. Refer to [Pricing](https://developers.cloudflare.com/queues/platform/pricing/) for more details.\n\nBefore you can use Queues, you must enable it via [the Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/workers/queues). You need a Workers Paid plan to enable Queues.\n\nTo enable Queues:\n\n1. In the Cloudflare dashboard, go to the **Queues** page.\n\n   [Go to **Queues**](https://dash.cloudflare.com/?to=/:account/workers/queues)\n\n2. Select **Enable Queues**.\n\n## 1. Create a new Workers application\n\nTo get started, create a Worker application using the [`create-cloudflare` CLI](https://github.com/cloudflare/workers-sdk/tree/main/packages/create-cloudflare). Open a terminal window and run the following command:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "For setup, select the following options:\n\n* For *What would you like to start with?*, choose `Hello World example`.\n* For *Which template would you like to use?*, choose `Worker only`.\n* For *Which language do you want to use?*, choose `TypeScript`.\n* For *Do you want to use git for version control?*, choose `Yes`.\n* For *Do you want to deploy your application?*, choose `No` (we will be making some changes before deploying).\n\nThen, go to your newly created directory:",
      "language": "unknown"
    },
    {
      "code": "## 2. Set up a Queue\n\nYou need to create a Queue and a binding to your Worker. Run the following command to create a Queue named `rate-limit-queue`:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### Add Queue bindings to your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/)\n\nIn your Wrangler file, add the following:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "It is important to include the `max_batch_size` of two to the consumer queue is important because the Resend API has a default rate limit of two requests per second. This batch size allows the queue to process the message in the batch size of two. If the batch size is less than two, the queue will wait for 10 seconds to collect the next message. If no more messages are available, the queue will process the message in the batch. For more information, refer to the [Batching, Retries and Delays documentation](https://developers.cloudflare.com/queues/configuration/batching-retries)\n\nYour final Wrangler file should look similar to the example below.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 3. Add bindings to environment\n\nAdd the bindings to the environment interface in `worker-configuration.d.ts`, so TypeScript correctly types the bindings. Type the queue as `Queue<any>`. Refer to the following step for instructions on how to change this type.",
      "language": "unknown"
    },
    {
      "code": "## 4. Send message to the queue\n\nThe application will send a message to the queue when the Worker receives a request. For simplicity, you will send the email address as a message to the queue. A new message will be sent to the queue with a delay of one second.",
      "language": "unknown"
    },
    {
      "code": "This will accept requests to any subpath and forwards the request's body. It expects that the request body to contain only an email. In production, you should check that the request was a `POST` request. You should also avoid sending such sensitive information (email) directly to the queue. Instead, you can send a message to the queue that contains a unique identifier for the user. Then, your consumer queue can use the unique identifier to look up the email address in a database and use that to send the email.\n\n## 5. Process the messages in the queue\n\nAfter the message is sent to the queue, it will be processed by the consumer Worker. The consumer Worker will process the message and send the email.\n\nSince you have not configured Resend yet, you will log the message to the console. After you configure Resend, you will use it to send the email.\n\nAdd the `queue()` handler as shown below:",
      "language": "unknown"
    },
    {
      "code": "The above `queue()` handler will log the email address to the console and send the email. It will also retry the message if sending the email fails. The `delaySeconds` is set to five seconds to avoid sending the email too quickly.\n\nTo test the application, run the following command:",
      "language": "unknown"
    },
    {
      "code": "Use the following cURL command to send a request to the application:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## 6. Set up Resend\n\nTo call the Resend API, you need to configure the Resend API key. Create a `.dev.vars` file in the root of your project and add the following:",
      "language": "unknown"
    },
    {
      "code": "Replace `your-resend-api-key` with your actual Resend API key.\n\nNext, update the `Env` interface in `worker-configuration.d.ts` to include the `RESEND_API_KEY` variable.",
      "language": "unknown"
    },
    {
      "code": "Lastly, install the [`resend` package](https://www.npmjs.com/package/resend) using the following command:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "You can now use the `RESEND_API_KEY` variable in your code.\n\n## 7. Send email with Resend\n\nIn your `src/index.ts` file, import the Resend package and update the `queue()` handler to send the email.",
      "language": "unknown"
    },
    {
      "code": "The `queue()` handler will now send the email using the Resend API. It also checks if sending the email failed and will retry the message.\n\nThe final script is included below:",
      "language": "unknown"
    },
    {
      "code": "To test the application, start the development server using the following command:",
      "language": "unknown"
    },
    {
      "code": "Use the following cURL command to send a request to the application:",
      "language": "unknown"
    },
    {
      "code": "On the Resend dashboard, you should see that the email was sent to the provided email address.\n\n## 8. Deploy your Worker\n\nTo deploy your Worker, run the following command:",
      "language": "unknown"
    },
    {
      "code": "Lastly, add the Resend API key using the following command:",
      "language": "unknown"
    },
    {
      "code": "Enter the value of your API key. Your API key will get added to your project. You can now use the `RESEND_API_KEY` variable in your code.\n\nYou have successfully created a Worker which can send emails using the Resend API respecting rate limits.\n\nTo test your Worker, you could use the following cURL request. Replace `<YOUR_WORKER_URL>` with the URL of your deployed Worker.",
      "language": "unknown"
    },
    {
      "code": "Refer to the [GitHub repository](https://github.com/harshil1712/queues-rate-limit) for the complete code for this tutorial. If you are using [Hono](https://hono.dev/), you can refer to the [Hono example](https://github.com/harshil1712/resend-rate-limit-demo).\n\n## Related resources\n\n* [How Queues works](https://developers.cloudflare.com/queues/reference/how-queues-works/)\n* [Queues Batching and Retries](https://developers.cloudflare.com/queues/configuration/batching-retries/)\n* [Resend](https://resend.com/docs/)\n\n</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Queues & Browser Rendering ¬∑ Cloudflare Queues docs\ndescription: Example of how to use Queues and Browser Rendering to power a web crawler.\nlastUpdated: 2025-11-06T19:11:47.000Z\nchatbotDeprioritize: false\ntags: TypeScript\nsource_url:\n  html: https://developers.cloudflare.com/queues/tutorials/web-crawler-with-browser-rendering/\n  md: https://developers.cloudflare.com/queues/tutorials/web-crawler-with-browser-rendering/index.md\n---\n\nThis tutorial explains how to build and deploy a web crawler with Queues, [Browser Rendering](https://developers.cloudflare.com/browser-rendering/), and [Puppeteer](https://developers.cloudflare.com/browser-rendering/puppeteer/).\n\nPuppeteer is a high-level library used to automate interactions with Chrome/Chromium browsers. On each submitted page, the crawler will find the number of links to `cloudflare.com` and take a screenshot of the site, saving results to [Workers KV](https://developers.cloudflare.com/kv/).\n\nYou can use Puppeteer to request all images on a page, save the colors used on a site, and more.\n\n## Prerequisites\n\n1. Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages).\n2. Install [`Node.js`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).\n\nNode.js version manager\n\nUse a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm) to avoid permission issues and change Node.js versions. [Wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/), discussed later in this guide, requires a Node version of `16.17.0` or later.\n\n## 1. Create new Workers application\n\nTo get started, create a Worker application using the [`create-cloudflare` CLI](https://github.com/cloudflare/workers-sdk/tree/main/packages/create-cloudflare). Open a terminal window and run the following command:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "For setup, select the following options:\n\n* For *What would you like to start with?*, choose `Hello World example`.\n* For *Which template would you like to use?*, choose `Worker only`.\n* For *Which language do you want to use?*, choose `TypeScript`.\n* For *Do you want to use git for version control?*, choose `Yes`.\n* For *Do you want to deploy your application?*, choose `No` (we will be making some changes before deploying).\n\nThen, move into your newly created directory:",
      "language": "unknown"
    },
    {
      "code": "## 2. Create KV namespace\n\nWe need to create a KV store. This can be done through the Cloudflare dashboard or the Wrangler CLI. For this tutorial, we will use the Wrangler CLI.\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "- npm",
      "language": "unknown"
    },
    {
      "code": "- yarn",
      "language": "unknown"
    },
    {
      "code": "- pnpm",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### Add KV bindings to the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/)\n\nThen, in your Wrangler file, add the following with the values generated in the terminal:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 3. Set up Browser Rendering\n\nNow, you need to set up your Worker for Browser Rendering.\n\nIn your current directory, install Cloudflare's [fork of Puppeteer](https://developers.cloudflare.com/browser-rendering/puppeteer/) and also [robots-parser](https://www.npmjs.com/package/robots-parser):\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "- npm",
      "language": "unknown"
    },
    {
      "code": "- yarn",
      "language": "unknown"
    },
    {
      "code": "- pnpm",
      "language": "unknown"
    },
    {
      "code": "Then, add a Browser Rendering binding. Adding a Browser Rendering binding gives the Worker access to a headless Chromium instance you will control with Puppeteer.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 4. Set up a Queue\n\nNow, we need to set up the Queue.\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### Add Queue bindings to wrangler.toml\n\nThen, in your Wrangler file, add the following:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Adding the `max_batch_timeout` of 60 seconds to the consumer queue is important because Browser Rendering has a limit of two new browsers per minute per account. This timeout waits up to a minute before collecting queue messages into a batch. The Worker will then remain under this browser invocation limit.\n\nYour final Wrangler file should look similar to the one below.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 5. Add bindings to environment\n\nAdd the bindings to the environment interface in `src/index.ts`, so TypeScript correctly types the bindings. Type the queue as `Queue<any>`. The following step will show you how to change this type.",
      "language": "unknown"
    },
    {
      "code": "## 6. Submit links to crawl\n\nAdd a `fetch()` handler to the Worker to submit links to crawl.",
      "language": "unknown"
    },
    {
      "code": "This will accept requests to any subpath and forwards the request's body to be crawled. It expects that the request body only contains a URL. In production, you should check that the request was a `POST` request and contains a well-formed URL in its body. This has been omitted for simplicity.\n\n## 7. Crawl with Puppeteer\n\nAdd a `queue()` handler to the Worker to process the links you send.",
      "language": "unknown"
    },
    {
      "code": "This is a skeleton for the crawler. It launches the Puppeteer browser and iterates through the Queue's received messages. It fetches the site's `robots.txt` and uses `robots-parser` to check that this site allows crawling. If crawling is not allowed, the message is `ack`'ed, removing it from the Queue. If crawling is allowed, you can continue to crawl the site.\n\nThe `puppeteer.launch()` is wrapped in a `try...catch` to allow the whole batch to be retried if the browser launch fails. The browser launch may fail due to going over the limit for number of browsers per account.",
      "language": "unknown"
    },
    {
      "code": "This helper function opens a new page in Puppeteer and navigates to the provided URL. `numCloudflareLinks` uses Puppeteer's `$$eval` (equivalent to `document.querySelectorAll`) to find the number of links to a `cloudflare.com` page. Checking if the link's `href` is to a `cloudflare.com` page is wrapped in a `try...catch` to handle cases where `href`s may not be URLs.\n\nThen, the function sets the browser viewport size and takes a screenshot of the full page. The screenshot is returned as a `Buffer` so it can be converted to an `ArrayBuffer` and written to KV.\n\nTo enable recursively crawling links, add a snippet after checking the number of Cloudflare links to send messages recursively from the queue consumer to the queue itself. Recursing too deep, as is possible with crawling, will cause a Durable Object `Subrequest depth limit exceeded.` error. If one occurs, it is caught, but the links are not retried.",
      "language": "unknown"
    },
    {
      "code": "Then, in the `queue` handler, call `crawlPage` on the URL.",
      "language": "unknown"
    },
    {
      "code": "This snippet saves the results from `crawlPage` into the appropriate KV namespaces. If an unexpected error occurred, the URL will be retried and resent to the queue again.\n\nSaving the timestamp of the crawl in KV helps you avoid crawling too frequently.\n\nAdd a snippet before checking `robots.txt` to check KV for a crawl within the last hour. This lists all KV keys beginning with the same URL (crawls of the same page), and check if any crawls have been done within the last hour. If any crawls have been done within the last hour, the message is `ack`'ed and not retried.",
      "language": "unknown"
    },
    {
      "code": "The final script is included below.",
      "language": "unknown"
    },
    {
      "code": "## 8. Deploy your Worker\n\nTo deploy your Worker, run the following command:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "You have successfully created a Worker which can submit URLs to a queue for crawling and save results to Workers KV.\n\nTo test your Worker, you could use the following cURL request to take a screenshot of this documentation page.",
      "language": "unknown"
    },
    {
      "code": "Refer to the [GitHub repository for the complete tutorial](https://github.com/cloudflare/queues-web-crawler), including a front end deployed with Pages to submit URLs and view crawler results.\n\n## Related resources\n\n* [How Queues works](https://developers.cloudflare.com/queues/reference/how-queues-works/)\n* [Queues Batching and Retries](https://developers.cloudflare.com/queues/configuration/batching-retries/)\n* [Browser Rendering](https://developers.cloudflare.com/browser-rendering/)\n* [Puppeteer Examples](https://github.com/puppeteer/puppeteer/tree/main/examples)\n\n</page>\n\n<page>\n---\ntitle: R2 SQL - Pricing ¬∑ R2 SQL docs\ndescription: R2 SQL is in open beta and available to any developer with an R2 subscription.\nlastUpdated: 2025-09-25T04:13:57.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2-sql/platform/pricing/\n  md: https://developers.cloudflare.com/r2-sql/platform/pricing/index.md\n---\n\nR2 SQL is in open beta and available to any developer with an [R2 subscription](https://developers.cloudflare.com/r2/pricing/).\n\nWe are not currently billing for R2 SQL during open beta. However, you will be billed for standard [R2 storage and operations](https://developers.cloudflare.com/r2/pricing/) for data accessed by queries.\n\nWe plan to bill based on the volume of data queried by R2 SQL. We'll provide at least 30 days notice before we make any changes or start charging for R2 SQL usage.\n\n</page>\n\n<page>\n---\ntitle: Limitations and best practices ¬∑ R2 SQL docs\ndescription: R2 SQL is designed for querying partitioned Apache Iceberg tables\n  in your R2 data catalog. This document outlines the supported features,\n  limitations, and best practices of R2 SQL.\nlastUpdated: 2025-12-12T16:58:55.000Z\nchatbotDeprioritize: false\ntags: SQL\nsource_url:\n  html: https://developers.cloudflare.com/r2-sql/reference/limitations-best-practices/\n  md: https://developers.cloudflare.com/r2-sql/reference/limitations-best-practices/index.md\n---\n\nNote\n\nR2 SQL is in open beta. Limitations and best practices will change over time.\n\nR2 SQL is designed for querying **partitioned** Apache Iceberg tables in your R2 data catalog. This document outlines the supported features, limitations, and best practices of R2 SQL.\n\n## Quick Reference\n\n| Feature | Supported | Notes |\n| - | - | - |\n| Basic SELECT | Yes | Columns, \\* |\n| Aggregation functions | Yes | COUNT(\\*), SUM, AVG, MIN, MAX |\n| Single table FROM | Yes | Note, aliasing not supported |\n| WHERE clause | Yes | Filters, comparisons, equality, etc |\n| JOINs | No | No table joins |\n| Array filtering | No | No array type support |\n| JSON filtering | No | No nested object queries |\n| Simple LIMIT | Yes | 1-10,000 range, no pagination support |\n| ORDER BY | Yes | Partition key or with GROUP BY columns |\n| GROUP BY | Yes | Supported |\n| HAVING | Yes | Supported |\n\n## Supported SQL Clauses\n\nR2 SQL supports: `DESCRIBE`, `SHOW`, `SELECT`, `FROM`, `WHERE`, `GROUP BY`, `HAVING`, `ORDER BY`, and `LIMIT`. New features will be released in the future, keep an eye on this page for the latest.\n\n***\n\n## SELECT Clause\n\n### Supported Features\n\n* **Individual columns**: `SELECT column1, column2`\n* **All columns**: `SELECT *`\n\n### Limitations\n\n* **No JSON field querying**: Cannot query individual fields from JSON objects\n* **Limited aggregation functions**: See Aggregation Functions section below for details\n* **No synthetic data**: Cannot create synthetic columns like `SELECT 1 AS what, \"hello\" AS greeting`\n* **No field aliasing**: `SELECT field AS another_name` (applies to both regular columns and aggregations)\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## Aggregation Functions\n\n### Supported Features\n\n* **COUNT(\\*)**: Count total rows **note**: only `*` is supported\n* **SUM(column)**: Sum numeric values\n* **AVG(column)**: Calculate average of numeric values\n* **MIN(column)**: Find minimum value\n* **MAX(column)**: Find maximum value\n* **With GROUP BY**: All aggregations work with `GROUP BY`\n\n### Limitations\n\n* **No aliases**: `AS` keyword not supported (`SELECT COUNT(*) AS total` fails)\n* **COUNT(\\*) only**: `COUNT(column_name)` or `COUNT(DISTINCT column)` is not supported\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## GROUP BY Clause\n\n### Supported Features\n\n* **Single column grouping**: `GROUP BY column`\n* **Multiple column grouping**: `GROUP BY column1, column2`\n* **With WHERE**: Filter before grouping\n* **With LIMIT**: Limit grouped results\n\n### Limitations\n\n* **No expressions**: Cannot use expressions in GROUP BY (e.g., `GROUP BY YEAR(date)`)\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## HAVING Clause\n\n### Supported Features\n\n* **With COUNT(\\*)**: Filter groups by count\n* **Comparison operators**: `>`, `>=`, `=`, `<`, `<=`, `!=`, `BETWEEN`, `AND`, `IS NOT NULL`\n* **With GROUP BY**: Must be used with GROUP BY\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## FROM Clause\n\n### Supported Features\n\n* **Single table queries**: `SELECT * FROM table_name`\n\n### Limitations\n\n* **No multiple tables**: Cannot specify multiple tables in FROM clause\n* **No subqueries**: `SELECT ... FROM (SELECT ...)` is not supported\n* **No JOINs**: No INNER, LEFT, RIGHT, or FULL JOINs\n* **No SQL functions**: Cannot use functions like `read_parquet()`\n* **No synthetic tables**: Cannot create tables from values\n* **No schema evolution**: Schema cannot be altered (no ALTER TABLE, migrations)\n* **Immutable datasets**: No UPDATE or DELETE operations allowed\n* **Fully defined schema**: Dynamic or union-type fields are not supported\n* **No table aliasing**: `SELECT * FROM table_name AS alias`\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## WHERE Clause\n\n### Supported Features\n\n* **Simple type filtering**: Supports `string`, `boolean`, `number` types, and timestamps expressed as RFC3339\n* **Boolean logic**: Supports `AND`, `OR`, `NOT` operators\n* **Comparison operators**: `>`, `>=`, `=`, `<`, `<=`, `!=`\n* **Grouped conditions**: `WHERE col_a=\"hello\" AND (col_b>5 OR col_c != 3)`\n* **Pattern matching:** `WHERE col_a LIKE ‚Äòhello w%‚Äô` (prefix matching only)\n* **NULL Handling :** `WHERE col_a IS NOT NULL` (`IS`/`IS NOT`)\n\n### Limitations\n\n* **No column-to-column comparisons**: Cannot use `WHERE col_a = col_b`\n* **No array filtering**: Cannot filter on array types (array\\[number], array\\[string], array\\[boolean])\n* **No JSON/object filtering**: Cannot filter on fields inside nested objects or JSON\n* **No SQL functions**: No function calls in WHERE clause\n* **No arithmetic operators**: Cannot use `+`, `-`, `*`, `/` in conditions\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## ORDER BY Clause\n\n### Supported Features\n\n* **ASC**: Ascending order\n* **DESC**: Descending order (Default, on full partition key)\n* **With partition key**: Order by partition key columns\n* **With GROUP BY**: Can order by all aggregation columns\n\n### Limitations\n\n* **Non-partition keys not supported**: `ORDER BY` on columns other than the partition key is not supported (except with aggregations)\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## LIMIT Clause\n\n### Supported Features\n\n* **Simple limits**: `LIMIT number`\n* **Range**: Minimum 1, maximum 10,000\n\n### Limitations\n\n* **No pagination**: `LIMIT offset, count` syntax not supported\n* **No SQL functions**: Cannot use functions to determine limit\n* **No arithmetic**: Cannot use expressions like `LIMIT 10 * 50`\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## Unsupported SQL Clauses\n\nThe following SQL clauses are **not supported**:\n\n* `UNION`/`INTERSECT`/`EXCEPT`\n* `WITH` (Common Table Expressions)\n* `WINDOW` functions\n* `INSERT`/`UPDATE`/`DELETE`\n* `CREATE`/`ALTER`/`DROP`\n\n***\n\n## Best Practices\n\n1. Always include time filters in your WHERE clause to ensure efficient queries.\n2. Use specific column selection instead of `SELECT *` when possible for better performance.\n3. Flatten your data to avoid nested JSON objects if you need to filter on those fields.\n4. Use `COUNT(*)` exclusively - avoid `COUNT(column_name)` or `COUNT(DISTINCT column)`.\n5. Enable compaction in R2 Data Catalog to reduce the number of data files needed to be scanned.\n\n***\n\n</page>\n\n<page>\n---\ntitle: Build an end to end data pipeline ¬∑ R2 SQL docs\ndescription: This tutorial demonstrates how to build a complete data pipeline\n  using Cloudflare Pipelines, R2 Data Catalog, and R2 SQL.\nlastUpdated: 2025-11-17T14:08:01.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2-sql/tutorials/end-to-end-pipeline/\n  md: https://developers.cloudflare.com/r2-sql/tutorials/end-to-end-pipeline/index.md\n---\n\nIn this tutorial, you will learn how to build a complete data pipeline using Cloudflare Pipelines, R2 Data Catalog, and R2 SQL. This also includes a sample Python script that creates and sends financial transaction data to your Pipeline that can be queried by R2 SQL or any Apache Iceberg-compatible query engine.\n\nThis tutorial demonstrates how to:\n\n* Set up R2 Data Catalog to store our transaction events in an Apache Iceberg table\n* Set up a Cloudflare Pipeline\n* Create transaction data with fraud patterns to send to your Pipeline\n* Query your data using R2 SQL for fraud analysis\n\n## Prerequisites\n\n1. Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up).\n2. Install [Node.js](https://nodejs.org/en/).\n3. Install [Python 3.8+](https://python.org) for the data generation script.\n\nNode.js version manager\n\nUse a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm) to avoid permission issues and change Node.js versions.\n\nWrangler requires a Node version of 16.17.0 or later.\n\n## 1. Set up authentication\n\nYou will need API tokens to interact with Cloudflare services.\n\n1. In the Cloudflare dashboard, go to the **API tokens** page.\n\n   [Go to **Account API tokens**](https://dash.cloudflare.com/?to=/:account/api-tokens)\n\n2. Select **Create Token**.\n\n3. Select **Get started** next to Create Custom Token.\n\n4. Enter a name for your API token.\n\n5. Under **Permissions**, choose:\n\n   * **Workers Pipelines** with Read, Send, and Edit permissions\n   * **Workers R2 Data Catalog** with Read and Edit permissions\n   * **Workers R2 SQL** with Read permissions\n   * **Workers R2 Storage** with Read and Edit permissions\n\n6. Optionally, add a TTL to this token.\n\n7. Select **Continue to summary**.\n\n8. Click **Create Token**\n\n9. Note the **Token value**.\n\nExport your new token as an environment variable:",
      "language": "unknown"
    },
    {
      "code": "If this is your first time using Wrangler, make sure to log in.",
      "language": "unknown"
    },
    {
      "code": "## 2. Create an R2 bucket and enable R2 Data Catalog\n\n* Wrangler CLI\n\n  Create an R2 bucket:",
      "language": "unknown"
    },
    {
      "code": "* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select **Create bucket**.\n\n  3. Enter the bucket name: `fraud-pipeline`\n\n  4. Select **Create bucket**.\n\nEnable the catalog on your R2 bucket:\n\n* Wrangler CLI",
      "language": "unknown"
    },
    {
      "code": "When you run this command, take note of the \"Warehouse\" and \"Catalog URI\". You will need these later.\n\n* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket: `fraud-pipeline`.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, and select **Enable**.\n\n  4. Once enabled, note the **Catalog URI** and **Warehouse name**.\n\nNote\n\nCopy the `warehouse` (ACCOUNTID\\_BUCKETNAME) and paste it in the `export` below. We will use it later in the tutorial.",
      "language": "unknown"
    },
    {
      "code": "### (Optional) Enable compaction on your R2 Data Catalog\n\nR2 Data Catalog can automatically compact tables for you. In production event streaming use cases, it is common to end up with many small files, so it is recommended to enable compaction. Since the tutorial only demonstrates a sample use case, this step is optional.\n\n* Wrangler CLI",
      "language": "unknown"
    },
    {
      "code": "* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket: `fraud-pipeline`.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, click on edit icon, and select **Enable**.\n\n  4. You can choose a target file size or leave the default. Click save.\n\n## 3. Set up the pipeline infrastructure\n\n### 3.1. Create the Pipeline stream\n\n* Wrangler CLI\n\n  First, create a schema file called `raw_transactions_schema.json` with the following `json` schema:",
      "language": "unknown"
    },
    {
      "code": "Create a stream to receive incoming fraud detection events:",
      "language": "unknown"
    },
    {
      "code": "Note\n\n  Note the **HTTP Ingest Endpoint URL** from the output. This is the endpoint you will use to send data to your pipeline.",
      "language": "unknown"
    },
    {
      "code": "The output should look like this:",
      "language": "unknown"
    },
    {
      "code": "### 3.2. Create the data sink\n\n  Create a sink that writes data to your R2 bucket as Apache Iceberg tables:",
      "language": "unknown"
    },
    {
      "code": "Note\n\n  This creates a `sink` configuration that will write to the Iceberg table `fraud_detection.transactions` in your R2 Data Catalog every 30 seconds. Pipelines automatically appends an `__ingest_ts` column that is used to partition the table by `DAY`.\n\n  ### 3.3. Create the pipeline\n\n  Connect your stream to your sink with SQL:",
      "language": "unknown"
    },
    {
      "code": "* Dashboard\n\n  1. In the Cloudflare dashboard, go to **Pipelines** > **Pipelines**.\n\n     [Go to **Pipelines**](https://dash.cloudflare.com/?to=/:account/pipelines/overview)\n\n  2. Select **Create Pipeline**.\n\n  3. **Connect to a Stream**:\n\n     * Pipeline name: `raw_events`\n     * Enable HTTP endpoint for sending data: Enabled\n     * HTTP authentication: Disabled (default)\n     * Select **Next**\n\n  4. **Define Input Schema**:\n\n     * Select **JSON editor**\n\n     * Copy in the schema:",
      "language": "unknown"
    },
    {
      "code": "* Select **Next**\n\n  5. **Define Sink**:\n\n     * Select your R2 bucket: `fraud-pipeline`\n     * Storage type: **R2 Data Catalog**\n     * Namespace: `fraud_detection`\n     * Table name: `transactions`\n     * **Advanced Settings**: Change **Maximum Time Interval** to `30 seconds`\n     * Select **Next**\n\n  6. **Credentials**:\n\n     * Disable **Automatically create an Account API token for your sink**\n     * Enter **Catalog Token** from step 1\n     * Select **Next**\n\n  7. **Pipeline Definition**:\n\n     * Leave the default SQL query:",
      "language": "unknown"
    },
    {
      "code": "* Select **Create Pipeline**\n\n  8. After pipeline creation, note the **Stream ID** for the next step.\n\n## 4. Generate sample fraud detection data\n\nCreate a Python script to generate realistic transaction data with fraud patterns:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Metrics",
      "id": "metrics"
    },
    {
      "level": "h3",
      "text": "Backlog",
      "id": "backlog"
    },
    {
      "level": "h3",
      "text": "Consumer concurrency",
      "id": "consumer-concurrency"
    },
    {
      "level": "h3",
      "text": "Message operations",
      "id": "message-operations"
    },
    {
      "level": "h2",
      "text": "Example GraphQL Queries",
      "id": "example-graphql-queries"
    },
    {
      "level": "h3",
      "text": "Get average queue backlog over time period",
      "id": "get-average-queue-backlog-over-time-period"
    },
    {
      "level": "h3",
      "text": "Get average consumer concurrency by hour",
      "id": "get-average-consumer-concurrency-by-hour"
    },
    {
      "level": "h3",
      "text": "Get message operations by minute",
      "id": "get-message-operations-by-minute"
    },
    {
      "level": "h2",
      "text": "Viewing audit logs",
      "id": "viewing-audit-logs"
    },
    {
      "level": "h2",
      "text": "Logged operations",
      "id": "logged-operations"
    },
    {
      "level": "h2",
      "text": "2025-04-17",
      "id": "2025-04-17"
    },
    {
      "level": "h2",
      "text": "2025-03-27",
      "id": "2025-03-27"
    },
    {
      "level": "h2",
      "text": "2025-02-14",
      "id": "2025-02-14"
    },
    {
      "level": "h2",
      "text": "2024-09-26",
      "id": "2024-09-26"
    },
    {
      "level": "h2",
      "text": "2024-03-26",
      "id": "2024-03-26"
    },
    {
      "level": "h2",
      "text": "2024-03-25",
      "id": "2024-03-25"
    },
    {
      "level": "h2",
      "text": "2024-03-18",
      "id": "2024-03-18"
    },
    {
      "level": "h2",
      "text": "2024-02-24",
      "id": "2024-02-24"
    },
    {
      "level": "h2",
      "text": "2023-10-07",
      "id": "2023-10-07"
    },
    {
      "level": "h2",
      "text": "2023-10-05",
      "id": "2023-10-05"
    },
    {
      "level": "h2",
      "text": "2023-03-28",
      "id": "2023-03-28"
    },
    {
      "level": "h2",
      "text": "2023-03-15",
      "id": "2023-03-15"
    },
    {
      "level": "h2",
      "text": "2023-03-02",
      "id": "2023-03-02"
    },
    {
      "level": "h2",
      "text": "2023-03-01",
      "id": "2023-03-01"
    },
    {
      "level": "h2",
      "text": "2022-12-13",
      "id": "2022-12-13"
    },
    {
      "level": "h2",
      "text": "2022-12-12",
      "id": "2022-12-12"
    },
    {
      "level": "h3",
      "text": "Increasing Queue Consumer Worker CPU Limits",
      "id": "increasing-queue-consumer-worker-cpu-limits"
    },
    {
      "level": "h2",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "What is a queue",
      "id": "what-is-a-queue"
    },
    {
      "level": "h2",
      "text": "Producers",
      "id": "producers"
    },
    {
      "level": "h3",
      "text": "Content types",
      "id": "content-types"
    },
    {
      "level": "h2",
      "text": "Consumers",
      "id": "consumers"
    },
    {
      "level": "h3",
      "text": "Create a consumer Worker",
      "id": "create-a-consumer-worker"
    },
    {
      "level": "h3",
      "text": "Remove a consumer",
      "id": "remove-a-consumer"
    },
    {
      "level": "h3",
      "text": "Pull consumers",
      "id": "pull-consumers"
    },
    {
      "level": "h2",
      "text": "Messages",
      "id": "messages"
    },
    {
      "level": "h2",
      "text": "`queues list`",
      "id": "`queues-list`"
    },
    {
      "level": "h2",
      "text": "`queues create`",
      "id": "`queues-create`"
    },
    {
      "level": "h2",
      "text": "`queues update`",
      "id": "`queues-update`"
    },
    {
      "level": "h2",
      "text": "`queues delete`",
      "id": "`queues-delete`"
    },
    {
      "level": "h2",
      "text": "`queues info`",
      "id": "`queues-info`"
    },
    {
      "level": "h2",
      "text": "`queues consumer add`",
      "id": "`queues-consumer-add`"
    },
    {
      "level": "h2",
      "text": "`queues consumer remove`",
      "id": "`queues-consumer-remove`"
    },
    {
      "level": "h2",
      "text": "`queues consumer http add`",
      "id": "`queues-consumer-http-add`"
    },
    {
      "level": "h2",
      "text": "`queues consumer http remove`",
      "id": "`queues-consumer-http-remove`"
    },
    {
      "level": "h2",
      "text": "`queues consumer worker add`",
      "id": "`queues-consumer-worker-add`"
    },
    {
      "level": "h2",
      "text": "`queues consumer worker remove`",
      "id": "`queues-consumer-worker-remove`"
    },
    {
      "level": "h2",
      "text": "`queues pause-delivery`",
      "id": "`queues-pause-delivery`"
    },
    {
      "level": "h2",
      "text": "`queues resume-delivery`",
      "id": "`queues-resume-delivery`"
    },
    {
      "level": "h2",
      "text": "`queues purge`",
      "id": "`queues-purge`"
    },
    {
      "level": "h2",
      "text": "`queues subscription create`",
      "id": "`queues-subscription-create`"
    },
    {
      "level": "h2",
      "text": "`queues subscription list`",
      "id": "`queues-subscription-list`"
    },
    {
      "level": "h2",
      "text": "`queues subscription get`",
      "id": "`queues-subscription-get`"
    },
    {
      "level": "h2",
      "text": "`queues subscription delete`",
      "id": "`queues-subscription-delete`"
    },
    {
      "level": "h2",
      "text": "`queues subscription update`",
      "id": "`queues-subscription-update`"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Create a new Workers application",
      "id": "1.-create-a-new-workers-application"
    },
    {
      "level": "h2",
      "text": "2. Set up a Queue",
      "id": "2.-set-up-a-queue"
    },
    {
      "level": "h3",
      "text": "Add Queue bindings to your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/)",
      "id": "add-queue-bindings-to-your-[wrangler-configuration-file](https://developers.cloudflare.com/workers/wrangler/configuration/)"
    },
    {
      "level": "h2",
      "text": "3. Add bindings to environment",
      "id": "3.-add-bindings-to-environment"
    },
    {
      "level": "h2",
      "text": "4. Send message to the queue",
      "id": "4.-send-message-to-the-queue"
    },
    {
      "level": "h2",
      "text": "5. Process the messages in the queue",
      "id": "5.-process-the-messages-in-the-queue"
    },
    {
      "level": "h2",
      "text": "6. Set up Resend",
      "id": "6.-set-up-resend"
    },
    {
      "level": "h2",
      "text": "7. Send email with Resend",
      "id": "7.-send-email-with-resend"
    },
    {
      "level": "h2",
      "text": "8. Deploy your Worker",
      "id": "8.-deploy-your-worker"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Create new Workers application",
      "id": "1.-create-new-workers-application"
    },
    {
      "level": "h2",
      "text": "2. Create KV namespace",
      "id": "2.-create-kv-namespace"
    },
    {
      "level": "h3",
      "text": "Add KV bindings to the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/)",
      "id": "add-kv-bindings-to-the-[wrangler-configuration-file](https://developers.cloudflare.com/workers/wrangler/configuration/)"
    },
    {
      "level": "h2",
      "text": "3. Set up Browser Rendering",
      "id": "3.-set-up-browser-rendering"
    },
    {
      "level": "h2",
      "text": "4. Set up a Queue",
      "id": "4.-set-up-a-queue"
    },
    {
      "level": "h3",
      "text": "Add Queue bindings to wrangler.toml",
      "id": "add-queue-bindings-to-wrangler.toml"
    },
    {
      "level": "h2",
      "text": "5. Add bindings to environment",
      "id": "5.-add-bindings-to-environment"
    },
    {
      "level": "h2",
      "text": "6. Submit links to crawl",
      "id": "6.-submit-links-to-crawl"
    },
    {
      "level": "h2",
      "text": "7. Crawl with Puppeteer",
      "id": "7.-crawl-with-puppeteer"
    },
    {
      "level": "h2",
      "text": "8. Deploy your Worker",
      "id": "8.-deploy-your-worker"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Quick Reference",
      "id": "quick-reference"
    },
    {
      "level": "h2",
      "text": "Supported SQL Clauses",
      "id": "supported-sql-clauses"
    },
    {
      "level": "h2",
      "text": "SELECT Clause",
      "id": "select-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "Aggregation Functions",
      "id": "aggregation-functions"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "GROUP BY Clause",
      "id": "group-by-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "HAVING Clause",
      "id": "having-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "FROM Clause",
      "id": "from-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "WHERE Clause",
      "id": "where-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "ORDER BY Clause",
      "id": "order-by-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "LIMIT Clause",
      "id": "limit-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "Unsupported SQL Clauses",
      "id": "unsupported-sql-clauses"
    },
    {
      "level": "h2",
      "text": "Best Practices",
      "id": "best-practices"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Set up authentication",
      "id": "1.-set-up-authentication"
    },
    {
      "level": "h2",
      "text": "2. Create an R2 bucket and enable R2 Data Catalog",
      "id": "2.-create-an-r2-bucket-and-enable-r2-data-catalog"
    },
    {
      "level": "h3",
      "text": "(Optional) Enable compaction on your R2 Data Catalog",
      "id": "(optional)-enable-compaction-on-your-r2-data-catalog"
    },
    {
      "level": "h2",
      "text": "3. Set up the pipeline infrastructure",
      "id": "3.-set-up-the-pipeline-infrastructure"
    },
    {
      "level": "h3",
      "text": "3.1. Create the Pipeline stream",
      "id": "3.1.-create-the-pipeline-stream"
    },
    {
      "level": "h2",
      "text": "4. Generate sample fraud detection data",
      "id": "4.-generate-sample-fraud-detection-data"
    }
  ],
  "url": "llms-txt#make-sure-to-replace-the-placeholder-with-your-shared-secret",
  "links": []
}