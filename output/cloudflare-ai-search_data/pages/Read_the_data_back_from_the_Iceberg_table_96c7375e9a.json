{
  "title": "Read the data back from the Iceberg table",
  "content": "result_df = spark.read \\\n    .format(\"iceberg\") \\\n    .load(\"default.my_table\")\n\nresult_df.show()\njava\npackage com.example\n\nimport org.apache.spark.sql.SparkSession\n\nobject R2DataCatalogDemo {\n    def main(args: Array[String]): Unit = {\n\nval uri = sys.env(\"CATALOG_URI\")\n        val warehouse = sys.env(\"WAREHOUSE\")\n        val token = sys.env(\"TOKEN\")\n\nval spark = SparkSession.builder()\n            .appName(\"My R2 Data Catalog Demo\")\n            .master(\"local[*]\")\n            .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n            .config(\"spark.sql.catalog.mydemo\", \"org.apache.iceberg.spark.SparkCatalog\")\n            .config(\"spark.sql.catalog.mydemo.type\", \"rest\")\n            .config(\"spark.sql.catalog.mydemo.uri\", uri)\n            .config(\"spark.sql.catalog.mydemo.warehouse\", warehouse)\n            .config(\"spark.sql.catalog.mydemo.token\", token)\n            .getOrCreate()\n\nimport spark.implicits._\n\nval data = Seq(\n            (1, \"Alice\", 25),\n            (2, \"Bob\", 30),\n            (3, \"Charlie\", 35),\n            (4, \"Diana\", 40)\n        ).toDF(\"id\", \"name\", \"age\")\n\nspark.sql(\"USE mydemo\")\n\nspark.sql(\"CREATE NAMESPACE IF NOT EXISTS demoNamespace\")\n\ndata.writeTo(\"demoNamespace.demotable\").createOrReplace()\n\nval readResult = spark.sql(\"SELECT * FROM demoNamespace.demotable WHERE age > 30\")\n        println(\"Records with age > 30:\")\n        readResult.show()\n    }\n}\njava\nname := \"R2DataCatalogDemo\"\n\nval sparkVersion = \"3.5.3\"\nval icebergVersion = \"1.8.1\"\n\n// You need to use binaries of Spark compiled with either 2.12 or 2.13; and 2.12 is more common.\n// If you download Spark 3.5.3 with sdkman, then it comes with 2.12.18\nscalaVersion := \"2.12.18\"\n\nlibraryDependencies ++= Seq(\n    \"org.apache.spark\" %% \"spark-core\" % sparkVersion,\n    \"org.apache.spark\" %% \"spark-sql\" % sparkVersion,\n    \"org.apache.iceberg\" % \"iceberg-core\" % icebergVersion,\n    \"org.apache.iceberg\" % \"iceberg-spark-runtime-3.5_2.12\" % icebergVersion,\n    \"org.apache.iceberg\" % \"iceberg-aws-bundle\" % icebergVersion,\n)\n\n// build a fat JAR with all dependencies\nassembly / assemblyMergeStrategy := {\n    case PathList(\"META-INF\", \"services\", xs @ _*) => MergeStrategy.concat\n    case PathList(\"META-INF\", xs @ _*) => MergeStrategy.discard\n    case \"reference.conf\" => MergeStrategy.concat\n    case \"application.conf\" => MergeStrategy.concat\n    case x if x.endsWith(\".properties\") => MergeStrategy.first\n    case x => MergeStrategy.first\n}\n\n// For Java  17 Compatability\nCompile / javacOptions ++= Seq(\"--release\", \"17\")\nplaintext\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"1.2.0\")\nbash\nsdk install java 17.0.14-amzn\nsdk install spark 3.5.3\nsdk install sbt 1.10.11\nbash\nsbt clean assembly\nplaintext",
  "code_samples": [
    {
      "code": "</page>\n\n<page>\n---\ntitle: Spark (Scala) · Cloudflare R2 docs\ndescription: Below is an example of how you can build an Apache Spark\n  application (with Scala) which connects to R2 Data Catalog. This application\n  is built to run locally, but it can be adapted to run on a cluster.\nlastUpdated: 2025-07-08T08:09:06.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-catalog/config-examples/spark-scala/\n  md: https://developers.cloudflare.com/r2/data-catalog/config-examples/spark-scala/index.md\n---\n\nBelow is an example of how you can build an [Apache Spark](https://spark.apache.org/) application (with Scala) which connects to R2 Data Catalog. This application is built to run locally, but it can be adapted to run on a cluster.\n\n## Prerequisites\n\n* Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages).\n\n* [Create an R2 bucket](https://developers.cloudflare.com/r2/buckets/create-buckets/) and [enable the data catalog](https://developers.cloudflare.com/r2/data-catalog/manage-catalogs/#enable-r2-data-catalog-on-a-bucket).\n\n* [Create an R2 API token](https://developers.cloudflare.com/r2/api/tokens/) with both [R2 and data catalog permissions](https://developers.cloudflare.com/r2/api/tokens/#permissions).\n\n* Install Java 17, Spark 3.5.3, and SBT 1.10.11\n\n  * Note: The specific versions of tools are critical for getting things to work in this example.\n  * Tip: [“SDKMAN”](https://sdkman.io/) is a convenient package manager for installing SDKs.\n\n## Example usage\n\nTo start, create a new empty project directory somewhere on your machine.\n\nInside that directory, create the following file at `src/main/scala/com/example/R2DataCatalogDemo.scala`. This will serve as the main entry point for your Spark application.",
      "language": "unknown"
    },
    {
      "code": "For building this application and managing dependencies, we will use [sbt (“simple build tool”)](https://www.scala-sbt.org/). The following is an example `build.sbt` file to place at the root of your project. It is configured to produce a \"fat JAR\", bundling all required dependencies.",
      "language": "unknown"
    },
    {
      "code": "To enable the [sbt-assembly plugin](https://github.com/sbt/sbt-assembly?tab=readme-ov-file) (used to build fat JARs), add the following to a new file at `project/assembly.sbt`:",
      "language": "unknown"
    },
    {
      "code": "Make sure Java, Spark, and sbt are installed and available in your shell. If you are using SDKMAN, you can install them as shown below:",
      "language": "unknown"
    },
    {
      "code": "With everything installed, you can now build the project using sbt. This will generate a single bundled JAR file.",
      "language": "unknown"
    },
    {
      "code": "After building, the output JAR should be located at `target/scala-2.12/R2DataCatalogDemo-assembly-1.0.jar`.\n\nTo run the application, you will use `spark-submit`. Below is an example shell script (`submit.sh`) that includes the necessary Java compatibility flags for Spark on Java 17:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "Example usage",
      "id": "example-usage"
    }
  ],
  "url": "llms-txt#read-the-data-back-from-the-iceberg-table",
  "links": []
}