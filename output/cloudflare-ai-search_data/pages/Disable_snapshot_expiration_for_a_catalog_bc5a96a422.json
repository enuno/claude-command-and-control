{
  "title": "Disable snapshot expiration for a catalog",
  "content": "npx wrangler r2 bucket catalog snapshot-expiration disable my-bucket\nsh\nexport AWS_REGION=auto\nexport AWS_ENDPOINT_URL=https://<account_id>.r2.cloudflarestorage.com\nexport AWS_ACCESS_KEY_ID=your_access_key_id\nexport AWS_SECRET_ACCESS_KEY=your_secret_access_key\nsh\n    npm i @aws-sdk/client-s3\n    sh\n    yarn add @aws-sdk/client-s3\n    sh\n    pnpm add @aws-sdk/client-s3\n    javascript\n  import { GetObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\n\nconst s3 = new S3Client();\n\nconst Bucket = \"<YOUR_BUCKET_NAME>\";\n  const Key = \"pfp.jpg\";\n\nconst object = await s3.send(\n    new GetObjectCommand({\n      Bucket,\n      Key,\n    }),\n  );\n\nconsole.log(\"Successfully fetched the object\", object.$metadata);\n\n// Process the data as needed\n  // For example, to get the content as a Buffer:\n  // const content = data.Body;\n\n// Or to save the file (requires 'fs' module):\n  // import { writeFile } from \"node:fs/promises\";\n  // await writeFile('ingested_0001.parquet', data.Body);\n  sh\n  pip install boto3\n  python\n  import boto3\n  from botocore.client import Config\n\n# Configure the S3 client for Cloudflare R2\n  s3_client = boto3.client('s3',\n    config=Config(signature_version='s3v4')\n  )\n\n# Specify the object key\n  #\n  bucket = '<YOUR_BUCKET_NAME>'\n  object_key = '2024/08/02/ingested_0001.parquet'\n\ntry:\n    # Fetch the object\n    response = s3_client.get_object(Bucket=bucket, Key=object_key)\n\nprint('Successfully fetched the object')\n\n# Process the response content as needed\n    # For example, to read the content:\n    # object_content = response['Body'].read()\n\n# Or to save the file:\n    # with open('ingested_0001.parquet', 'wb') as f:\n    #     f.write(response['Body'].read())\n\nexcept Exception as e:\n    print(f'Failed to fetch the object. Error: {str(e)}')\n  sh\n  go get github.com/aws/aws-sdk-go-v2\n  go get github.com/aws/aws-sdk-go-v2/config\n  go get github.com/aws/aws-sdk-go-v2/credentials\n  go get github.com/aws/aws-sdk-go-v2/service/s3\n  go\n  package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"log\"\n    \"github.com/aws/aws-sdk-go-v2/aws\"\n    \"github.com/aws/aws-sdk-go-v2/config\"\n    \"github.com/aws/aws-sdk-go-v2/service/s3\"\n  )\n\nfunc main() {\n      cfg, err := config.LoadDefaultConfig(context.TODO())\n      if err != nil {\n        log.Fatalf(\"Unable to load SDK config, %v\", err)\n      }\n\n// Create an S3 client\n      client := s3.NewFromConfig(cfg)\n\n// Specify the object key\n      bucket := \"<YOUR_BUCKET_NAME>\"\n      objectKey := \"pfp.jpg\"\n\n// Fetch the object\n      output, err := client.GetObject(context.TODO(), &s3.GetObjectInput{\n        Bucket: aws.String(bucket),\n        Key:    aws.String(objectKey),\n      })\n      if err != nil {\n        log.Fatalf(\"Unable to fetch object, %v\", err)\n      }\n      defer output.Body.Close()\n\nfmt.Println(\"Successfully fetched the object\")\n\n// Process the object content as needed\n      // For example, to save the file:\n      // file, err := os.Create(\"ingested_0001.parquet\")\n      // if err != nil {\n      //   log.Fatalf(\"Unable to create file, %v\", err)\n      // }\n      // defer file.Close()\n      // _, err = io.Copy(file, output.Body)\n      // if err != nil {\n      //   log.Fatalf(\"Unable to write file, %v\", err)\n      // }\n\n// Or to read the content:\n      content, err := io.ReadAll(output.Body)\n      if err != nil {\n        log.Fatalf(\"Unable to read object content, %v\", err)\n      }\n      fmt.Printf(\"Object content length: %d bytes\\n\", len(content))\n  }\n  sh\n  npm i @aws-sdk/client-s3\n  sh\n  yarn add @aws-sdk/client-s3\n  sh\n  pnpm add @aws-sdk/client-s3\n  js\nexport default {\n  async fetch(request, env, context) {\n    try {\n      const url = new URL(request.url);\n\n// Construct the cache key from the cache URL\n      const cacheKey = new Request(url.toString(), request);\n      const cache = caches.default;\n\n// Check whether the value is already available in the cache\n      // if not, you will need to fetch it from R2, and store it in the cache\n      // for future access\n      let response = await cache.match(cacheKey);\n\nif (response) {\n        console.log(`Cache hit for: ${request.url}.`);\n        return response;\n      }\n\nconsole.log(\n        `Response for request url: ${request.url} not present in cache. Fetching and caching request.`\n      );\n\n// If not in cache, get it from R2\n      const objectKey = url.pathname.slice(1);\n      const object = await env.MY_BUCKET.get(objectKey);\n      if (object === null) {\n        return new Response('Object Not Found', { status: 404 });\n      }\n\n// Set the appropriate object headers\n      const headers = new Headers();\n      object.writeHttpMetadata(headers);\n      headers.set('etag', object.httpEtag);\n\n// Cache API respects Cache-Control headers. Setting s-max-age to 10\n      // will limit the response to be in cache for 10 seconds max\n      // Any changes made to the response here will be reflected in the cached value\n      headers.append('Cache-Control', 's-maxage=10');\n\nresponse = new Response(object.body, {\n        headers,\n      });\n\n// Store the fetched response as cacheKey\n      // Use waitUntil so you can return the response without blocking on\n      // writing to cache\n      context.waitUntil(cache.put(cacheKey, response.clone()));\n\nreturn response;\n    } catch (e) {\n      return new Response('Error thrown ' + e.message);\n    }\n  },\n};\nsh\nrclone config file",
  "code_samples": [
    {
      "code": "### Choose the right retention policy\n\nDifferent workloads require different snapshot retention strategies:\n\n* **Development/testing tables**: Shorter retention (2-7 days, 5 snapshots) to minimize storage costs\n* **Production analytics tables**: Medium retention (7-30 days, 10-20 snapshots) for debugging and analysis\n* **Compliance/audit tables**: Longer retention (30-90 days, 50+ snapshots) to meet regulatory requirements\n* **High-frequency ingest**: Higher minimum snapshot count to preserve more granular history\n\nThese are generic recommendations, make sure to consider:\n\n* Time travel requirements\n* Compliance requirements\n* Storage costs\n\n## Current limitations\n\n* During open beta, compaction will compact up to 2 GB worth of files once per hour for each table.\n* Only data files stored in parquet format are currently supported with compaction.\n* Orphan file cleanup is not supported yet.\n* Minimum target file size for compaction is 64 MB and maximum is 512 MB.\n\n</page>\n\n<page>\n---\ntitle: Authenticate against R2 API using auth tokens · Cloudflare R2 docs\ndescription: The following example shows how to authenticate against R2 using\n  the S3 API and an API token.\nlastUpdated: 2025-09-24T08:37:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens/\n  md: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens/index.md\n---\n\nThe following example shows how to authenticate against R2 using the S3 API and an API token.\n\nNote\n\nFor providing secure access to bucket objects for anonymous users, we recommend using [pre-signed URLs](https://developers.cloudflare.com/r2/api/s3/presigned-urls/) instead.\n\nPre-signed URLs do not require users to be a member of your organization and enable direct programmatic access to R2.\n\nEnsure you have set the following environment variables prior to running either example. Refer to [Authentication](https://developers.cloudflare.com/r2/api/tokens/) for more information.",
      "language": "unknown"
    },
    {
      "code": "* JavaScript\n\n  Install the `@aws-sdk/client-s3` package for the S3 API:\n\n  * npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Run the following Node.js script with `node index.js`. Ensure you change `Bucket` to the name of your bucket, and `Key` to point to an existing file in your R2 bucket.\n\n  Note, tutorial below should function for TypeScript as well.",
      "language": "unknown"
    },
    {
      "code": "* Python\n\n  Install the `boto3` S3 API client:",
      "language": "unknown"
    },
    {
      "code": "Run the following Python script with `python3 get_r2_object.py`. Ensure you change `bucket` to the name of your bucket, and `object_key` to point to an existing file in your R2 bucket.",
      "language": "unknown"
    },
    {
      "code": "* Go\n\n  Use `go get` to add the `aws-sdk-go-v2` packages to your Go project:",
      "language": "unknown"
    },
    {
      "code": "Run the following Go application as a script with `go run main.go`. Ensure you change `bucket` to the name of your bucket, and `objectKey` to point to an existing file in your R2 bucket.",
      "language": "unknown"
    },
    {
      "code": "* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Use the Cache API · Cloudflare R2 docs\ndescription: Use the Cache API to store R2 objects in Cloudflare's cache.\nlastUpdated: 2024-08-21T16:27:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/cache-api/\n  md: https://developers.cloudflare.com/r2/examples/cache-api/index.md\n---\n\nUse the [Cache API](https://developers.cloudflare.com/workers/runtime-apis/cache/) to store R2 objects in Cloudflare's cache.\n\nNote\n\nYou will need to [connect a custom domain](https://developers.cloudflare.com/workers/configuration/routing/custom-domains/) or [route](https://developers.cloudflare.com/workers/configuration/routing/routes/) to your Worker in order to use the Cache API. Cache API operations in the Cloudflare Workers dashboard editor, Playground previews, and any `*.workers.dev` deployments will have no impact.",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: S3 SDKs · Cloudflare R2 docs\nlastUpdated: 2024-09-29T02:09:56.000Z\nchatbotDeprioritize: true\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/aws/\n  md: https://developers.cloudflare.com/r2/examples/aws/index.md\n---\n\n* [aws CLI](https://developers.cloudflare.com/r2/examples/aws/aws-cli/)\n* [aws-sdk-go](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-go/)\n* [aws-sdk-java](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-java/)\n* [aws-sdk-js](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-js/)\n* [aws-sdk-js-v3](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-js-v3/)\n* [aws-sdk-net](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-net/)\n* [aws-sdk-php](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-php/)\n* [aws-sdk-ruby](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-ruby/)\n* [aws-sdk-rust](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-rust/)\n* [aws4fetch](https://developers.cloudflare.com/r2/examples/aws/aws4fetch/)\n* [boto3](https://developers.cloudflare.com/r2/examples/aws/boto3/)\n* [Configure custom headers](https://developers.cloudflare.com/r2/examples/aws/custom-header/)\n\n</page>\n\n<page>\n---\ntitle: Multi-cloud setup · Cloudflare R2 docs\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/multi-cloud/\n  md: https://developers.cloudflare.com/r2/examples/multi-cloud/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Rclone · Cloudflare R2 docs\ndescription: You must generate an Access Key before getting started. All\n  examples will utilize access_key_id and access_key_secret variables which\n  represent the Access Key ID and Secret Access Key values you generated.\nlastUpdated: 2025-08-20T18:25:25.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/rclone/\n  md: https://developers.cloudflare.com/r2/examples/rclone/index.md\n---\n\nYou must [generate an Access Key](https://developers.cloudflare.com/r2/api/tokens/) before getting started. All examples will utilize `access_key_id` and `access_key_secret` variables which represent the **Access Key ID** and **Secret Access Key** values you generated.\n\n\n\nRclone is a command-line tool which manages files on cloud storage. You can use rclone to upload objects to R2 concurrently.\n\n## Configure rclone\n\nWith [`rclone`](https://rclone.org/install/) installed, you may run [`rclone config`](https://rclone.org/s3/) to configure a new S3 storage provider. You will be prompted with a series of questions for the new provider details.\n\nRecommendation\n\nIt is recommended that you choose a unique provider name and then rely on all default answers to the prompts.\n\nThis will create a `rclone` configuration file, which you can then modify with the preset configuration given below.\n\n1. Create new remote by selecting `n`.\n2. Select a name for the new remote. For example, use `r2`.\n3. Select the `Amazon S3 Compliant Storage Providers` storage type.\n4. Select `Cloudflare R2 storage` for the provider.\n5. Select whether you would like to enter AWS credentials manually, or get it from the runtime environment.\n6. Enter the AWS Access Key ID.\n7. Enter AWS Secret Access Key (password).\n8. Select the region to connect to (optional).\n9. Select the S3 API endpoint.\n\nNote\n\nEnsure you are running `rclone` v1.59 or greater ([rclone downloads](https://beta.rclone.org/)). Versions prior to v1.59 may return `HTTP 401: Unauthorized` errors, as earlier versions of `rclone` do not strictly align to the S3 specification in all cases.\n\n### Edit an existing rclone configuration\n\nIf you have already configured `rclone` in the past, you may run `rclone config file` to print the location of your `rclone` configuration file:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Choose the right retention policy",
      "id": "choose-the-right-retention-policy"
    },
    {
      "level": "h2",
      "text": "Current limitations",
      "id": "current-limitations"
    },
    {
      "level": "h2",
      "text": "Configure rclone",
      "id": "configure-rclone"
    },
    {
      "level": "h3",
      "text": "Edit an existing rclone configuration",
      "id": "edit-an-existing-rclone-configuration"
    }
  ],
  "url": "llms-txt#disable-snapshot-expiration-for-a-catalog",
  "links": []
}