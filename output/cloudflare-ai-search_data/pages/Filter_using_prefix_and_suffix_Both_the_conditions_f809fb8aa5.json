{
  "title": "Filter using prefix and suffix. Both the conditions will be used for filtering",
  "content": "$ npx wrangler r2 bucket notification create <BUCKET_NAME> --event-type <EVENT_TYPE> --queue <QUEUE_NAME> --prefix \"<PREFIX_VALUE>\" --suffix \"<SUFFIX_VALUE>\"\njson\n{\n  \"account\": \"3f4b7e3dcab231cbfdaa90a6a28bd548\",\n  \"action\": \"CopyObject\",\n  \"bucket\": \"my-bucket\",\n  \"object\": {\n    \"key\": \"my-new-object\",\n    \"size\": 65536,\n    \"eTag\": \"c846ff7a18f28c2e262116d6e8719ef0\"\n  },\n  \"eventTime\": \"2024-05-24T19:36:44.379Z\",\n  \"copySource\": {\n    \"bucket\": \"my-bucket\",\n    \"object\": \"my-original-object\"\n  }\n}\nsh\nnpx wrangler r2 bucket lifecycle add <BUCKET_NAME> [OPTIONS]\nsh\nnpx wrangler r2 bucket lifecycle set <BUCKET_NAME> --file <FILE_PATH>\njs\nconst client = new S3({\n  endpoint: \"https://<account_id>.r2.cloudflarestorage.com\",\n  credentials: {\n    accessKeyId: \"<access_key_id>\",\n    secretAccessKey: \"<access_key_secret>\",\n  },\n  region: \"auto\",\n});\njavascript\nawait client\n  .putBucketLifecycleConfiguration({\n    Bucket: \"testBucket\",\n    LifecycleConfiguration: {\n      Rules: [\n        // Example: deleting objects on a specific date\n        // Delete 2019 documents in 2024\n        {\n          ID: \"Delete 2019 Documents\",\n          Status: \"Enabled\",\n          Filter: {\n            Prefix: \"2019/\",\n          },\n          Expiration: {\n            Date: new Date(\"2024-01-01\"),\n          },\n        },\n        // Example: transitioning objects to Infrequent Access storage by age\n        // Transition objects older than 30 days to Infrequent Access storage\n        {\n          ID: \"Transition Objects To Infrequent Access\",\n          Status: \"Enabled\",\n          Transitions: [\n            {\n              Days: 30,\n              StorageClass: \"STANDARD_IA\",\n            },\n          ],\n        },\n        // Example: deleting objects by age\n        // Delete logs older than 90 days\n        {\n          ID: \"Delete Old Logs\",\n          Status: \"Enabled\",\n          Filter: {\n            Prefix: \"logs/\",\n          },\n          Expiration: {\n            Days: 90,\n          },\n        },\n        // Example: abort all incomplete multipart uploads after a week\n        {\n          ID: \"Abort Incomplete Multipart Uploads\",\n          Status: \"Enabled\",\n          AbortIncompleteMultipartUpload: {\n            DaysAfterInitiation: 7,\n          },\n        },\n        // Example: abort user multipart uploads after a day\n        {\n          ID: \"Abort User Incomplete Multipart Uploads\",\n          Status: \"Enabled\",\n          Filter: {\n            Prefix: \"useruploads/\",\n          },\n          AbortIncompleteMultipartUpload: {\n            // For uploads matching the prefix, this rule will take precedence\n            // over the one above due to its earlier expiration.\n            DaysAfterInitiation: 1,\n          },\n        },\n      ],\n    },\n  })\n  .promise();\nsh\nnpx wrangler r2 bucket lifecycle list <BUCKET_NAME>\njs\nimport S3 from \"aws-sdk/clients/s3.js\";\n\n// Configure the S3 client to talk to R2.\nconst client = new S3({\n  endpoint: \"https://<account_id>.r2.cloudflarestorage.com\",\n  credentials: {\n    accessKeyId: \"<access_key_id>\",\n    secretAccessKey: \"<access_key_secret>\",\n  },\n  region: \"auto\",\n});\n\n// Get lifecycle configuration for bucket\nconsole.log(\n  await client\n    .getBucketLifecycleConfiguration({\n      Bucket: \"bucketName\",\n    })\n    .promise(),\n);\nsh\nnpx wrangler r2 bucket lifecycle remove <BUCKET_NAME> --id <RULE_ID>\njs\nimport S3 from \"aws-sdk/clients/s3.js\";\n\n// Configure the S3 client to talk to R2.\nconst client = new S3({\n  endpoint: \"https://<account_id>.r2.cloudflarestorage.com\",\n  credentials: {\n    accessKeyId: \"<access_key_id>\",\n    secretAccessKey: \"<access_key_secret>\",\n  },\n  region: \"auto\",\n});\n\n// Delete lifecycle configuration for bucket\nawait client\n  .deleteBucketLifecycle({\n    Bucket: \"bucketName\",\n  })\n  .promise();\nsh\naws s3api copy-object \\\n  --endpoint-url https://<ACCOUNT_ID>.r2.cloudflarestorage.com \\\n  --bucket bucket-name \\\n  --key path/to/object.txt \\\n  --copy-source /bucket-name/path/to/object.txt \\\n  --storage-class STANDARD_IA\njson\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:Get*\", \"s3:List*\"],\n      \"Resource\": [\"arn:aws:s3:::<BUCKET_NAME>\", \"arn:aws:s3:::<BUCKET_NAME>/*\"]\n    }\n  ]\n}\nsh\nnpx wrangler r2 bucket sippy enable <BUCKET_NAME>\nsh\nnpx wrangler r2 bucket sippy disable <BUCKET_NAME>\njson\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Action\": [\"s3:ListBucket*\", \"s3:GetObject*\"],\n         \"Resource\": [\n           \"arn:aws:s3:::<BUCKET_NAME>\",\n           \"arn:aws:s3:::<BUCKET_NAME>/*\"\n         ]\n       }\n     ]\n   }\n   bash",
  "code_samples": [
    {
      "code": "For a more complete step-by-step example, refer to the [Log and store upload events in R2 with event notifications](https://developers.cloudflare.com/r2/tutorials/upload-logs-event-notifications/) example.\n\n## Event notification rules\n\nEvent notification rules determine the [event types](https://developers.cloudflare.com/r2/buckets/event-notifications/#event-types) that trigger notifications and optionally enable filtering based on object `prefix` and `suffix`. You can have up to 100 event notification rules per R2 bucket.\n\n## Event types\n\n| Event type | Description | Trigger actions |\n| - | - | - |\n| `object-create` | Triggered when new objects are created or existing objects are overwritten. | * `PutObject`\n* `CopyObject`\n* `CompleteMultipartUpload` |\n| `object-delete` | Triggered when an object is explicitly removed from the bucket. | - `DeleteObject`\n- `LifecycleDeletion` |\n\n## Message format\n\nQueue consumers receive notifications as [Messages](https://developers.cloudflare.com/queues/configuration/javascript-apis/#message). The following is an example of the body of a message that a consumer Worker will receive:",
      "language": "unknown"
    },
    {
      "code": "### Properties\n\n| Property | Type | Description |\n| - | - | - |\n| `account` | String | The Cloudflare account ID that the event is associated with. |\n| `action` | String | The type of action that triggered the event notification. Example actions include: `PutObject`, `CopyObject`, `CompleteMultipartUpload`, `DeleteObject`. |\n| `bucket` | String | The name of the bucket where the event occurred. |\n| `object` | Object | A nested object containing details about the object involved in the event. |\n| `object.key` | String | The key (or name) of the object within the bucket. |\n| `object.size` | Number | The size of the object in bytes. Note: not present for object-delete events. |\n| `object.eTag` | String | The entity tag (eTag) of the object. Note: not present for object-delete events. |\n| `eventTime` | String | The time when the action that triggered the event occurred. |\n| `copySource` | Object | A nested object containing details about the source of a copied object. Note: only present for events triggered by `CopyObject`. |\n| `copySource.bucket` | String | The bucket that contained the source object. |\n| `copySource.object` | String | The name of the source object. |\n\n## Notes\n\n* Queues [per-queue message throughput](https://developers.cloudflare.com/queues/platform/limits/) is currently 5,000 messages per second. If your workload produces more than 5,000 notifications per second, we recommend splitting notification rules across multiple queues.\n* Rules without prefix/suffix apply to all objects in the bucket.\n* Overlapping or conflicting rules that could trigger multiple notifications for the same event are not allowed. For example, if you have an `object-create` (or `PutObject` action) rule without a prefix and suffix, then adding another `object-create` (or `PutObject` action) rule with a prefix like `images/` could trigger more than one notification for a single upload, which is invalid.\n\n</page>\n\n<page>\n---\ntitle: Object lifecycles · Cloudflare R2 docs\ndescription: Object lifecycles determine the retention period of objects\n  uploaded to your bucket and allow you to specify when objects should\n  transition from Standard storage to Infrequent Access storage.\nlastUpdated: 2025-09-03T16:40:54.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/buckets/object-lifecycles/\n  md: https://developers.cloudflare.com/r2/buckets/object-lifecycles/index.md\n---\n\nObject lifecycles determine the retention period of objects uploaded to your bucket and allow you to specify when objects should transition from Standard storage to Infrequent Access storage.\n\nA lifecycle configuration is a collection of lifecycle rules that define actions to apply to objects during their lifetime.\n\nFor example, you can create an object lifecycle rule to delete objects after 90 days, or you can set a rule to transition objects to Infrequent Access storage after 30 days.\n\n## Behavior\n\n* Objects will typically be removed from a bucket within 24 hours of the `x-amz-expiration` value.\n* When a lifecycle configuration is applied that deletes objects, newly uploaded objects' `x-amz-expiration` value immediately reflects the expiration based on the new rules, but existing objects may experience a delay. Most objects will be transitioned within 24 hours but may take longer depending on the number of objects in the bucket. While objects are being migrated, you may see old applied rules from the previous configuration.\n* An object is no longer billable once it has been deleted.\n* Buckets have a default lifecycle rule to expire multipart uploads seven days after initiation.\n* When an object is transitioned from Standard storage to Infrequent Access storage, a [Class A operation](https://developers.cloudflare.com/r2/pricing/#class-a-operations) is incurred.\n* When rules conflict and specify both a storage class transition and expire transition within a 24-hour period, the expire (or delete) lifecycle transition takes precedence over transitioning storage class.\n\n## Configure lifecycle rules for your bucket\n\nWhen you create an object lifecycle rule, you can specify which prefix you would like it to apply to.\n\n* Note that object lifecycles currently has a 1000 rule maximum.\n* Managing object lifecycles is a bucket-level action, and requires an API token with the [`Workers R2 Storage Write`](https://developers.cloudflare.com/r2/api/tokens/#permission-groups) permission group.\n\n### Dashboard\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Locate and select your bucket from the list.\n\n3. From the bucket page, select **Settings**.\n\n4. Under **Object Lifecycle Rules**, select **Add rule**.\n\n5. Fill out the fields for the new rule.\n\n6. When you are done, select **Save changes**.\n\n### Wrangler\n\n1. Install [`npm`](https://docs.npmjs.com/getting-started).\n2. Install [Wrangler, the Developer Platform CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/).\n3. Log in to Wrangler with the [`wrangler login` command](https://developers.cloudflare.com/workers/wrangler/commands/#login).\n4. Add a lifecycle rule to your bucket by running the [`r2 bucket lifecycle add` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-lifecycle-add).",
      "language": "unknown"
    },
    {
      "code": "Alternatively you can set the entire lifecycle configuration for a bucket from a JSON file using the [`r2 bucket lifecycle set` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-lifecycle-set).",
      "language": "unknown"
    },
    {
      "code": "The JSON file should be in the format of the request body of the [put object lifecycle configuration API](https://developers.cloudflare.com/api/resources/r2/subresources/buckets/subresources/lifecycle/methods/update/).\n\n### S3 API\n\nBelow is an example of configuring a lifecycle configuration (a collection of lifecycle rules) with different sets of rules for different potential use cases.",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## Get lifecycle rules for your bucket\n\n### Wrangler\n\nTo get the list of lifecycle rules associated with your bucket, run the [`r2 bucket lifecycle list` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-lifecycle-list).",
      "language": "unknown"
    },
    {
      "code": "### S3 API",
      "language": "unknown"
    },
    {
      "code": "## Delete lifecycle rules from your bucket\n\n### Dashboard\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Locate and select your bucket from the list.\n\n3. From the bucket page, select **Settings**.\n\n4. Under **Object lifecycle rules**, select the rules you would like to delete.\n\n5. When you are done, select **Delete rule(s)**.\n\n### Wrangler\n\nTo remove a specific lifecycle rule from your bucket, run the [`r2 bucket lifecycle remove` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-lifecycle-remove).",
      "language": "unknown"
    },
    {
      "code": "### S3 API",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Public buckets · Cloudflare R2 docs\ndescription: Public Bucket is a feature that allows users to expose the contents\n  of their R2 buckets directly to the Internet. By default, buckets are never\n  publicly accessible and will always require explicit user permission to\n  enable.\nlastUpdated: 2025-10-23T19:01:53.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/buckets/public-buckets/\n  md: https://developers.cloudflare.com/r2/buckets/public-buckets/index.md\n---\n\nPublic Bucket is a feature that allows users to expose the contents of their R2 buckets directly to the Internet. By default, buckets are never publicly accessible and will always require explicit user permission to enable.\n\nPublic buckets can be set up in either one of two ways:\n\n* Expose your bucket as a custom domain under your control.\n* Expose your bucket using a Cloudflare-managed `https://r2.dev` subdomain for non-production use cases.\n\nThese options can be used independently. Enabling custom domains does not require enabling `r2.dev` access.\n\nTo use features like WAF custom rules, caching, access controls, or bot management, you must configure your bucket behind a custom domain. These capabilities are not available when using the `r2.dev` development url.\n\nNote\n\nCurrently, public buckets do not let you list the bucket contents at the root of your (sub) domain.\n\n## Custom domains\n\n### Caching\n\nDomain access through a custom domain allows you to use [Cloudflare Cache](https://developers.cloudflare.com/cache/) to accelerate access to your R2 bucket.\n\nConfigure your cache to use [Smart Tiered Cache](https://developers.cloudflare.com/cache/how-to/tiered-cache/#smart-tiered-cache) to have a single upper tier data center next to your R2 bucket.\n\nNote\n\nBy default, only certain file types are cached. To cache all files in your bucket, you must set a Cache Everything page rule.\n\nFor more information on default Cache behavior and how to customize it, refer to [Default Cache Behavior](https://developers.cloudflare.com/cache/concepts/default-cache-behavior/#default-cached-file-extensions)\n\n### Access control\n\nTo restrict access to your custom domain's bucket, use Cloudflare's existing security products.\n\n* [Cloudflare Zero Trust Access](https://developers.cloudflare.com/cloudflare-one/access-controls/): Protects buckets that should only be accessible by your teammates. Refer to [Protect an R2 Bucket with Cloudflare Access](https://developers.cloudflare.com/r2/tutorials/cloudflare-access/) tutorial for more information.\n* [Cloudflare WAF Token Authentication](https://developers.cloudflare.com/waf/custom-rules/use-cases/configure-token-authentication/): Restricts access to documents, files, and media to selected users by providing them with an access token.\n\nWarning\n\nDisable public access to your [`r2.dev` subdomain](#disable-public-development-url) when using products like WAF or Cloudflare Access. If you do not disable public access, your bucket will remain publicly available through your `r2.dev` subdomain.\n\n### Minimum TLS Version & Cipher Suites\n\nTo customise the minimum TLS version or cipher suites of a custom hostname of an R2 bucket, you can issue an API call to edit [R2 custom domain settings](https://developers.cloudflare.com/api/resources/r2/subresources/buckets/subresources/domains/subresources/custom/methods/update/). You will need to add the optional `minTLS` and `ciphers` parameters to the request body. For a list of the cipher suites you can specify, refer to [Supported cipher suites](https://developers.cloudflare.com/ssl/edge-certificates/additional-options/cipher-suites/supported-cipher-suites/).\n\n## Add your domain to Cloudflare\n\nThe domain being used must have been added as a [zone](https://developers.cloudflare.com/fundamentals/concepts/accounts-and-zones/#zones) in the same account as the R2 bucket.\n\n* If your domain is already managed by Cloudflare, you can proceed to [Connect a bucket to a custom domain](https://developers.cloudflare.com/r2/buckets/public-buckets/#connect-a-bucket-to-a-custom-domain).\n* If your domain is not managed by Cloudflare, you need to set it up using a [partial (CNAME) setup](https://developers.cloudflare.com/dns/zone-setups/partial-setup/) to add it to your account.\n\nOnce the domain exists in your Cloudflare account (regardless of setup type), you can link it to your bucket.\n\n## Connect a bucket to a custom domain\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Select your bucket.\n\n3. Select **Settings**.\n\n4. Under **Custom Domains**, select **Add**.\n\n5. Enter the domain name you want to connect to and select **Continue**.\n\n6. Review the new record that will be added to the DNS table and select **Connect Domain**.\n\nYour domain is now connected. The status takes a few minutes to change from **Initializing** to **Active**, and you may need to refresh to review the status update. If the status has not changed, select the *...* next to your bucket and select **Retry connection**.\n\nTo view the added DNS record, select **...** next to the connected domain and select **Manage DNS**.\n\nNote\n\nIf the zone is on an Enterprise plan, make sure that you [release the zone hold](https://developers.cloudflare.com/fundamentals/account/account-security/zone-holds/#release-zone-holds) before adding the custom domain.\n\nA zone hold would prevent the custom subdomain from activating.\n\n## Disable domain access\n\nDisabling a domain will turn off public access to your bucket through that domain. Access through other domains or the managed `r2.dev` subdomain are unaffected. The specified domain will also remain connected to R2 until you remove it or delete the bucket.\n\nTo disable a domain:\n\n1. In **R2**, select the bucket you want to modify.\n2. On the bucket page, Select **Settings**, go to **Custom Domains**.\n3. Next to the domain you want to disable, select **...** and **Disable domain**.\n4. The badge under **Access to Bucket** will update to **Not allowed**.\n\n## Remove domain\n\nRemoving a custom domain will disconnect it from your bucket and delete its configuration from the dashboard. Your bucket will remain publicly accessible through any other enabled access method, but the domain will no longer appear in the connected domains list.\n\nTo remove a domain:\n\n1. In **R2**, select the bucket you want to modify.\n2. On the bucket page, Select **Settings**, go to **Custom Domains**.\n3. Next to the domain you want to disable, select **...** and **Remove domain**.\n4. Select **Remove domain** in the confirmation window. This step also removes the CNAME record pointing to the domain. You can always add the domain again.\n\n## Public development URL\n\nExpose the contents of this R2 bucket to the internet through a Cloudflare-managed r2.dev subdomain. This endpoint is intended for non-production traffic.\n\nNote\n\nPublic access through `r2.dev` subdomains are rate limited and should only be used for development purposes.\n\nTo enable access management, Cache and bot management features, you must set up a custom domain when enabling public access to your bucket.\n\nAvoid creating a CNAME record pointing to the `r2.dev` subdomain. This is an **unsupported access path**, and we cannot guarantee consistent reliability or performance. For production use, [add your domain to Cloudflare](#add-your-domain-to-cloudflare) instead.\n\n### Enable public development url\n\nWhen you enable public development URL access for your bucket, its contents become available on the internet through a Cloudflare-managed `r2.dev` subdomain.\n\nTo enable access through `r2.dev` for your buckets:\n\n1. In **R2**, select the bucket you want to modify.\n2. On the bucket page, select **Settings**.\n3. Under **Public Development URL**, select **Enable**.\n4. In **Allow Public Access?**, confirm your choice by typing `allow` to confirm and select **Allow**.\n5. You can now access the bucket and its objects using the Public Bucket URL.\n\nTo verify that your bucket is publicly accessible, check that **Public URL Access** shows **Allowed** in you bucket settings.\n\n### Disable public development url\n\nDisabling public development URL access removes your bucket's exposure through the `r2.dev` subdomain. The bucket and its objects will no longer be accessible via the Public Bucket URL.\n\nIf you have connected other domains, the bucket will remain accessible for those domains.\n\nTo disable public access for your bucket:\n\n1. In **R2**, select the bucket you want to modify.\n2. On the bucket page, select **Settings**.\n3. Under **Public Development URL**, select **Disable**.\n4. In **Disallow Public Access?**, type `disallow` to confirm and select **Disallow**.\n\n</page>\n\n<page>\n---\ntitle: Storage classes · Cloudflare R2 docs\ndescription: Storage classes allow you to trade off between the cost of storage\n  and the cost of accessing data. Every object stored in R2 has an associated\n  storage class.\nlastUpdated: 2025-10-14T11:41:30.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/buckets/storage-classes/\n  md: https://developers.cloudflare.com/r2/buckets/storage-classes/index.md\n---\n\nStorage classes allow you to trade off between the cost of storage and the cost of accessing data. Every object stored in R2 has an associated storage class.\n\nAll storage classes share the following characteristics:\n\n* Compatible with Workers API, S3 API, and public buckets.\n* 99.999999999% (eleven 9s) of annual durability.\n* No minimum object size.\n\n## Available storage classes\n\n| Storage class | Minimum storage duration | Data retrieval fees (processing) | Egress fees (data transfer to Internet) |\n| - | - | - | - |\n| Standard | None | None | None |\n| Infrequent Access | 30 days | Yes | None |\n\nFor more information on how storage classes impact pricing, refer to [Pricing](https://developers.cloudflare.com/r2/pricing/).\n\n### Standard storage\n\nStandard storage is designed for data that is accessed frequently. This is the default storage class for new R2 buckets unless otherwise specified.\n\n#### Example use cases\n\n* Website and application data\n* Media content (e.g., images, video)\n* Storing large datasets for analysis and processing\n* AI training data\n* Other workloads involving frequently accessed data\n\n### Infrequent Access storage\n\nInfrequent Access storage is ideal for data that is accessed less frequently. This storage class offers lower storage cost compared to Standard storage, but includes [retrieval fees](https://developers.cloudflare.com/r2/pricing/#data-retrieval) and a 30 day [minimum storage duration](https://developers.cloudflare.com/r2/pricing/#minimum-storage-duration) requirement.\n\nNote\n\nFor objects stored in Infrequent Access storage, you will be charged for the object for the minimum storage duration even if the object was deleted, moved, or replaced before the specified duration.\n\n#### Example use cases\n\n* Long-term data archiving (for example, logs and historical records needed for compliance)\n* Data backup and disaster recovery\n* Long tail user-generated content\n\n## Set default storage class for buckets\n\nBy setting the default storage class for a bucket, all objects uploaded into the bucket will automatically be assigned the selected storage class unless otherwise specified. Default storage class can be changed after bucket creation in the Dashboard.\n\nTo learn more about creating R2 buckets, refer to [Create new buckets](https://developers.cloudflare.com/r2/buckets/create-buckets/).\n\n## Set storage class for objects\n\n### Specify storage class during object upload\n\nTo learn more about how to specify the storage class for new objects, refer to the [Workers API](https://developers.cloudflare.com/r2/api/workers/) and [S3 API](https://developers.cloudflare.com/r2/api/s3/) documentation.\n\n### Use object lifecycle rules to transition objects to Infrequent Access storage\n\nNote\n\nOnce an object is stored in Infrequent Access, it cannot be transitioned to Standard Access using lifecycle policies.\n\nTo learn more about how to transition objects from Standard storage to Infrequent Access storage, refer to [Object lifecycles](https://developers.cloudflare.com/r2/buckets/object-lifecycles/).\n\n## Change storage class for objects\n\nYou can change the storage class of an object which is already stored in R2 using the [`CopyObject` API](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CopyObject.html).\n\nUse the `x-amz-storage-class` header to change between `STANDARD` and `STANDARD_IA`.\n\nAn example of switching an object from `STANDARD` to `STANDARD_IA` using `aws cli` is shown below:",
      "language": "unknown"
    },
    {
      "code": "* Refer to [aws CLI](https://developers.cloudflare.com/r2/examples/aws/aws-cli/) for more information on using `aws CLI`.\n* Refer to [object-level operations](https://developers.cloudflare.com/r2/api/s3/api/#object-level-operations) for the full list of object-level API operations with R2-compatible S3 API.\n\n</page>\n\n<page>\n---\ntitle: Migration Strategies · Cloudflare R2 docs\ndescription: You can use a combination of Super Slurper and Sippy to effectively\n  migrate all objects with minimal downtime.\nlastUpdated: 2025-10-21T17:09:06.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-migration/migration-strategies/\n  md: https://developers.cloudflare.com/r2/data-migration/migration-strategies/index.md\n---\n\nYou can use a combination of Super Slurper and Sippy to effectively migrate all objects with minimal downtime.\n\n### When the source bucket is actively being read from / written to\n\n1. Enable Sippy and start using the R2 bucket in your application.\n\n   * This copies objects from your previous bucket into the R2 bucket on demand when they are requested by the application.\n   * New uploads will go to the R2 bucket.\n\n2. Use Super Slurper to trigger a one-off migration to copy the remaining objects into the R2 bucket.\n   * In the **Destination R2 bucket** > **Overwrite files?**, select \"Skip existing\".\n\n### When the source bucket is not being read often\n\n1. Use Super Slurper to copy all objects to the R2 bucket.\n   * Note that Super Slurper may skip some objects if they are uploaded after it lists the objects to be copied.\n\n2. Enable Sippy on your R2 bucket, then start using the R2 bucket in your application.\n\n   * New uploads will go to the R2 bucket.\n   * Objects which were uploaded while Super Slurper was copying the objects will be copied on-demand (by Sippy) when they are requested by the application.\n\n### Optimizing your Slurper data migration performance\n\nFor an account, you can run three concurrent Slurper migration jobs at any given time, and each Slurper migration job can process a set amount of requests per second.\n\nTo increase overall throughput and reliability, we recommend splitting your migration into smaller, concurrent jobs using the prefix (or bucket subpath) option.\n\nWhen creating a migration job:\n\n1. Go to the **Source bucket** step.\n2. Under **Define rules**, in **Bucket subpath**, specify subpaths to divide your data by prefix.\n3. Complete the data migration set up.\n\nFor example, suppose your source bucket contains:\n\nYou can create separate jobs with prefixes such as:\n\n* `/photos/2024` to migrate all 2024 files\n* `/photos/202` to migrate all files from 2023 and 2024\n\nEach prefix runs as an independent migration job, allowing Slurper to transfer data in parallel. This improves total transfer speed and ensures that a failure in one job does not interrupt the others.\n\n</page>\n\n<page>\n---\ntitle: Super Slurper · Cloudflare R2 docs\ndescription: Super Slurper allows you to quickly and easily copy objects from\n  other cloud providers to an R2 bucket of your choice.\nlastUpdated: 2025-09-03T16:40:54.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-migration/super-slurper/\n  md: https://developers.cloudflare.com/r2/data-migration/super-slurper/index.md\n---\n\nSuper Slurper allows you to quickly and easily copy objects from other cloud providers to an R2 bucket of your choice.\n\nMigration jobs:\n\n* Preserve custom object metadata from source bucket by copying them on the migrated objects on R2.\n* Do not delete any objects from source bucket.\n* Use TLS encryption over HTTPS connections for safe and private object transfers.\n\n## When to use Super Slurper\n\nUsing Super Slurper as part of your strategy can be a good choice if the cloud storage bucket you are migrating consists primarily of objects less than 1 TB. Objects greater than 1 TB will be skipped and need to be copied separately.\n\nFor migration use cases that do not meet the above criteria, we recommend using tools such as [rclone](https://developers.cloudflare.com/r2/examples/rclone/).\n\n## Use Super Slurper to migrate data to R2\n\n1. In the Cloudflare dashboard, go to the **R2 data migration** page.\n\n   [Go to **Data migration**](https://dash.cloudflare.com/?to=/:account/r2/slurper)\n\n2. Select **Migrate files**.\n\n3. Select the source cloud storage provider that you will be migrating data from.\n\n4. Enter your source bucket name and associated credentials and select **Next**.\n\n5. Enter your R2 bucket name and associated credentials and select **Next**.\n\n6. After you finish reviewing the details of your migration, select **Migrate files**.\n\nYou can view the status of your migration job at any time by selecting your migration from **Data Migration** page.\n\n### Source bucket options\n\n#### Bucket sub path (optional)\n\nThis setting specifies the prefix within the source bucket where objects will be copied from.\n\n### Destination R2 bucket options\n\n#### Overwrite files?\n\nThis setting determines what happens when an object being copied from the source storage bucket matches the path of an existing object in the destination R2 bucket. There are two options:\n\n* Overwrite (default)\n* Skip\n\n## Supported cloud storage providers\n\nCloudflare currently supports copying data from the following cloud object storage providers to R2:\n\n* Amazon S3\n* Cloudflare R2\n* Google Cloud Storage (GCS)\n* All S3-compatible storage providers\n\n### Tested S3-compatible storage providers\n\nThe following S3-compatible storage providers have been tested and verified to work with Super Slurper:\n\n* Backblaze B2\n* DigitalOcean Spaces\n* Scaleway Object Storage\n* Wasabi Cloud Object Storage\n\nSuper Slurper should support transfers from all S3-compatible storage providers, but the ones listed have been explicitly tested.\n\nNote\n\nHave you tested and verified another S3-compatible provider? [Open a pull request](https://github.com/cloudflare/cloudflare-docs/edit/production/src/content/docs/r2/data-migration/super-slurper.mdx) or [create a GitHub issue](https://github.com/cloudflare/cloudflare-docs/issues/new).\n\n## Create credentials for storage providers\n\n### Amazon S3\n\nTo copy objects from Amazon S3, Super Slurper requires access permissions to your S3 bucket. While you can use any AWS Identity and Access Management (IAM) user credentials with the correct permissions, Cloudflare recommends you create a user with a narrow set of permissions.\n\nTo create credentials with the correct permissions:\n\n1. Log in to your AWS IAM account.\n2. Create a policy with the following format and replace `<BUCKET_NAME>` with the bucket you want to grant access to:",
      "language": "unknown"
    },
    {
      "code": "1. Create a new user and attach the created policy to that user.\n\nYou can now use both the Access Key ID and Secret Access Key when defining your source bucket.\n\n### Google Cloud Storage\n\nTo copy objects from Google Cloud Storage (GCS), Super Slurper requires access permissions to your GCS bucket. You can use the Google Cloud predefined `Storage Admin` role, but Cloudflare recommends creating a custom role with a narrower set of permissions.\n\nTo create a custom role with the necessary permissions:\n\n1. Log in to your Google Cloud console.\n2. Go to **IAM & Admin** > **Roles**.\n3. Find the `Storage Object Viewer` role and select **Create role from this role**.\n4. Give your new role a name.\n5. Select **Add permissions** and add the `storage.buckets.get` permission.\n6. Select **Create**.\n\nTo create credentials with your custom role:\n\n1. Log in to your Google Cloud console.\n2. Go to **IAM & Admin** > **Service Accounts**.\n3. Create a service account with the your custom role.\n4. Go to the **Keys** tab of the service account you created.\n5. Select **Add Key** > **Create a new key** and download the JSON key file.\n\nYou can now use this JSON key file when enabling Super Slurper.\n\n## Caveats\n\n### ETags\n\nWhile R2's ETag generation is compatible with S3's during the regular course of operations, ETags are not guaranteed to be equal when an object is migrated using Super Slurper. Super Slurper makes autonomous decisions about the operations it uses when migrating objects to optimize for performance and network usage. It may choose to migrate an object in multiple parts, which affects [ETag calculation](https://developers.cloudflare.com/r2/objects/multipart-objects#etags).\n\nFor example, a 320 MiB object originally uploaded to S3 using a single `PutObject` operation might be migrated to R2 via multipart operations. In this case, its ETag on R2 will not be the same as its ETag on S3. Similarly, an object originally uploaded to S3 using multipart operations might also have a different ETag on R2 if the part sizes Super Slurper chooses for its migration differ from the part sizes this object was originally uploaded with.\n\nRelying on matching ETags before and after the migration is therefore discouraged.\n\n### Archive storage classes\n\nObjects stored using AWS S3 [archival storage classes](https://aws.amazon.com/s3/storage-classes/#Archive) will be skipped and need to be copied separately. Specifically:\n\n* Files stored using S3 Glacier tiers (not including Glacier Instant Retrieval) will be skipped and logged in the migration log.\n* Files stored using S3 Intelligent Tiering and placed in Deep Archive tier will be skipped and logged in the migration log.\n\n</page>\n\n<page>\n---\ntitle: Sippy · Cloudflare R2 docs\ndescription: Sippy is a data migration service that allows you to copy data from\n  other cloud providers to R2 as the data is requested, without paying\n  unnecessary cloud egress fees typically associated with moving large amounts\n  of data.\nlastUpdated: 2025-09-03T16:40:54.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-migration/sippy/\n  md: https://developers.cloudflare.com/r2/data-migration/sippy/index.md\n---\n\nSippy is a data migration service that allows you to copy data from other cloud providers to R2 as the data is requested, without paying unnecessary cloud egress fees typically associated with moving large amounts of data.\n\nMigration-specific egress fees are reduced by leveraging requests within the flow of your application where you would already be paying egress fees to simultaneously copy objects to R2.\n\n## How it works\n\nWhen enabled for an R2 bucket, Sippy implements the following migration strategy across [Workers](https://developers.cloudflare.com/r2/api/workers/), [S3 API](https://developers.cloudflare.com/r2/api/s3/), and [public buckets](https://developers.cloudflare.com/r2/buckets/public-buckets/):\n\n* When an object is requested, it is served from your R2 bucket if it is found.\n* If the object is not found in R2, the object will simultaneously be returned from your source storage bucket and copied to R2.\n* All other operations, including put and delete, continue to work as usual.\n\n## When is Sippy useful?\n\nUsing Sippy as part of your migration strategy can be a good choice when:\n\n* You want to start migrating your data, but you want to avoid paying upfront egress fees to facilitate the migration of your data all at once.\n* You want to experiment by serving frequently accessed objects from R2 to eliminate egress fees, without investing time in data migration.\n* You have frequently changing data and are looking to conduct a migration while avoiding downtime. Sippy can be used to serve requests while [Super Slurper](https://developers.cloudflare.com/r2/data-migration/super-slurper/) can be used to migrate your remaining data.\n\nIf you are looking to migrate all of your data from an existing cloud provider to R2 at one time, we recommend using [Super Slurper](https://developers.cloudflare.com/r2/data-migration/super-slurper/).\n\n## Get started with Sippy\n\nBefore getting started, you will need:\n\n* An existing R2 bucket. If you don't already have one, refer to [Create buckets](https://developers.cloudflare.com/r2/buckets/create-buckets/).\n* [API credentials](https://developers.cloudflare.com/r2/data-migration/sippy/#create-credentials-for-storage-providers) for your source object storage bucket.\n* (Wrangler only) Cloudflare R2 Access Key ID and Secret Access Key with read and write permissions. For more information, refer to [Authentication](https://developers.cloudflare.com/r2/api/tokens/).\n\n### Enable Sippy via the Dashboard\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Select the bucket you'd like to migrate objects to.\n\n3. Switch to the **Settings** tab, then scroll down to the **On Demand Migration** card.\n\n4. Select **Enable** and enter details for the AWS / GCS bucket you'd like to migrate objects from. The credentials you enter must have permissions to read from this bucket. Cloudflare also recommends scoping your credentials to only allow reads from this bucket.\n\n5. Select **Enable**.\n\n### Enable Sippy via Wrangler\n\n#### Set up Wrangler\n\nTo begin, install [`npm`](https://docs.npmjs.com/getting-started). Then [install Wrangler, the Developer Platform CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/).\n\n#### Enable Sippy on your R2 bucket\n\nLog in to Wrangler with the [`wrangler login` command](https://developers.cloudflare.com/workers/wrangler/commands/#login). Then run the [`r2 bucket sippy enable` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-sippy-enable):",
      "language": "unknown"
    },
    {
      "code": "This will prompt you to select between supported object storage providers and lead you through setup.\n\n### Enable Sippy via API\n\nFor information on required parameters and examples of how to enable Sippy, refer to the [API documentation](https://developers.cloudflare.com/api/resources/r2/subresources/buckets/subresources/sippy/methods/update/). For information about getting started with the Cloudflare API, refer to [Make API calls](https://developers.cloudflare.com/fundamentals/api/how-to/make-api-calls/).\n\nNote\n\nIf your bucket is setup with [jurisdictional restrictions](https://developers.cloudflare.com/r2/reference/data-location/#jurisdictional-restrictions), you will need to pass a `cf-r2-jurisdiction` request header with that jurisdiction. For example, `cf-r2-jurisdiction: eu`.\n\n### View migration metrics\n\nWhen enabled, Sippy exposes metrics that help you understand the progress of your ongoing migrations.\n\n| Metric | Description |\n| - | - |\n| Requests served by Sippy | The percentage of overall requests served by R2 over a period of time. A higher percentage indicates that fewer requests need to be made to the source bucket. |\n| Data migrated by Sippy | The amount of data that has been copied from the source bucket to R2 over a period of time. Reported in bytes. |\n\nTo view current and historical metrics:\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Select your bucket.\n\n3. Select the **Metrics** tab.\n\nYou can optionally select a time window to query. This defaults to the last 24 hours.\n\n## Disable Sippy on your R2 bucket\n\n### Dashboard\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Select the bucket you'd like to disable Sippy for.\n\n3. Switch to the **Settings** tab and scroll down to the **On Demand Migration** card.\n\n4. Press **Disable**.\n\n### Wrangler\n\nTo disable Sippy, run the [`r2 bucket sippy disable` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-sippy-disable):",
      "language": "unknown"
    },
    {
      "code": "### API\n\nFor more information on required parameters and examples of how to disable Sippy, refer to the [API documentation](https://developers.cloudflare.com/api/resources/r2/subresources/buckets/subresources/sippy/methods/delete/).\n\n## Supported cloud storage providers\n\nCloudflare currently supports copying data from the following cloud object storage providers to R2:\n\n* Amazon S3\n* Google Cloud Storage (GCS)\n\n## R2 API interactions\n\nWhen Sippy is enabled, it changes the behavior of certain actions on your R2 bucket across [Workers](https://developers.cloudflare.com/r2/api/workers/), [S3 API](https://developers.cloudflare.com/r2/api/s3/), and [public buckets](https://developers.cloudflare.com/r2/buckets/public-buckets/).\n\n| Action | New behavior |\n| - | - |\n| GetObject | Calls to GetObject will first attempt to retrieve the object from your R2 bucket. If the object is not present, the object will be served from the source storage bucket and simultaneously uploaded to the requested R2 bucket. Additional considerations:- Modifications to objects in the source bucket will not be reflected in R2 after the initial copy. Once an object is stored in R2, it will not be re-retrieved and updated.\n- Only user-defined metadata that is prefixed by `x-amz-meta-` in the HTTP response will be migrated. Remaining metadata will be omitted.\n- For larger objects (greater than 199 MiB), multiple GET requests may be required to fully copy the object to R2.\n- If there are multiple simultaneous GET requests for an object which has not yet been fully copied to R2, Sippy may fetch the object from the source storage bucket multiple times to serve those requests. |\n| HeadObject | Behaves similarly to GetObject, but only retrieves object metadata. Will not copy objects to the requested R2 bucket. |\n| PutObject | No change to behavior. Calls to PutObject will add objects to the requested R2 bucket. |\n| DeleteObject | No change to behavior. Calls to DeleteObject will delete objects in the requested R2 bucket. Additional considerations:- If deletes to objects in R2 are not also made in the source storage bucket, subsequent GetObject requests will result in objects being retrieved from the source bucket and copied to R2. |\n\nActions not listed above have no change in behavior. For more information, refer to [Workers API reference](https://developers.cloudflare.com/r2/api/workers/workers-api-reference/) or [S3 API compatibility](https://developers.cloudflare.com/r2/api/s3/api/).\n\n## Create credentials for storage providers\n\n### Amazon S3\n\nTo copy objects from Amazon S3, Sippy requires access permissions to your bucket. While you can use any AWS Identity and Access Management (IAM) user credentials with the correct permissions, Cloudflare recommends you create a user with a narrow set of permissions.\n\nTo create credentials with the correct permissions:\n\n1. Log in to your AWS IAM account.\n\n2. Create a policy with the following format and replace `<BUCKET_NAME>` with the bucket you want to grant access to:",
      "language": "unknown"
    },
    {
      "code": "3. Create a new user and attach the created policy to that user.\n\nYou can now use both the Access Key ID and Secret Access Key when enabling Sippy.\n\n### Google Cloud Storage\n\nTo copy objects from Google Cloud Storage (GCS), Sippy requires access permissions to your bucket. Cloudflare recommends using the Google Cloud predefined `Storage Object Viewer` role.\n\nTo create credentials with the correct permissions:\n\n1. Log in to your Google Cloud console.\n2. Go to **IAM & Admin** > **Service Accounts**.\n3. Create a service account with the predefined `Storage Object Viewer` role.\n4. Go to the **Keys** tab of the service account you created.\n5. Select **Add Key** > **Create a new key** and download the JSON key file.\n\nYou can now use this JSON key file when enabling Sippy via Wrangler or API.\n\n## Caveats\n\n### ETags\n\nWhile R2's ETag generation is compatible with S3's during the regular course of operations, ETags are not guaranteed to be equal when an object is migrated using Sippy. Sippy makes autonomous decisions about the operations it uses when migrating objects to optimize for performance and network usage. It may choose to migrate an object in multiple parts, which affects [ETag calculation](https://developers.cloudflare.com/r2/objects/multipart-objects#etags).\n\nFor example, a 320 MiB object originally uploaded to S3 using a single `PutObject` operation might be migrated to R2 via multipart operations. In this case, its ETag on R2 will not be the same as its ETag on S3. Similarly, an object originally uploaded to S3 using multipart operations might also have a different ETag on R2 if the part sizes Sippy chooses for its migration differ from the part sizes this object was originally uploaded with.\n\nRelying on matching ETags before and after the migration is therefore discouraged.\n\n</page>\n\n<page>\n---\ntitle: Connect to Iceberg engines · Cloudflare R2 docs\ndescription: Find detailed setup instructions for Apache Spark and other common\n  query engines.\nlastUpdated: 2025-09-25T04:10:41.000Z\nchatbotDeprioritize: true\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-catalog/config-examples/\n  md: https://developers.cloudflare.com/r2/data-catalog/config-examples/index.md\n---\n\nBelow are configuration examples to connect various Iceberg engines to [R2 Data Catalog](https://developers.cloudflare.com/r2/data-catalog/):\n\n* [Apache Trino](https://developers.cloudflare.com/r2/data-catalog/config-examples/trino/)\n* [DuckDB](https://developers.cloudflare.com/r2/data-catalog/config-examples/duckdb/)\n* [PyIceberg](https://developers.cloudflare.com/r2/data-catalog/config-examples/pyiceberg/)\n* [Snowflake](https://developers.cloudflare.com/r2/data-catalog/config-examples/snowflake/)\n* [Spark (PySpark)](https://developers.cloudflare.com/r2/data-catalog/config-examples/spark-python/)\n* [Spark (Scala)](https://developers.cloudflare.com/r2/data-catalog/config-examples/spark-scala/)\n* [StarRocks](https://developers.cloudflare.com/r2/data-catalog/config-examples/starrocks/)\n\n</page>\n\n<page>\n---\ntitle: Deleting data · Cloudflare R2 docs\ndescription: How to properly delete data from R2 Data Catalog\nlastUpdated: 2025-12-18T17:16:51.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-catalog/deleting-data/\n  md: https://developers.cloudflare.com/r2/data-catalog/deleting-data/index.md\n---\n\n## Deleting data in R2 Data Catalog\n\nDeleting data from R2 Data Catalog or any Apache Iceberg catalog requires that operations are done in a transaction through the catalog itself. Manually deleting metadata or data files directly can lead to data catalog corruption.\n\n## Examples of enabling automatic table maintenance in R2 Data Catalog",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Event notification rules",
      "id": "event-notification-rules"
    },
    {
      "level": "h2",
      "text": "Event types",
      "id": "event-types"
    },
    {
      "level": "h2",
      "text": "Message format",
      "id": "message-format"
    },
    {
      "level": "h3",
      "text": "Properties",
      "id": "properties"
    },
    {
      "level": "h2",
      "text": "Notes",
      "id": "notes"
    },
    {
      "level": "h2",
      "text": "Behavior",
      "id": "behavior"
    },
    {
      "level": "h2",
      "text": "Configure lifecycle rules for your bucket",
      "id": "configure-lifecycle-rules-for-your-bucket"
    },
    {
      "level": "h3",
      "text": "Dashboard",
      "id": "dashboard"
    },
    {
      "level": "h3",
      "text": "Wrangler",
      "id": "wrangler"
    },
    {
      "level": "h3",
      "text": "S3 API",
      "id": "s3-api"
    },
    {
      "level": "h2",
      "text": "Get lifecycle rules for your bucket",
      "id": "get-lifecycle-rules-for-your-bucket"
    },
    {
      "level": "h3",
      "text": "Wrangler",
      "id": "wrangler"
    },
    {
      "level": "h3",
      "text": "S3 API",
      "id": "s3-api"
    },
    {
      "level": "h2",
      "text": "Delete lifecycle rules from your bucket",
      "id": "delete-lifecycle-rules-from-your-bucket"
    },
    {
      "level": "h3",
      "text": "Dashboard",
      "id": "dashboard"
    },
    {
      "level": "h3",
      "text": "Wrangler",
      "id": "wrangler"
    },
    {
      "level": "h3",
      "text": "S3 API",
      "id": "s3-api"
    },
    {
      "level": "h2",
      "text": "Custom domains",
      "id": "custom-domains"
    },
    {
      "level": "h3",
      "text": "Caching",
      "id": "caching"
    },
    {
      "level": "h3",
      "text": "Access control",
      "id": "access-control"
    },
    {
      "level": "h3",
      "text": "Minimum TLS Version & Cipher Suites",
      "id": "minimum-tls-version-&-cipher-suites"
    },
    {
      "level": "h2",
      "text": "Add your domain to Cloudflare",
      "id": "add-your-domain-to-cloudflare"
    },
    {
      "level": "h2",
      "text": "Connect a bucket to a custom domain",
      "id": "connect-a-bucket-to-a-custom-domain"
    },
    {
      "level": "h2",
      "text": "Disable domain access",
      "id": "disable-domain-access"
    },
    {
      "level": "h2",
      "text": "Remove domain",
      "id": "remove-domain"
    },
    {
      "level": "h2",
      "text": "Public development URL",
      "id": "public-development-url"
    },
    {
      "level": "h3",
      "text": "Enable public development url",
      "id": "enable-public-development-url"
    },
    {
      "level": "h3",
      "text": "Disable public development url",
      "id": "disable-public-development-url"
    },
    {
      "level": "h2",
      "text": "Available storage classes",
      "id": "available-storage-classes"
    },
    {
      "level": "h3",
      "text": "Standard storage",
      "id": "standard-storage"
    },
    {
      "level": "h3",
      "text": "Infrequent Access storage",
      "id": "infrequent-access-storage"
    },
    {
      "level": "h2",
      "text": "Set default storage class for buckets",
      "id": "set-default-storage-class-for-buckets"
    },
    {
      "level": "h2",
      "text": "Set storage class for objects",
      "id": "set-storage-class-for-objects"
    },
    {
      "level": "h3",
      "text": "Specify storage class during object upload",
      "id": "specify-storage-class-during-object-upload"
    },
    {
      "level": "h3",
      "text": "Use object lifecycle rules to transition objects to Infrequent Access storage",
      "id": "use-object-lifecycle-rules-to-transition-objects-to-infrequent-access-storage"
    },
    {
      "level": "h2",
      "text": "Change storage class for objects",
      "id": "change-storage-class-for-objects"
    },
    {
      "level": "h3",
      "text": "When the source bucket is actively being read from / written to",
      "id": "when-the-source-bucket-is-actively-being-read-from-/-written-to"
    },
    {
      "level": "h3",
      "text": "When the source bucket is not being read often",
      "id": "when-the-source-bucket-is-not-being-read-often"
    },
    {
      "level": "h3",
      "text": "Optimizing your Slurper data migration performance",
      "id": "optimizing-your-slurper-data-migration-performance"
    },
    {
      "level": "h2",
      "text": "When to use Super Slurper",
      "id": "when-to-use-super-slurper"
    },
    {
      "level": "h2",
      "text": "Use Super Slurper to migrate data to R2",
      "id": "use-super-slurper-to-migrate-data-to-r2"
    },
    {
      "level": "h3",
      "text": "Source bucket options",
      "id": "source-bucket-options"
    },
    {
      "level": "h3",
      "text": "Destination R2 bucket options",
      "id": "destination-r2-bucket-options"
    },
    {
      "level": "h2",
      "text": "Supported cloud storage providers",
      "id": "supported-cloud-storage-providers"
    },
    {
      "level": "h3",
      "text": "Tested S3-compatible storage providers",
      "id": "tested-s3-compatible-storage-providers"
    },
    {
      "level": "h2",
      "text": "Create credentials for storage providers",
      "id": "create-credentials-for-storage-providers"
    },
    {
      "level": "h3",
      "text": "Amazon S3",
      "id": "amazon-s3"
    },
    {
      "level": "h3",
      "text": "Google Cloud Storage",
      "id": "google-cloud-storage"
    },
    {
      "level": "h2",
      "text": "Caveats",
      "id": "caveats"
    },
    {
      "level": "h3",
      "text": "ETags",
      "id": "etags"
    },
    {
      "level": "h3",
      "text": "Archive storage classes",
      "id": "archive-storage-classes"
    },
    {
      "level": "h2",
      "text": "How it works",
      "id": "how-it-works"
    },
    {
      "level": "h2",
      "text": "When is Sippy useful?",
      "id": "when-is-sippy-useful?"
    },
    {
      "level": "h2",
      "text": "Get started with Sippy",
      "id": "get-started-with-sippy"
    },
    {
      "level": "h3",
      "text": "Enable Sippy via the Dashboard",
      "id": "enable-sippy-via-the-dashboard"
    },
    {
      "level": "h3",
      "text": "Enable Sippy via Wrangler",
      "id": "enable-sippy-via-wrangler"
    },
    {
      "level": "h3",
      "text": "Enable Sippy via API",
      "id": "enable-sippy-via-api"
    },
    {
      "level": "h3",
      "text": "View migration metrics",
      "id": "view-migration-metrics"
    },
    {
      "level": "h2",
      "text": "Disable Sippy on your R2 bucket",
      "id": "disable-sippy-on-your-r2-bucket"
    },
    {
      "level": "h3",
      "text": "Dashboard",
      "id": "dashboard"
    },
    {
      "level": "h3",
      "text": "Wrangler",
      "id": "wrangler"
    },
    {
      "level": "h3",
      "text": "API",
      "id": "api"
    },
    {
      "level": "h2",
      "text": "Supported cloud storage providers",
      "id": "supported-cloud-storage-providers"
    },
    {
      "level": "h2",
      "text": "R2 API interactions",
      "id": "r2-api-interactions"
    },
    {
      "level": "h2",
      "text": "Create credentials for storage providers",
      "id": "create-credentials-for-storage-providers"
    },
    {
      "level": "h3",
      "text": "Amazon S3",
      "id": "amazon-s3"
    },
    {
      "level": "h3",
      "text": "Google Cloud Storage",
      "id": "google-cloud-storage"
    },
    {
      "level": "h2",
      "text": "Caveats",
      "id": "caveats"
    },
    {
      "level": "h3",
      "text": "ETags",
      "id": "etags"
    },
    {
      "level": "h2",
      "text": "Deleting data in R2 Data Catalog",
      "id": "deleting-data-in-r2-data-catalog"
    },
    {
      "level": "h2",
      "text": "Examples of enabling automatic table maintenance in R2 Data Catalog",
      "id": "examples-of-enabling-automatic-table-maintenance-in-r2-data-catalog"
    }
  ],
  "url": "llms-txt#filter-using-prefix-and-suffix.-both-the-conditions-will-be-used-for-filtering",
  "links": []
}