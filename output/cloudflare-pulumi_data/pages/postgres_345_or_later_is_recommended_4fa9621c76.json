{
  "title": "postgres 3.4.5 or later is recommended",
  "content": "npm i drizzle-orm postgres dotenv\nnpm i -D drizzle-kit tsx @types/node\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n   // src/db/schema.ts\n   import { pgTable, serial, varchar, timestamp } from \"drizzle-orm/pg-core\";\n\nexport const users = pgTable(\"users\", {\n     id: serial(\"id\").primaryKey(),\n     name: varchar(\"name\", { length: 255 }).notNull(),\n     email: varchar(\"email\", { length: 255 }).notNull().unique(),\n     createdAt: timestamp(\"created_at\").defaultNow(),\n   });\n   ts\n// src/index.ts\nimport { drizzle } from \"drizzle-orm/postgres-js\";\nimport postgres from \"postgres\";\nimport { users } from \"./db/schema\";\n\nexport interface Env {\n  HYPERDRIVE: Hyperdrive;\n}\n\nexport default {\n  async fetch(request, env, ctx): Promise<Response> {\n    // Create a database client with postgres.js driver connected via Hyperdrive\n    const sql = postgres(env.HYPERDRIVE.connectionString, {\n      // Limit the connections for the Worker request to 5 due to Workers' limits on concurrent external connections\n      max: 5,\n      // If you are not using array types in your Postgres schema, disable `fetch_types` to avoid an additional round-trip (unnecessary latency)\n      fetch_types: false,\n    });\n\n// Create the Drizzle client with the postgres.js connection\n    const db = drizzle(sql);\n\n// Sample query to get all users\n    const allUsers = await db.select().from(users);\n\nreturn Response.json(allUsers);\n  },\n} satisfies ExportedHandler<Env>;\ntoml\n   # .env\n   # Replace with your direct database connection string\n   DATABASE_URL='postgres://user:password@db-host.cloud/database-name'\n   ts\n   // drizzle.config.ts\n   import \"dotenv/config\";\n   import { defineConfig } from \"drizzle-kit\";\n   export default defineConfig({\n     out: \"./drizzle\",\n     schema: \"./src/db/schema.ts\",\n     dialect: \"postgresql\",\n     dbCredentials: {\n       url: process.env.DATABASE_URL!,\n     },\n   });\n   bash\n   npx drizzle-kit generate\n   bash\n   No config path provided, using default 'drizzle.config.ts'\n   Reading config file 'drizzle.config.ts'\n   1 tables\n   users 4 columns 0 indexes 0 fks\n\n[âœ“] Your SQL migration file âžœ drizzle/0000_mysterious_queen_noir.sql ðŸš€\n   bash\n   npx drizzle-kit migrate\n   bash\n   No config path provided, using default 'drizzle.config.ts'\n   Reading config file 'drizzle.config.ts'\n   Using 'postgres' driver for database querying\n   bash\nnpx wrangler deploy\nsh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\nsh\n  npm i postgres@>3.4.5\n  sh\n  yarn add postgres@>3.4.5\n  sh\n  pnpm add postgres@>3.4.5\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport postgres from \"postgres\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a database client that connects to your database via Hyperdrive\n    // using the Hyperdrive credentials\n    const sql = postgres(env.HYPERDRIVE.connectionString, {\n      // Limit the connections for the Worker request to 5 due to Workers' limits on concurrent external connections\n      max: 5,\n      // If you are not using array types in your Postgres schema, disable `fetch_types` to avoid an additional round-trip (unnecessary latency)\n      fetch_types: false,\n\n// This is set to true by default, but certain query generators such as Kysely or queries using sql.unsafe() will set this to false. Hyperdrive will not cache prepared statements when this option is set to false and will require additional round-trips.\n      prepare: true,\n    });\n\ntry {\n      // A very simple test query\n      const result = await sql`select * from pg_tables`;\n\n// Return result rows as JSON\n      return Response.json({ success: true, result: result });\n    } catch (e: any) {\n      console.error(\"Database error:\", e.message);\n\nreturn Response.error();\n    }\n  },\n} satisfies ExportedHandler<Env>;\nsh\n  npm i -D prisma\n  sh\n  yarn add -D prisma\n  sh\n  pnpm add -D prisma\n  sh\n  npm i pg@>8.13.0 @prisma/adapter-pg\n  sh\n  yarn add pg@>8.13.0 @prisma/adapter-pg\n  sh\n  pnpm add pg@>8.13.0 @prisma/adapter-pg\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  sh\nnpx prisma init\nprisma\ngenerator client {\n  provider        = \"prisma-client-js\"\n  previewFeatures = [\"driverAdapters\"]\n}\n\ndatasource db {\n  provider = \"postgresql\"\n  url      = env(\"DATABASE_URL\")\n}\n\nmodel User {\n  id        Int      @id @default(autoincrement())\n  name      String\n  email     String   @unique\n  createdAt DateTime @default(now())\n}\ntxt\nDATABASE_URL=\"postgres://user:password@host:port/database\"\njson\n\"scripts\": {\n  \"migrate\": \"npx prisma migrate dev\",\n  \"generate\": \"npx prisma generate --no-engine\",\n  \"studio\": \"npx prisma studio\"\n}\nsh\nnpm run generate\nsh\nnpm run migrate\nts\nimport { PrismaPg } from \"@prisma/adapter-pg\";\nimport { PrismaClient } from \"@prisma/client\";\n\nexport interface Env {\n  HYPERDRIVE: Hyperdrive;\n}\n\nexport default {\n  async fetch(request, env, ctx): Promise<Response> {\n    // Create Prisma client using driver adapter with Hyperdrive connection string\n    const adapter = new PrismaPg({ connectionString: env.HYPERDRIVE.connectionString });\n    const prisma = new PrismaClient({ adapter });\n\n// Sample query to create and fetch users\n    const user = await prisma.user.create({\n      data: {\n        name: \"John Doe\",\n        email: `john.doe.${Date.now()}@example.com`,\n      },\n    });\n\nconst allUsers = await prisma.user.findMany();\n\nreturn Response.json({\n      newUser: user,\n      allUsers: allUsers,\n    });\n  },\n} satisfies ExportedHandler<Env>;\nbash\nnpx wrangler deploy\njson\n{\n  \"origin_steering\": {\n    \"policy\": \"least_outstanding_requests\"\n  }\n}\njson\n  // PUT /zones/:zone_id/load_balancers\n  {\n    \"description\": \"Load Balancer for www.example.com\",\n    \"name\": \"www.example.com\",\n    \"ttl\": 30,\n    \"proxied\": true,\n    \"fallback_pool\": \"ff02c959d17f7bb2b1184a202e3c0af7\",\n    \"default_pools\": [\n      \"17b5962d775c646f3f9725cbc7a53df4\",\n      \"ff02c959d17f7bb2b1184a202e3c0af7\"\n    ],\n    \"region_pools\": {\n      \"WNAM\": [\n        \"17b5962d775c646f3f9725cbc7a53df4\",\n        \"ff02c959d17f7bb2b1184a202e3c0af7\"\n      ],\n      \"ENAM\": [\n        \"17b5962d775c646f3f9725cbc7a53df4\",\n        \"ff02c959d17f7bb2b1184a202e3c0af7\"\n      ],\n      \"EEU\": [\n        \"ff02c959d17f7bb2b1184a202e3c0af7\",\n        \"17b5962d775c646f3f9725cbc7a53df4\"\n      ]\n    }\n  }\n  json\n{\n  \"steering_policy\": \"least_outstanding_requests\"\n}\njson\n{\n  \"Id\": \"<POLICY_ID>\",\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1506627150918\",\n      \"Action\": [\"s3:PutObject\"],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::burritobot/logs/*\",\n      \"Principal\": {\n        \"AWS\": [\"arn:aws:iam::391854517948:user/cloudflare-logpush\"]\n      }\n    }\n  ]\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/settings/aegis\" \\\n  --request PATCH \\\n  --header \"Authorization: Bearer $CLOUDFLARE_API_TOKEN\" \\\n  --json '{\n    \"id\": \"aegis\",\n    \"value\": {\n        \"enabled\": true,\n        \"pool_id\": \"<YOUR_EGRESS_POOL_ID>\"\n    }\n  }'\ntxt\nhttps://logpush.yourdestinationendpoint.com?header_X-Logpush-Secret=YOUR_RANDOM_SECRET_TOKEN\ntxt\n  (http.host eq \"logpush.yourdestinationendpoint.com\" and all(http.request.headers[\"x-logpush-secret\"][*] ne \"YOUR_RANDOM_SECRET_TOKEN\"))\n  txt\n  (http.host eq \"logpush.yourdestinationendpoint.com\" and not ip.geoip.asnum in {13335 132892})\n  txt\nhttps://logpush.yourdestinationendpoint.com?header_CF-Access-Client-Id=YOUR_CLIENT_ID&header_CF-Access-Client-Secret=YOUR_CLIENT_SECRET\nbash\n$ curl https://logpush.yourdestinationendpoint.com",
  "code_samples": [
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 2. Configure Drizzle\n\n### 2.1. Define a schema\n\nWith Drizzle ORM, we define the schema in TypeScript rather than writing raw SQL.\n\n1. Create a folder `/db/` in `/src/`.\n\n2. Create a `schema.ts` file.\n\n3. In `schema.ts`, define a `users` table as shown below.",
      "language": "unknown"
    },
    {
      "code": "### 2.2. Connect Drizzle ORM to the database with Hyperdrive\n\nUse your Hyperdrive configuration for your database when using the Drizzle ORM.\n\nPopulate your `index.ts` file as shown below.",
      "language": "unknown"
    },
    {
      "code": "Note\n\nYou may use [node-postgres](https://orm.drizzle.team/docs/get-started-postgresql#node-postgres) or [Postgres.js](https://orm.drizzle.team/docs/get-started-postgresql#postgresjs) when using Drizzle ORM. Both are supported and compatible.\n\n### 2.3. Configure Drizzle-Kit for migrations (optional)\n\nNote\n\nYou need to set up the tables in your database so that Drizzle ORM can make queries that work.\n\nIf you have already set it up (for example, if another user has applied the schema to your database), or if you are starting to use Drizzle ORM and the schema matches what already exists in your database, then you do not need to run the migration.\n\nYou can generate and run SQL migrations on your database based on your schema using Drizzle Kit CLI. Refer to [Drizzle ORM docs](https://orm.drizzle.team/docs/get-started/postgresql-new) for additional guidance.\n\n1. Create a `.env` file the root folder of your project, and add your database connection string. The Drizzle Kit CLI will use this connection string to create and apply the migrations.",
      "language": "unknown"
    },
    {
      "code": "2. Create a `drizzle.config.ts` file in the root folder of your project to configure Drizzle Kit and add the following content:",
      "language": "unknown"
    },
    {
      "code": "3. Generate the migration file for your database according to your schema files and apply the migrations to your database.\n\n   Run the following two commands:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## 3. Deploy your Worker\n\nDeploy your Worker.",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: node-postgres (pg) Â· Cloudflare Hyperdrive docs\ndescription: node-postgres (pg) is a widely-used PostgreSQL driver for Node.js\n  applications. This example demonstrates how to use node-postgres with\n  Cloudflare Hyperdrive in a Workers application.\nlastUpdated: 2025-11-11T16:21:10.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/node-postgres/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/node-postgres/index.md\n---\n\n[node-postgres](https://node-postgres.com/) (pg) is a widely-used PostgreSQL driver for Node.js applications. This example demonstrates how to use node-postgres with Cloudflare Hyperdrive in a Workers application.\n\nRecommended driver\n\n[Node-postgres](https://node-postgres.com/) (`pg`) is the recommended driver for connecting to your Postgres database from JavaScript or TypeScript Workers. It has the best compatibility with Hyperdrive's caching and is commonly available with popular ORM libraries. [Postgres.js](https://github.com/porsager/postgres) is also supported.\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Postgres.js Â· Cloudflare Hyperdrive docs\ndescription: Postgres.js is a modern, fully-featured PostgreSQL driver for\n  Node.js. This example demonstrates how to use Postgres.js with Cloudflare\n  Hyperdrive in a Workers application.\nlastUpdated: 2025-11-11T16:21:10.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/postgres-js/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/postgres-js/index.md\n---\n\n[Postgres.js](https://github.com/porsager/postgres) is a modern, fully-featured PostgreSQL driver for Node.js. This example demonstrates how to use Postgres.js with Cloudflare Hyperdrive in a Workers application.\n\nRecommended driver\n\n[Node-postgres](https://node-postgres.com/) (`pg`) is the recommended driver for connecting to your Postgres database from JavaScript or TypeScript Workers. It has the best compatibility with Hyperdrive's caching and is commonly available with popular ORM libraries. [Postgres.js](https://github.com/porsager/postgres) is also supported.\n\nInstall [Postgres.js](https://github.com/porsager/postgres):\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `postgres-js` required for Hyperdrive is `3.4.5`.\n\nAdd the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a Worker that connects to your PostgreSQL database via Hyperdrive:",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Prisma ORM Â· Cloudflare Hyperdrive docs\ndescription: Prisma ORM is a Node.js and TypeScript ORM with a focus on type\n  safety and developer experience. This example demonstrates how to use Prisma\n  ORM with PostgreSQL via Cloudflare Hyperdrive in a Workers application.\nlastUpdated: 2025-09-16T09:16:24.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/prisma-orm/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/prisma-orm/index.md\n---\n\n[Prisma ORM](https://www.prisma.io/docs) is a Node.js and TypeScript ORM with a focus on type safety and developer experience. This example demonstrates how to use Prisma ORM with PostgreSQL via Cloudflare Hyperdrive in a Workers application.\n\n## Prerequisites\n\n* A Cloudflare account with Workers access\n* A PostgreSQL database (such as [Prisma Postgres](https://www.prisma.io/postgres))\n* A [Hyperdrive configuration to your PostgreSQL database](https://developers.cloudflare.com/hyperdrive/get-started/#3-connect-hyperdrive-to-a-database)\n* An existing [Worker project](https://developers.cloudflare.com/workers/get-started/guide/)\n\n## 1. Install Prisma ORM\n\nInstall Prisma CLI as a dev dependency:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Install the `pg` driver and Prisma driver adapter for use with Hyperdrive:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "If using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.toml` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 2. Configure Prisma ORM\n\n### 2.1. Initialize Prisma\n\nInitialize Prisma in your application:",
      "language": "unknown"
    },
    {
      "code": "This creates a `prisma` folder with a `schema.prisma` file and an `.env` file.\n\n### 2.2. Define a schema\n\nDefine your database schema in the `prisma/schema.prisma` file:",
      "language": "unknown"
    },
    {
      "code": "### 2.3. Set up environment variables\n\nAdd your database connection string to the `.env` file created by Prisma:",
      "language": "unknown"
    },
    {
      "code": "Add helper scripts to your `package.json`:",
      "language": "unknown"
    },
    {
      "code": "### 2.4. Generate Prisma Client\n\nGenerate the Prisma client with driver adapter support:",
      "language": "unknown"
    },
    {
      "code": "### 2.5. Run migrations\n\nGenerate and apply the database schema:",
      "language": "unknown"
    },
    {
      "code": "When prompted, provide a name for the migration (for example, `init`).\n\n## 3. Connect Prisma ORM to Hyperdrive\n\nUse your Hyperdrive configuration when using Prisma ORM. Update your `src/index.ts` file:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nWhen using Prisma ORM with Hyperdrive, you must use driver adapters to properly utilize the Hyperdrive connection string. The `@prisma/adapter-pg` driver adapter allows Prisma ORM to work with the `pg` driver and Hyperdrive's connection pooling. This approach provides connection pooling at the network level through Hyperdrive, so you don't need Prisma-specific connection pooling extensions like Prisma Accelerate.\n\n## 4. Deploy your Worker\n\nDeploy your Worker:",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Hash steering Â· Cloudflare Load Balancing docs\ndescription: Hash steering guides Cloudflare to send requests to endpoints based\n  on a combination of endpoint weights and previous requests from that IP\n  address. Ensures requests from the same IP address will hit the same endpoint,\n  but actual traffic distribution may differ from endpoint weights.\nlastUpdated: 2025-02-27T15:51:29.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/hash-origin-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/hash-origin-steering/index.md\n---\n\n**Hash steering** guides Cloudflare to send requests to endpoints based on a combination of [endpoint weights](https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/#weights) and previous requests from that IP address. Ensures requests from the same IP address will hit the same endpoint, but actual traffic distribution may differ from endpoint weights.\n\n## Limitation when using Workers\n\nHash Steering relies on the `x-forwarded-for` header to determine the originating IP address of a request. However, when a [Cloudflare Worker](https://developers.cloudflare.com/workers/) is used in front of a load balancer, this can affect how Hash Steering functions.\n\nWhen a request originates from a browser, it lacks an `x-forwarded-for` header, but if a Worker proxies the request to a load balancer, the header is populated with the Worker's IP instead of the original client IP. Since the Worker's IP â€” often a Cloudflare public IP â€” can change between requests, Hash Steering may direct the same client's requests to different endpoints, leading to inconsistent traffic routing.\n\n### Workaround\n\nTo ensure Hash Steering works correctly when using a Worker in front of a Load Balancer, manually set the `x-forwarded-for` header in the Worker to the client's original IP address. By manually setting `x-forwarded-for` to `CF-Connecting-IP`, Hash Steering will function as expected, ensuring traffic consistency for end users.\n\n</page>\n\n<page>\n---\ntitle: Least Outstanding Requests steering - Endpoint-level steering Â·\n  Cloudflare Load Balancing docs\ndescription: Least Outstanding Requests steering allows you to route traffic to\n  endpoints that currently have the lowest number of outstanding requests.\nlastUpdated: 2025-02-10T10:26:10.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/least-outstanding-requests-pools/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/least-outstanding-requests-pools/index.md\n---\n\n**Least Outstanding Requests steering** allows you to route traffic to endpoints that currently have the lowest number of outstanding requests.\n\nThis steering policy selects an endpoint by taking into consideration endpoint weights, as well as each endpoint's number of in-flight requests. Endpoints with more pending requests are weighted proportionately less in relation to others.\n\nLeast Outstanding Requests steering is best to use if your endpoints are easily overwhelmed by a spike in concurrent requests. It supports [adaptive routing](https://developers.cloudflare.com/load-balancing/understand-basics/adaptive-routing/) and [session affinity](https://developers.cloudflare.com/load-balancing/understand-basics/session-affinity/).\n\n## Configure via the API",
      "language": "unknown"
    },
    {
      "code": "Refer to the [API documentation](https://developers.cloudflare.com/api/resources/load_balancers/subresources/pools/methods/update/) for more information on the pool configuration.\n\nNote\n\nLeast Outstanding Requests steering can also be configured on a load balancer as a [global traffic steering policy](https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/least-outstanding-requests/), taking into account outstanding request counts and `random_steering` weights for pools on the load balancer.\n\n## Limitations\n\nLeast Outstanding Requests steering can be configured for pools that are part of [DNS-only load balancers](https://developers.cloudflare.com/load-balancing/understand-basics/proxy-modes/#dns-only-load-balancing), but is only supported in a no-operation form. When endpoint steering logic is applied for a pool on a DNS-only load balancer, all endpoint outstanding request counts are considered to be zero, meaning traffic is served solely based on endpoint weights.\n\nAlthough it is configurable, it is not recommended to associate pools that use Least Outstanding Requests steering with DNS-only load balancers due to its partial support.\n\n</page>\n\n<page>\n---\ntitle: Random steering Â· Cloudflare Load Balancing docs\ndescription: Random steering sends requests to endpoints purely based on\n  endpoint weights. Distributes traffic more accurately, but may cause requests\n  from the same IP to hit different endpoints.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/random-origin-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/random-origin-steering/index.md\n---\n\n**Random steering** sends requests to endpoints purely based on [endpoint weights](https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/#weights). Distributes traffic more accurately, but may cause requests from the same IP to hit different endpoints.\n\n</page>\n\n<page>\n---\ntitle: Dynamic steering Â· Cloudflare Load Balancing docs\ndescription: Dynamic steering uses health monitor data to identify the fastest\n  pool for a given Cloudflare Region or data center.\nlastUpdated: 2025-01-10T15:14:38.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/dynamic-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/dynamic-steering/index.md\n---\n\n**Dynamic steering** uses health monitor data to identify the fastest pool for a given Cloudflare Region or data center.\n\nDynamic steering creates Round Trip Time (RTT) profiles based on an exponential weighted moving average (EWMA) of RTT to determine the fastest pool. If there is no current RTT data for your pool in a region or colocation center, Cloudflare directs traffic to the pools in failover order.\n\nRTT values are collected each time a health probe request is made and based on the response from the endpoint to the monitor request. When a request is made, Cloudflare inspects the RTT data and uses it to sort pools by their RTT values.\n\nWhen enabling Dynamic steering the first time for a pool, allow 10 minutes for the change to take effect while Cloudflare builds an RTT profile for that pool.\n\nFor TCP health monitors, calculated latency may not reflect the true latency to the endpoint if you are terminating TCP at a cloud provider edge location.\n\nThe diagram below shows how Cloudflare would route traffic to the pool with the lowest EWMA among three regions: Eastern North America, Europe, and Australia. In this case, the ENAM pool is selected because it has the lowest RTT.\n\n![Dynamic steering routes traffic to the fastest available pool](https://developers.cloudflare.com/_astro/traffic-steering-2.CEeFHZfg_Z2bHxLO.webp)\n\nNote\n\nTo ensure dynamic steering works as expected, the [Health Monitor Region](https://developers.cloudflare.com/load-balancing/monitors/#health-monitor-regions) must be set to **All Regions**. The Enterprise-only **All Data Centers** option is also a viable alternative.\n\n</page>\n\n<page>\n---\ntitle: Geo steering Â· Cloudflare Load Balancing docs\ndescription: Geo steering directs traffic to pools tied to specific countries,\n  regions, or â€” for Enterprise customers only â€” data centers.\nlastUpdated: 2025-01-16T16:11:26.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/geo-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/geo-steering/index.md\n---\n\n**Geo steering** directs traffic to pools tied to specific countries, regions, or â€” for Enterprise customers only â€” data centers.\n\nThis option is extremely useful when you want site visitors to access the endpoint closest to them, which improves page-loading performance.\n\nNote\n\nCustom load balancing rules are incompatible with Geo steering. As a result, any custom rule applied to Geo-steered Load Balancers will not function as expected.\n\n## Pool assignment\n\nYou can assign multiple pools to the same area and the load balancer will use them in failover order. Any options not explicitly defined â€” whether in data centers, countries, or regions â€” will fall back to using default pools and failover.\n\n### Region steering\n\nCloudflare has [13 geographic regions](https://developers.cloudflare.com/load-balancing/reference/region-mapping-api/#list-of-load-balancer-regions) that span the world. The region of a client is determined by the region of the Cloudflare data center that answers the clientâ€™s DNS query.\n\nWarning\n\nIf you add a pool to a region, you cannot [delete this pool](https://developers.cloudflare.com/load-balancing/pools/create-pool/#delete-a-pool) until you remove it from the **Geo steering** configuration. The configuration is **not** automatically removed when you change to a different **Traffic Steering** method.\n\n* Dashboard\n\n  When [creating or editing a load balancer](https://developers.cloudflare.com/load-balancing/load-balancers/create-load-balancer/):\n\n  1. Go to the **Traffic steering** step.\n  2. Select **Geo steering**.\n  3. For **Region**, select a region > **Add Region**.\n  4. Select **Edit**.\n  5. Select a pool > **Add Pool**.\n  6. If adding multiple pools, re-order them into your preferred failback order.\n  7. (optional) Add more regions if needed.\n\n* API\n\n  Use the `regions_pool` property of the [Update Load Balancers](https://developers.cloudflare.com/api/resources/load_balancers/methods/update/) command to specify an array of regions. Specify each region using the [appropriate region code](https://developers.cloudflare.com/load-balancing/reference/region-mapping-api/#list-of-load-balancer-regions) followed by a list of endpoints to use for that region.\n\n  In the example below, `WNAM` and `ENAM` represent the West and East Coasts of North America, respectively.",
      "language": "unknown"
    },
    {
      "code": "If you only define `WNAM`, then traffic from the East Coast will be routed to the `default_pools`. You can test this using a client in each of those locations.\n\n### Country steering\n\n* Dashboard\n\n  When [creating or editing a load balancer](https://developers.cloudflare.com/load-balancing/load-balancers/create-load-balancer/):\n\n  1. Follow the [create a load balancer procedure](https://developers.cloudflare.com/load-balancing/load-balancers/create-load-balancer/#create-a-load-balancer) until you reach the **Traffic steering** step.\n  2. Select **Geo steering**.\n  3. For **Country**, select a country > **Add Region**.\n  4. Select **Edit**.\n  5. Select a pool > **Add Pool**.\n  6. If adding multiple pools, re-order them into your preferred failback order.\n  7. (optional) Add more countries if needed.\n\n* API\n\n  When creating a load balancer [via the API](https://developers.cloudflare.com/api/resources/load_balancers/methods/create/), include the `country_pools` object to map countries to a list of pool IDs (ordered by their failover priority).\n\n  To get a list of country codes, use the [Region API](https://developers.cloudflare.com/load-balancing/reference/region-mapping-api/).\n\n  Any country not explicitly defined will fall back to using the corresponding `region_pool` mapping (if it exists), then to the associated default pools.\n\n### PoP steering\n\nWhen creating a load balancer [via the API](https://developers.cloudflare.com/api/resources/load_balancers/methods/create/), include the `pop_pools` object to map Cloudflare data centers to a list of pool IDs (ordered by their failover priority).\n\nFor help finding data center identifiers, refer to [this community thread](https://community.cloudflare.com/t/is-there-a-way-to-retrieve-cloudflare-pops-list-and-locations-programmatically/234643).\n\nAny data center not explicitly defined will fall back to using the corresponding `country_pool`, then `region_pool` mapping (if it exists), and finally to associated default pools.\n\nNote\n\nPoP steering is only available to Enterprise customers and only accessible via the API.\n\n### Failover behavior\n\nA fallback pool will be used if there is only one pool in the same region and it is unavailable. If there are multiple pools in the same region, the order of the pools will be respected. For example, if the first pool is unavailable, the second pool will be used.\n\n</page>\n\n<page>\n---\ntitle: Least Outstanding Requests steering - Steering policies Â· Cloudflare Load\n  Balancing docs\ndescription: Least Outstanding Requests steering allows you to route traffic to\n  pools that currently have the lowest number of outstanding requests.\nlastUpdated: 2025-02-10T10:26:10.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/least-outstanding-requests/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/least-outstanding-requests/index.md\n---\n\n**Least Outstanding Requests steering** allows you to route traffic to pools that currently have the lowest number of outstanding requests.\n\nThis steering policy selects a pool by taking into consideration `random_steering` weights, as well as each pool's number of in-flight requests. Pools with more pending requests are weighted proportionately less in relation to others.\n\nLeast Outstanding Requests steering is best to use if your pools are easily overwhelmed by a spike in concurrent requests. This steering method lends itself to applications that value server health above latency, geographic alignment, or other metrics. It takes into account the [pool's health status](https://developers.cloudflare.com/load-balancing/understand-basics/health-details/#how-a-pool-becomes-unhealthy), [adaptive routing](https://developers.cloudflare.com/load-balancing/understand-basics/adaptive-routing/), and [session affinity](https://developers.cloudflare.com/load-balancing/understand-basics/session-affinity/).\n\n## Configure via the API",
      "language": "unknown"
    },
    {
      "code": "Refer to the [API documentation](https://developers.cloudflare.com/api/resources/load_balancers/methods/update/) for more information on the load balancer configuration.\n\nNote\n\nLeast Outstanding Requests steering can also be configured on a pool as a [local traffic steering policy](https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/least-outstanding-requests-pools/), taking into account outstanding request counts and weights for endpoints within the pool.\n\n## Limitations\n\nLeast Outstanding Requests steering can be configured for [DNS-only load balancers](https://developers.cloudflare.com/load-balancing/understand-basics/proxy-modes/#dns-only-load-balancing), but is only supported in a no-operation form. For DNS-only load balancers, all pool outstanding request counts are considered to be zero, meaning traffic is served solely based on `random_steering` weights.\n\nAlthough it is configurable, it is not recommended to use Least Outstanding Requests steering for DNS-only load balancers due to its partial support.\n\n</page>\n\n<page>\n---\ntitle: Proximity steering Â· Cloudflare Load Balancing docs\ndescription: Proximity steering routes visitors or internal services to the\n  closest physical data center.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/proximity-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/proximity-steering/index.md\n---\n\n**Proximity steering** routes visitors or internal services to the closest physical data center.\n\nTo use proximity steering on a load balancer, you first need to add GPS coordinates to each pool.\n\n## When to add proximity steering\n\n* For new pools, add GPS coordinates when you create a pool.\n* For existing pools, add GPS coordinates when [managing pools](https://developers.cloudflare.com/load-balancing/pools/create-pool/#edit-a-pool) or in the **Add Traffic steering** step of [creating a load balancer](https://developers.cloudflare.com/load-balancing/load-balancers/create-load-balancer/).\n\n## How to add proximity steering\n\nTo add coordinates when creating or editing a pool:\n\n1. Click the *Configure coordinates for Proximity Steering* dropdown.\n2. Enter the latitude and longitude or drag a marker on the map.\n3. Select **Save**.\n\nWarning:\n\nFor accurate proximity steering, add GPS coordinates to all pools within the same load balancer.\n\n</page>\n\n<page>\n---\ntitle: Standard traffic steering policies Â· Cloudflare Load Balancing docs\ndescription: Standard steering policies include Off - Failover and Random.\nlastUpdated: 2024-12-19T15:49:57.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/standard-options/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/standard-options/index.md\n---\n\n**Standard steering** policies include **Off - Failover** and **Random**.\n\nThese are the only steering policies available to non-Enterprise customers who have not purchased **Traffic steering**.\n\n## Off - Failover\n\nFailover steering uses the pool order to determine failover priority (the failover order).\n\nFailover directs traffic from unhealthy pools â€” determined by [health monitors](https://developers.cloudflare.com/load-balancing/monitors/) and the **Health Threshold** â€” to the next healthy pool in the configuration. Customers commonly use this option to set up [active - passive failover](https://developers.cloudflare.com/load-balancing/load-balancers/common-configurations/#active---passive-failover).\n\nIf all pools are marked unhealthy, Load Balancing will direct traffic to the fallback pool. The default fallback pool is the last pool listed in the Load Balancing configuration.\n\nIf no monitors are attached to the load balancer, it will direct traffic to the primary pool exclusively.\n\n### Failback behavior\n\nIn an active/standby setup, with two origin pools:\n\n* Traffic always routes to Pool 1 (the primary pool) unless it becomes unhealthy.\n* If Pool 1 is marked unhealthy, traffic shifts to Pool 2 (the standby pool).\n* Once Pool 1 becomes healthy again, traffic automatically shifts back to Pool 1, assuming no [session affinity](https://developers.cloudflare.com/load-balancing/understand-basics/session-affinity/) or other settings require subsequent requests to stay at Pool 2.\n\nThis behavior is known as failback and ensures traffic resumes normal routing when the primary pool recovers.\n\n## Random steering\n\nChoose **Random** to route traffic to a healthy pool at random. Customers can use this option to set up [active - active failover](https://developers.cloudflare.com/load-balancing/load-balancers/common-configurations/#active---active-failover) (also known as round robin), where traffic is split equally between multiple pools.\n\nSimilar to setting Weights to direct the amount of traffic going to each endpoint, customers can also set Weights on pools via the [API's](https://developers.cloudflare.com/api/resources/load_balancers/methods/create/) `random_steering` object to determine the percentage of traffic sent to each pool.\n\n</page>\n\n<page>\n---\ntitle: CMB support by dataset Â· Cloudflare Logs docs\nlastUpdated: 2025-07-28T14:13:32.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/cmb/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/cmb/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Account-scoped datasets Â· Cloudflare Logs docs\nlastUpdated: 2025-07-25T16:42:51.000Z\nchatbotDeprioritize: true\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/index.md\n---\n\n* [Access requests](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/access_requests/)\n* [Audit Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/audit_logs/)\n* [Audit Logs V2](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/audit_logs_v2/)\n* [Browser Isolation User Actions](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/biso_user_actions/)\n* [CASB Findings](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/casb_findings/)\n* [Device posture results](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/device_posture_results/)\n* [DEX Application Tests](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/dex_application_tests/)\n* [DEX Device State Events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/dex_device_state_events/)\n* [DLP Forensic Copies](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/dlp_forensic_copies/)\n* [DNS Firewall Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/dns_firewall_logs/)\n* [Email security Alerts](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/email_security_alerts/)\n* [Gateway DNS](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/gateway_dns/)\n* [Gateway HTTP](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/gateway_http/)\n* [Gateway Network](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/gateway_network/)\n* [IPSec Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/ipsec_logs/)\n* [Magic IDS Detections](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/magic_ids_detections/)\n* [Network Analytics Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/network_analytics_logs/)\n* [Sinkhole HTTP Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/sinkhole_http_logs/)\n* [SSH Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/ssh_logs/)\n* [WARP Config Changes](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/warp_config_changes/)\n* [WARP Toggle Changes](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/warp_toggle_changes/)\n* [Workers Trace Events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/workers_trace_events/)\n* [Zero Trust Network Session Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/zero_trust_network_sessions/)\n\n</page>\n\n<page>\n---\ntitle: Enable Logpush to Amazon S3 Â· Cloudflare Logs docs\ndescription: Cloudflare Logpush supports pushing logs directly to Amazon S3 via\n  the Cloudflare dashboard or via API. Customers that use AWS GovCloud locations\n  should use our S3-compatible endpoint and not the Amazon S3 endpoint.\nlastUpdated: 2025-10-10T13:43:07.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/aws-s3/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/aws-s3/index.md\n---\n\nCloudflare Logpush supports pushing logs directly to Amazon S3 via the Cloudflare dashboard or via API. Customers that use AWS GovCloud locations should use our **S3-compatible endpoint** and not the **Amazon S3 endpoint**.\n\n## Manage via the Cloudflare dashboard\n\n1. In the Cloudflare dashboard, go to the **Logpush** page at the account or or domain (also known as zone) level.\n\n   For account: [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/logs)\n\n   For domain (also known as zone): [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/:zone/analytics/logs)\n\n2. Depending on your choice, you have access to [account-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/) and [zone-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/), respectively.\n\n3. Select **Create a Logpush job**.\n\n1) In **Select a destination**, choose **Amazon S3**.\n\n2) Enter or select the following destination information:\n\n   * **Bucket** - S3 bucket name\n   * **Path** - bucket location within the storage container\n   * **Organize logs into daily subfolders** (recommended)\n   * **Bucket region**\n   * If your policy requires [AWS SSE-S3 AES256 Server Side Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html).\n   * For **Grant Cloudflare access to upload files to your bucket**, make sure your bucket has a [policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-policies-s3.html#iam-policy-ex0) (if you did not add it already):\n     * Copy the JSON policy, then go to your bucket in the Amazon S3 console and paste the policy in **Permissions** > **Bucket Policy** and select **Save**.\n\nWhen you are done entering the destination details, select **Continue**.\n\n1. To prove ownership, Cloudflare will send a file to your designated destination. To find the token, select the **Open** button in the **Overview** tab of the ownership challenge file, then paste it into the Cloudflare dashboard to verify your access to the bucket. Enter the **Ownership Token** and select **Continue**.\n\n2. Select the dataset to push to the storage service.\n\n3. In the next step, you need to configure your logpush job:\n\n   * Enter the **Job name**.\n   * Under **If logs match**, you can select the events to include and/or remove from your logs. Refer to [Filters](https://developers.cloudflare.com/logs/logpush/logpush-job/filters/) for more information. Not all datasets have this option available.\n   * In **Send the following fields**, you can choose to either push all logs to your storage destination or selectively choose which logs you want to push.\n\n4. In **Advanced Options**, you can:\n\n   * Choose the format of timestamp fields in your logs (`RFC3339`(default),`Unix`, or `UnixNano`).\n   * Select a [sampling rate](https://developers.cloudflare.com/logs/logpush/logpush-job/api-configuration/#sampling-rate) for your logs or push a randomly-sampled percentage of logs.\n   * Enable redaction for `CVE-2021-44228`. This option will replace every occurrence of `${` with `x{`.\n\n5. Select **Submit** once you are done configuring your logpush job.\n\n## Create and get access to an S3 bucket\n\nCloudflare uses Amazon Identity and Access Management (IAM) to gain access to your S3 bucket. The Cloudflare IAM user needs `PutObject` permission for the bucket.\n\nLogs are written into that bucket as gzipped objects using the S3 Access Control List (ACL) `Bucket-owner-full-control` permission.\n\nFor illustrative purposes, imagine that you want to store logs in the bucket `burritobot`, in the `logs` directory. The S3 URL would then be `s3://burritobot/logs`.\n\nEnsure **Log Share** permissions are enabled, before attempting to read or configure a Logpush job. For more information refer to the [Roles section](https://developers.cloudflare.com/logs/logpush/permissions/#roles).\n\n\n\nTo enable Logpush to Amazon S3:\n\n1. Create an S3 bucket. Refer to [instructions from Amazon](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html).\n\n   Note\n\n   Buckets in China regions (`cn-north-1`, `cn-northwest-1`) are currently not supported.\n\n2. Edit and paste the policy below into **S3** > **Bucket** > **Permissions** > **Bucket Policy**, replacing the `Resource` value with your own bucket path. The `AWS` `Principal` is owned by Cloudflare and should not be changed.",
      "language": "unknown"
    },
    {
      "code": "Note\n\nLogpush uses multipart upload for S3. Aborted uploads will result in incomplete files remaining in your bucket. To minimize your storage costs, Amazon recommends configuring a lifecycle rule using the `AbortIncompleteMultipartUpload` action. Refer to [Uploading and copying objects using multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html#mpu-abort-incomplete-mpu-lifecycle-config).\n\n</page>\n\n<page>\n---\ntitle: Zone-scoped datasets Â· Cloudflare Logs docs\nlastUpdated: 2025-07-25T16:42:51.000Z\nchatbotDeprioritize: true\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/index.md\n---\n\n* [DNS logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/dns_logs/)\n* [Firewall events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/firewall_events/)\n* [HTTP requests](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/http_requests/)\n* [NEL reports](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/nel_reports/)\n* [Page Shield events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/page_shield_events/)\n* [Spectrum events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/spectrum_events/)\n* [Zaraz Events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/zaraz_events/)\n\n</page>\n\n<page>\n---\ntitle: Enable Logpush to Microsoft Azure Â· Cloudflare Logs docs\ndescription: Cloudflare Logpush supports pushing logs directly to Microsoft\n  Azure via the Cloudflare dashboard or via API.\nlastUpdated: 2025-11-19T11:40:36.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/azure/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/azure/index.md\n---\n\nCloudflare Logpush supports pushing logs directly to Microsoft Azure via the Cloudflare dashboard or via API.\n\nNote\n\nThe [Microsoft Sentinel](https://developers.cloudflare.com/analytics/analytics-integrations/sentinel/) integration for Cloudflare is available in two connector versions.\n\n## Manage via the Cloudflare dashboard\n\n1. In the Cloudflare dashboard, go to the **Logpush** page at the account or or domain (also known as zone) level.\n\n   For account: [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/logs)\n\n   For domain (also known as zone): [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/:zone/analytics/logs)\n\n2. Depending on your choice, you have access to [account-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/) and [zone-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/), respectively.\n\n3. Select **Create a Logpush job**.\n\n1) In **Select a destination**, choose **Microsoft Azure**.\n\n2) Enter or select the following destination details:\n\n   * **SAS URL** - a pre-signed URL that grants access to Azure Storage resources. Refer to [Azure storage documentation](https://learn.microsoft.com/en-us/azure/storage/storage-explorer/vs-azure-tools-storage-manage-with-storage-explorer?tabs=macos#shared-access-signature-sas-url) for more information on generating a SAS URL using Azure Storage Explorer. The service must be set to Blob-only (`ss=b`), and the resource type must be set to Object-only (`srt=o`).\n   * **Path** - bucket location within the storage container\n   * **Organize logs into daily subfolders** (recommended)\n\nWhen you are done entering the destination details, select **Continue**.\n\n1. Select the dataset to push to the storage service.\n\n2. In the next step, you need to configure your logpush job:\n\n   * Enter the **Job name**.\n   * Under **If logs match**, you can select the events to include and/or remove from your logs. Refer to [Filters](https://developers.cloudflare.com/logs/logpush/logpush-job/filters/) for more information. Not all datasets have this option available.\n   * In **Send the following fields**, you can choose to either push all logs to your storage destination or selectively choose which logs you want to push.\n\n3. In **Advanced Options**, you can:\n\n   * Choose the format of timestamp fields in your logs (`RFC3339`(default),`Unix`, or `UnixNano`).\n   * Select a [sampling rate](https://developers.cloudflare.com/logs/logpush/logpush-job/api-configuration/#sampling-rate) for your logs or push a randomly-sampled percentage of logs.\n   * Enable redaction for `CVE-2021-44228`. This option will replace every occurrence of `${` with `x{`.\n\n4. Select **Submit** once you are done configuring your logpush job.\n\n## Create and get access to a Blob Storage container\n\nCloudflare uses a shared access signature (SAS) token to gain access to your Blob Storage container. You will need to provide `Write` permission and an expiration period of at least five years, which will allow you to not worry about the SAS token expiring.\n\nEnsure **Log Share** permissions are enabled, before attempting to read or configure a Logpush job. For more information refer to the [Roles section](https://developers.cloudflare.com/logs/logpush/permissions/#roles).\n\n\n\nTo enable Logpush to Azure:\n\n1. Create a Blob Storage container. Refer to [instructions from Azure](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal).\n\n2. Create a [shared access signature (SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview) to secure and restrict access to your blob storage container. Use [Storage Explorer](https://learn.microsoft.com/en-us/azure/storage/storage-explorer/vs-azure-tools-storage-manage-with-storage-explorer) to navigate to your container and right click to create a signature. Set the signature to expire at least five years from now and only provide write permission.\n\n3. Provide the SAS URL when prompted by the Logpush API or UI.\n\nNote\n\nLogpush will stop pushing logs if your SAS token expires, which is why an expiration period of at least five years is required. The renewal for your SAS token needs to be done via API, updating the `destination_conf` parameter in your Logpush job.\n\n</page>\n\n<page>\n---\ntitle: Enable Logpush to BigQuery Â· Cloudflare Logs docs\ndescription: Configure Logpush to send batches of Cloudflare logs to BigQuery.\nlastUpdated: 2025-07-21T15:07:21.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/bigquery/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/bigquery/index.md\n---\n\nConfigure Logpush to send batches of Cloudflare logs to BigQuery.\n\nBigQuery supports loading up to 1,500 jobs per table per day (including failures) with up to 10 million files in each load. That means you can load into BigQuery once per minute and include up to 10 million files in a load. For more information, refer to BigQuery's quotas for load jobs.\n\nLogpush delivers batches of logs as soon as possible, which means you could receive more than one batch of files per minute. Ensure your BigQuery job is configured to ingest files on a given time interval, like every minute, as opposed to when files are received. Ingesting files into BigQuery as each Logpush file is received could exhaust your BigQuery quota quickly.\n\nFor a community-supported example of how to set up a schedule job load with BigQuery, refer to [Cloudflare + Google Cloud | Integrations repository](https://github.com/cloudflare/cloudflare-gcp/tree/master/logpush-to-bigquery). Note that this repository is provided on a best-effort basis and is not maintained routinely.\n\n</page>\n\n<page>\n---\ntitle: Dedicated Egress IP for Logpush Â· Cloudflare Logs docs\ndescription: This guide covers Dedicated CDN Egress IPs and Logpush\n  configuration and testing instructions to enable log delivery with a fixed,\n  dedicated egress IP.\nlastUpdated: 2025-12-19T01:58:33.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/egress-ip/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/egress-ip/index.md\n---\n\nThis guide covers [Dedicated CDN Egress IPs](https://developers.cloudflare.com/smart-shield/configuration/dedicated-egress-ips/) and Logpush configuration and testing instructions to enable log delivery with a fixed, dedicated egress IP.\n\n## Prerequisites\n\nTo use Logpush with a dedicated egress IP, you will need to have [Smart Shield Advanced](https://developers.cloudflare.com/smart-shield/get-started/#smart-shield-advanced) with Dedicated CDN Egress IPs (formerly known as Aegis). Note that the Dedicated CDN Egress IPs pool is associated with a zone, not with an account. To use Logpush with dedicated IPs, traffic must be routed to a single zone.\n\nThe general approach is to have your Logpush job proxying Logpush data through a Cloudflare zone with Dedicated CDN Egress IPs enabled to send data to your desired destination. This way your destination will only need to allowlist the provisioned dedicated egress IPs of your proxy zone.\n\nAs a prerequisite, you need to create a dedicated zone or use an existing zone. If using an existing zone, be aware that the zone's egress will be restricted to Dedicated CDN Egress IPs. Make sure all services using that zone will not be impacted.\n\nIt is recommended to use a separate, dedicated zone as a proxy to avoid impacting production systems. If you choose to create a new zone, follow the [steps](https://developers.cloudflare.com/registrar/get-started/register-domain/) to register a new domain with Cloudflare.\n\nThe following example shows how to set up logpush and Dedicated CDN Egress IPs to proxy an HTTPS destination, but the proxying should work for any supported Logpush destination as all destinations use the HTTP protocol underneath.\n\n## 1. Provision dedicated egress IP Pool\n\n1. Work with your Cloudflare account team to purchase [Dedicated CDN Egress IPs](https://developers.cloudflare.com/smart-shield/configuration/dedicated-egress-ips/) for your zone.\n\n2. (Optional but recommended) Request two IPs â€” one in PDX-B and one in SJC-A â€” to ensure coverage across regions.\n\n3. Confirm Pool ID once provisioned.\n\n## 2. Configure a zone\n\n1. Register or use an existing zone for the dedicated egress IPs pool.\n2. Contact your account team to get the ID for your dedicated egress IPs pool.\n3. Make a `PATCH` request to the [Edit Zone Setting](https://developers.cloudflare.com/api/resources/zones/subresources/settings/methods/edit/) endpoint:\n\n* Specify `aegis` as the setting ID in the URL.\n* In the request body, set `enabled` to `true` and use the ID from the previous step as `pool_id`.\n\nRequired API token permissions\n\nAt least one of the following [token permissions](https://developers.cloudflare.com/fundamentals/api/reference/permissions/) is required:\n\n* `Zone Settings Write`",
      "language": "unknown"
    },
    {
      "code": "## 3. Proxy zone setup\n\n1. In your zone, add a DNS record (CNAME or A/AAAA) with **Target** as HTTP destination endpoint.\n\n![Create a DNS record in the Cloudflare dashboard to define the HTTP destination endpoint](https://developers.cloudflare.com/_astro/endpoint.DmFFJC-j_14N2lf.webp)\n\n1. If needed, configure [origin rules](https://developers.cloudflare.com/rules/origin-rules/) to specify a custom port. This is useful if your destination only accepts traffic on a non standard port, for example `12345`. You can configure `logpush.yourdestinationendpoint.com` (without specifying a port, as Cloudflare by default only proxies traffic on HTTP/HTTPS ports) to proxy to `yourdestinationendpoint.com:12345`.\n\n## 4. Configure Logpush\n\n1. Create a Logpush job with the following details:\n\n* Destination: HTTP\n* Endpoint: Use the domain/path set up (the Cloudflare dashboard will auto-validate the destination). Use the server name specified in the **Name** section in the DNS record. In this case, `logpush.yourdestionationendpoint.com`.\n\n![Enter destination details when creating a Logpush job in the Cloudflare dashboard](https://developers.cloudflare.com/_astro/destination-details.imLwZlEZ_1Y0Gk.webp)\n\n* Configuration: Select dataset, job name, filters, and fields. Refer to the [Logpush documentation](https://developers.cloudflare.com/logs/logpush/) for more details.\n\n1. Check destination to confirm if the logs are received.\n\n## 5. Secure your proxy zone endpoint\n\nThe proxy zone hostname is publicly resolvable, but traffic passes through Cloudflare's edge where you can apply security controls. Use the following best practices to protect your endpoint.\n\n### Add a secret header with WAF validation\n\nAdd a secret token as an HTTP header in your Logpush job, then create a WAF rule to block requests without it. This is the recommended approach for most deployments.\n\n**Configure Logpush with a secret header**\n\nAny URL parameter starting with `header_` becomes an HTTP header in the request. When creating or updating your Logpush job, add the secret header to your destination URL:",
      "language": "unknown"
    },
    {
      "code": "Generate a strong random token using `openssl rand -hex 32`.\n\n**Create a WAF custom rule**\n\nIn the proxy zone, go to **Security** > **WAF** > **Custom rules** and create a rule to block requests without the correct secret header.\n\n* **Expression:**",
      "language": "unknown"
    },
    {
      "code": "* **Action:** Block\n\n### Add ASN-based filtering\n\nFor defense in depth, add a rule to only allow traffic from Cloudflare's ASN. Logpush traffic originates from Cloudflare's network (ASN 13335 or 132892).\n\n* **Expression:**",
      "language": "unknown"
    },
    {
      "code": "* **Action:** Block\n\nNote\n\nASN filtering alone is insufficient because other Cloudflare customers' traffic also originates from these ASNs. Always combine with secret header validation.\n\n### Use Access Service Tokens for high-security environments\n\nFor stronger authentication, use [Cloudflare Access Service Tokens](https://developers.cloudflare.com/cloudflare-one/access-controls/service-credentials/service-tokens/) for machine-to-machine authentication. Create a Service Token in the Zero Trust dashboard, then configure Logpush with the Access headers:",
      "language": "unknown"
    },
    {
      "code": "### Verify your security configuration\n\nTest that your WAF rules are blocking unauthorized requests:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "2. Configure Drizzle",
      "id": "2.-configure-drizzle"
    },
    {
      "level": "h3",
      "text": "2.1. Define a schema",
      "id": "2.1.-define-a-schema"
    },
    {
      "level": "h3",
      "text": "2.2. Connect Drizzle ORM to the database with Hyperdrive",
      "id": "2.2.-connect-drizzle-orm-to-the-database-with-hyperdrive"
    },
    {
      "level": "h3",
      "text": "2.3. Configure Drizzle-Kit for migrations (optional)",
      "id": "2.3.-configure-drizzle-kit-for-migrations-(optional)"
    },
    {
      "level": "h2",
      "text": "3. Deploy your Worker",
      "id": "3.-deploy-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Install Prisma ORM",
      "id": "1.-install-prisma-orm"
    },
    {
      "level": "h2",
      "text": "2. Configure Prisma ORM",
      "id": "2.-configure-prisma-orm"
    },
    {
      "level": "h3",
      "text": "2.1. Initialize Prisma",
      "id": "2.1.-initialize-prisma"
    },
    {
      "level": "h3",
      "text": "2.2. Define a schema",
      "id": "2.2.-define-a-schema"
    },
    {
      "level": "h3",
      "text": "2.3. Set up environment variables",
      "id": "2.3.-set-up-environment-variables"
    },
    {
      "level": "h3",
      "text": "2.4. Generate Prisma Client",
      "id": "2.4.-generate-prisma-client"
    },
    {
      "level": "h3",
      "text": "2.5. Run migrations",
      "id": "2.5.-run-migrations"
    },
    {
      "level": "h2",
      "text": "3. Connect Prisma ORM to Hyperdrive",
      "id": "3.-connect-prisma-orm-to-hyperdrive"
    },
    {
      "level": "h2",
      "text": "4. Deploy your Worker",
      "id": "4.-deploy-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "Limitation when using Workers",
      "id": "limitation-when-using-workers"
    },
    {
      "level": "h3",
      "text": "Workaround",
      "id": "workaround"
    },
    {
      "level": "h2",
      "text": "Configure via the API",
      "id": "configure-via-the-api"
    },
    {
      "level": "h2",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h2",
      "text": "Pool assignment",
      "id": "pool-assignment"
    },
    {
      "level": "h3",
      "text": "Region steering",
      "id": "region-steering"
    },
    {
      "level": "h3",
      "text": "Country steering",
      "id": "country-steering"
    },
    {
      "level": "h3",
      "text": "PoP steering",
      "id": "pop-steering"
    },
    {
      "level": "h3",
      "text": "Failover behavior",
      "id": "failover-behavior"
    },
    {
      "level": "h2",
      "text": "Configure via the API",
      "id": "configure-via-the-api"
    },
    {
      "level": "h2",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h2",
      "text": "When to add proximity steering",
      "id": "when-to-add-proximity-steering"
    },
    {
      "level": "h2",
      "text": "How to add proximity steering",
      "id": "how-to-add-proximity-steering"
    },
    {
      "level": "h2",
      "text": "Off - Failover",
      "id": "off---failover"
    },
    {
      "level": "h3",
      "text": "Failback behavior",
      "id": "failback-behavior"
    },
    {
      "level": "h2",
      "text": "Random steering",
      "id": "random-steering"
    },
    {
      "level": "h2",
      "text": "Manage via the Cloudflare dashboard",
      "id": "manage-via-the-cloudflare-dashboard"
    },
    {
      "level": "h2",
      "text": "Create and get access to an S3 bucket",
      "id": "create-and-get-access-to-an-s3-bucket"
    },
    {
      "level": "h2",
      "text": "Manage via the Cloudflare dashboard",
      "id": "manage-via-the-cloudflare-dashboard"
    },
    {
      "level": "h2",
      "text": "Create and get access to a Blob Storage container",
      "id": "create-and-get-access-to-a-blob-storage-container"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Provision dedicated egress IP Pool",
      "id": "1.-provision-dedicated-egress-ip-pool"
    },
    {
      "level": "h2",
      "text": "2. Configure a zone",
      "id": "2.-configure-a-zone"
    },
    {
      "level": "h2",
      "text": "3. Proxy zone setup",
      "id": "3.-proxy-zone-setup"
    },
    {
      "level": "h2",
      "text": "4. Configure Logpush",
      "id": "4.-configure-logpush"
    },
    {
      "level": "h2",
      "text": "5. Secure your proxy zone endpoint",
      "id": "5.-secure-your-proxy-zone-endpoint"
    },
    {
      "level": "h3",
      "text": "Add a secret header with WAF validation",
      "id": "add-a-secret-header-with-waf-validation"
    },
    {
      "level": "h3",
      "text": "Add ASN-based filtering",
      "id": "add-asn-based-filtering"
    },
    {
      "level": "h3",
      "text": "Use Access Service Tokens for high-security environments",
      "id": "use-access-service-tokens-for-high-security-environments"
    },
    {
      "level": "h3",
      "text": "Verify your security configuration",
      "id": "verify-your-security-configuration"
    }
  ],
  "url": "llms-txt#postgres-3.4.5-or-later-is-recommended",
  "links": []
}