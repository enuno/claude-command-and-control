{
  "title": "zephyr-7b-beta-awq Beta",
  "content": "Text Generation • thebloke\n\n@hf/thebloke/zephyr-7b-beta-awq\n\nZephyr 7B Beta AWQ is an efficient, accurate and blazing-fast low-bit weight quantized Zephyr model variant.\n\n| Model Info | |\n| - | - |\n| Deprecated | 10/1/2025 |\n| Context Window[](https://developers.cloudflare.com/workers-ai/glossary/) | 4,096 tokens |\n| More information | [link](https://huggingface.co/TheBloke/zephyr-7B-beta-AWQ) |\n| Beta | Yes |\n\nTry out this model with Workers AI LLM Playground. It does not require any setup or authentication and an instant way to preview and test a model directly in the browser.\n\n[Launch the LLM Playground](https://playground.ai.cloudflare.com/?model=@hf/thebloke/zephyr-7b-beta-awq)\n\nOpenAI compatible endpoints\n\nWorkers AI also supports OpenAI compatible API endpoints for `/v1/chat/completions` and `/v1/embeddings`. For more details, refer to [Configurations ](https://developers.cloudflare.com/workers-ai/configuration/open-ai-compatibility/).\n\n\\* indicates a required field\n\n* `prompt` string required min 1\n\nThe input text prompt for the model to generate a response.\n\nName of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.\n\n* `response_format` object\n\nIf true, a chat template is not applied and you must adhere to the specific model's expected formatting.\n\nIf true, the response will be streamed back incrementally using SSE, Server Sent Events.\n\n* `max_tokens` integer default 256\n\nThe maximum number of tokens to generate in the response.\n\n* `temperature` number default 0.6 min 0 max 5\n\nControls the randomness of the output; higher values produce more random results.\n\n* `top_p` number min 0.001 max 1\n\nAdjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.\n\n* `top_k` integer min 1 max 50\n\nLimits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.\n\n* `seed` integer min 1 max 9999999999\n\nRandom seed for reproducibility of the generation.\n\n* `repetition_penalty` number min 0 max 2\n\nPenalty for repeated tokens; higher values discourage repetition.\n\n* `frequency_penalty` number min -2 max 2\n\nDecreases the likelihood of the model repeating the same lines verbatim.\n\n* `presence_penalty` number min -2 max 2\n\nIncreases the likelihood of the model introducing new topics.\n\n* `messages` array required\n\nAn array of message objects representing the conversation history.\n\n* `role` string required\n\nThe role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').\n\n* `content` string required\n\nThe content of the message as a string.\n\n* `name` string required\n\n* `code` string required\n\nA list of tools available for the assistant to use.\n\n* `name` string required\n\nThe name of the tool. More descriptive the better.\n\n* `description` string required\n\nA brief description of what the tool does.\n\n* `parameters` object required\n\nSchema defining the parameters accepted by the tool.\n\n* `type` string required\n\nThe type of the parameters object (usually 'object').\n\nList of required parameter names.\n\n* `properties` object required\n\nDefinitions of each parameter.\n\n* `additionalProperties` object\n\n* `type` string required\n\nThe data type of the parameter.\n\n* `description` string required\n\nA description of the expected parameter.\n\n* `type` string required\n\nSpecifies the type of tool (e.g., 'function').\n\n* `function` object required\n\nDetails of the function tool.\n\n* `name` string required\n\nThe name of the function.\n\n* `description` string required\n\nA brief description of what the function does.\n\n* `parameters` object required\n\nSchema defining the parameters accepted by the function.\n\n* `type` string required\n\nThe type of the parameters object (usually 'object').\n\nList of required parameter names.\n\n* `properties` object required\n\nDefinitions of each parameter.\n\n* `additionalProperties` object\n\n* `type` string required\n\nThe data type of the parameter.\n\n* `description` string required\n\nA description of the expected parameter.\n\n* `response_format` object\n\nIf true, a chat template is not applied and you must adhere to the specific model's expected formatting.\n\nIf true, the response will be streamed back incrementally using SSE, Server Sent Events.\n\n* `max_tokens` integer default 256\n\nThe maximum number of tokens to generate in the response.\n\n* `temperature` number default 0.6 min 0 max 5\n\nControls the randomness of the output; higher values produce more random results.\n\n* `top_p` number min 0.001 max 1\n\nAdjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.\n\n* `top_k` integer min 1 max 50\n\nLimits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.\n\n* `seed` integer min 1 max 9999999999\n\nRandom seed for reproducibility of the generation.\n\n* `repetition_penalty` number min 0 max 2\n\nPenalty for repeated tokens; higher values discourage repetition.\n\n* `frequency_penalty` number min -2 max 2\n\nDecreases the likelihood of the model repeating the same lines verbatim.\n\n* `presence_penalty` number min -2 max 2\n\nIncreases the likelihood of the model introducing new topics.\n\n* `response` string required\n\nThe generated text response from the model\n\nUsage statistics for the inference request\n\n* `prompt_tokens` number 0\n\nTotal number of tokens in input\n\n* `completion_tokens` number 0\n\nTotal number of tokens in output\n\n* `total_tokens` number 0\n\nTotal number of input and output tokens\n\nAn array of tool calls requests made during the response generation\n\nThe arguments passed to be passed to the tool call request\n\nThe name of the tool to be called\n\nThe following schemas are based on JSON Schema\n\n<page>\n---\ntitle: AI Gateway · Cloudflare Workers AI docs\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/platform/ai-gateway/\n  md: https://developers.cloudflare.com/workers-ai/platform/ai-gateway/index.md\n---\n\n<page>\n---\ntitle: Your Data and Workers AI · Cloudflare Workers AI docs\ndescription: Cloudflare processes certain customer data in order to provide the\n  Workers AI service, subject to our Privacy Policy and Self-Serve Subscription\n  Agreement or Enterprise Subscription Agreement (as applicable).\nlastUpdated: 2025-04-28T18:43:22.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/platform/data-usage/\n  md: https://developers.cloudflare.com/workers-ai/platform/data-usage/index.md\n---\n\nCloudflare processes certain customer data in order to provide the Workers AI service, subject to our [Privacy Policy](https://www.cloudflare.com/privacypolicy/) and [Self-Serve Subscription Agreement](https://www.cloudflare.com/terms/) or [Enterprise Subscription Agreement](https://www.cloudflare.com/enterpriseterms/) (as applicable).\n\nCloudflare neither creates nor trains the AI models made available on Workers AI. The models constitute Third-Party Services and may be subject to open source or other license terms that apply between you and the model provider. Be sure to review the license terms applicable to each model (if any).\n\nYour inputs (e.g., text prompts, image submissions, audio files, etc.), outputs (e.g., generated text/images, translations, etc.), embeddings, and training data constitute Customer Content.\n\n* You own, and are responsible for, all of your Customer Content.\n* Cloudflare does not make your Customer Content available to any other Cloudflare customer.\n* Cloudflare does not use your Customer Content to (1) train any AI models made available on Workers AI or (2) improve any Cloudflare or third-party services, and would not do so unless we received your explicit consent.\n* Your Customer Content for Workers AI may be stored by Cloudflare if you specifically use a storage service (e.g., R2, KV, DO, Vectorize, etc.) in conjunction with Workers AI.\n\n<page>\n---\ntitle: Errors · Cloudflare Workers AI docs\ndescription: Below is a list of Workers AI errors.\nlastUpdated: 2025-04-03T16:21:18.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/platform/errors/\n  md: https://developers.cloudflare.com/workers-ai/platform/errors/index.md\n---\n\nBelow is a list of Workers AI errors.\n\n| **Name** | **Internal Code** | **HTTP Code** | **Description** |\n| - | - | - | - |\n| No such model | `5007` | `400` | No such model `${model}` or task |\n| Invalid data | `5004` | `400` | Invalid data type for base64 input: `${type}` |\n| Finetune missing required files | `3039` | `400` | Finetune is missing required files `(model.safetensors and config.json)` |\n| Incomplete request | `3003` | `400` | Request is missing headers or body: `{what}` |\n| Account not allowed for private model | `5018` | `403` | The account is not allowed to access this model |\n| Model agreement | `5016` | `403` | User has not agreed to Llama3.2 model terms |\n| Account blocked | `3023` | `403` | Service unavailable for account |\n| Account not allowed for private model | `3041` | `403` | The account is not allowed to access this model |\n| Deprecated SDK version | `5019` | `405` | Request trying to use deprecated SDK version |\n| LoRa unsupported | `5005` | `405` | The model `${this.model}` does not support LoRa inference |\n| Invalid model ID | `3042` | `404` | The model name is invalid |\n| Request too large | `3006` | `413` | Request is too large |\n| Timeout | `3007` | `408` | Request timeout |\n| Aborted | `3008` | `408` | Request was aborted |\n| Account limited | `3036` | `429` | You have used up your daily free allocation of 10,000 neurons. Please upgrade to Cloudflare's Workers Paid plan if you would like to continue usage. |\n| Out of capacity | `3040` | `429` | No more data centers to forward the request to |\n\n<page>\n---\ntitle: Event subscriptions · Cloudflare Workers AI docs\ndescription: Event subscriptions allow you to receive messages when events occur\n  across your Cloudflare account. Cloudflare products (e.g., KV, Workers AI,\n  Workers) can publish structured events to a queue, which you can then consume\n  with Workers or HTTP pull consumers to build custom workflows, integrations,\n  or logic.\nlastUpdated: 2025-11-06T01:33:23.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/platform/event-subscriptions/\n  md: https://developers.cloudflare.com/workers-ai/platform/event-subscriptions/index.md\n---\n\n[Event subscriptions](https://developers.cloudflare.com/queues/event-subscriptions/) allow you to receive messages when events occur across your Cloudflare account. Cloudflare products (e.g., [KV](https://developers.cloudflare.com/kv/), [Workers AI](https://developers.cloudflare.com/workers-ai/), [Workers](https://developers.cloudflare.com/workers/)) can publish structured events to a [queue](https://developers.cloudflare.com/queues/), which you can then consume with Workers or [HTTP pull consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/) to build custom workflows, integrations, or logic.\n\nFor more information on [Event Subscriptions](https://developers.cloudflare.com/queues/event-subscriptions/), refer to the [management guide](https://developers.cloudflare.com/queues/event-subscriptions/manage-event-subscriptions/).\n\n## Available Workers AI events\n\nTriggered when a batch request is queued.\n\n#### `batch.succeeded`\n\nTriggered when a batch request has completed.\n\nTriggered when a batch request has failed.\n\n<page>\n---\ntitle: Glossary · Cloudflare Workers AI docs\ndescription: Review the definitions for terms used across Cloudflare's Workers\n  AI documentation.\nlastUpdated: 2025-04-03T16:21:18.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/platform/glossary/\n  md: https://developers.cloudflare.com/workers-ai/platform/glossary/index.md\n---\n\nReview the definitions for terms used across Cloudflare's Workers AI documentation.\n\n| Term | Definition |\n| - | - |\n| AI models | [An AI model](https://developers.cloudflare.com/workers-ai/models) is a trained system that processes input data to generate predictions, decisions, or outputs based on patterns it has learned. |\n| API Tokens | [API Tokens](https://developers.cloudflare.com/workers-ai/get-started/rest-api/) are authentication credentials used to securely access and manage Workers AI resources via the REST API. |\n| Cloudflare Dashboard | [Cloudflare Dashboard](https://developers.cloudflare.com/workers-ai/get-started/dashboard/) is a web-based interface that allows users to manage Workers AI services, including model deployment and monitoring. |\n| Context Window | In generative AI, the context window is the sum of the number of input, reasoning, and completion or response tokens a model supports. You can find the context window limit on each [model page](https://developers.cloudflare.com/workers-ai/models/). |\n| D1 | [D1](https://developers.cloudflare.com/d1/) is Cloudflare's managed, serverless database with SQLite's SQL semantics, built-in disaster recovery, and Worker and HTTP API access. |\n| Environment Variables | [Environment Variables](https://developers.cloudflare.com/workers-ai/configuration/bindings/) are dynamic values that can be used within Workers to manage configuration settings, including those related to AI integrations. |\n| Fine-Tuning | [Fine-Tuning](https://developers.cloudflare.com/workers-ai/fine-tunes/) is a general term for modifying an AI model by continuing to train it with additional data. |\n| Function Calling | [Function Calling](https://developers.cloudflare.com/workers-ai/function-calling/) enables people to take Large Language Models (LLMs) and use the model response to execute functions or interact with external APIs. |\n| Inference | [Inference](https://developers.cloudflare.com/workers-ai/fine-tunes/public-loras/#running-inference-with-public-loras) refers to the process of using a trained machine learning model to make predictions or generate outputs based on new data. |\n| LoRA Adapters | [LoRA Adapters](https://developers.cloudflare.com/workers-ai/fine-tunes/loras/) (Low-Rank Adaptation adapters) are used in machine learning to fine-tune models efficiently by adjusting a small number of parameters, allowing for customization of AI models in Workers AI.[Public LoRA Adapters](https://developers.cloudflare.com/workers-ai/fine-tunes/public-loras/) are pre-trained Low-Rank Adaptation adapters available for public use. |\n| Maximum Tokens | In generative AI, the user-defined property `max_tokens` defines the maximum number of tokens at which the model should stop responding. This limit cannot exceed the context window. |\n| Model Catalog | [Model Catalog](https://developers.cloudflare.com/workers-ai/models/) is a curated collection of AI models available within Workers AI, providing developers with a variety of pre-trained models for different tasks. |\n| Prompt Engineering | [Prompt Engineering](https://developers.cloudflare.com/workers-ai/guides/prompting/) is the practice of designing and refining input prompts to effectively elicit desired responses from AI models. |\n| Prompt Templates | [Prompt Templates](https://developers.cloudflare.com/workers-ai/guides/prompting/) are predefined structures that guide the input provided to AI models, enhancing consistency and effectiveness in responses. |\n| REST API | [REST API](https://developers.cloudflare.com/workers-ai/get-started/rest-api/) is an application programming interface that allows developers to interact with Workers AI services over HTTP, enabling model management and inference requests. |\n| Serverless GPUs | [Serverless GPUs](https://developers.cloudflare.com/workers-ai/) are graphics processing units provided by Cloudflare in a serverless environment, enabling scalable and efficient execution of machine learning models without the need for managing underlying hardware. |\n| Worker Bindings | [Worker Bindings](https://developers.cloudflare.com/workers-ai/configuration/bindings/) are configurations that connect Workers scripts to external resources, such as AI models, enabling seamless integration and functionality. |\n| Workers AI | [Workers AI](https://developers.cloudflare.com/workers-ai/) is a Cloudflare service that enables running machine learning models on Cloudflare's global network, utilizing serverless GPUs. It allows developers to integrate AI capabilities into their applications using Workers, Pages, or via the REST API. |\n| Workers KV | [Workers KV](https://developers.cloudflare.com/kv/)is a data storage that allows you to store and retrieve data globally. |\n| Wrangler CLI | [Wrangler CLI](https://developers.cloudflare.com/workers-ai/get-started/workers-wrangler/) is a command-line tool for building and deploying Cloudflare Workers, facilitating the integration of AI models into applications. |\n\n<page>\n---\ntitle: Limits · Cloudflare Workers AI docs\ndescription: Workers AI is now Generally Available. We've updated our rate\n  limits to reflect this.\nlastUpdated: 2025-08-20T18:47:44.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/platform/limits/\n  md: https://developers.cloudflare.com/workers-ai/platform/limits/index.md\n---\n\nWorkers AI is now Generally Available. We've updated our rate limits to reflect this.\n\nNote that model inferences in local mode using Wrangler will also count towards these limits. Beta models may have lower rate limits while we work on performance and scale.\n\nIf you have custom requirements like private custom models or higher limits, complete the [Custom Requirements Form](https://forms.gle/axnnpGDb6xrmR31T6). Cloudflare will contact you with next steps.\n\nRate limits are default per task type, with some per-model limits defined as follows:\n\n## Rate limits by task type\n\n### [Automatic Speech Recognition](https://developers.cloudflare.com/workers-ai/models/)\n\n* 720 requests per minute\n\n### [Image Classification](https://developers.cloudflare.com/workers-ai/models/)\n\n* 3000 requests per minute\n\n### [Image-to-Text](https://developers.cloudflare.com/workers-ai/models/)\n\n* 720 requests per minute\n\n### [Object Detection](https://developers.cloudflare.com/workers-ai/models/)\n\n* 3000 requests per minute\n\n### [Summarization](https://developers.cloudflare.com/workers-ai/models/)\n\n* 1500 requests per minute\n\n### [Text Classification](https://developers.cloudflare.com/workers-ai/models/)\n\n* 2000 requests per minute\n\n### [Text Embeddings](https://developers.cloudflare.com/workers-ai/models/)\n\n* 3000 requests per minute\n* [@cf/baai/bge-large-en-v1.5](https://developers.cloudflare.com/workers-ai/models/bge-large-en-v1.5/) is 1500 requests per minute\n\n### [Text Generation](https://developers.cloudflare.com/workers-ai/models/)\n\n* 300 requests per minute\n* [@hf/thebloke/mistral-7b-instruct-v0.1-awq](https://developers.cloudflare.com/workers-ai/models/mistral-7b-instruct-v0.1-awq/) is 400 requests per minute\n* [@cf/microsoft/phi-2](https://developers.cloudflare.com/workers-ai/models/phi-2/) is 720 requests per minute\n* [@cf/qwen/qwen1.5-0.5b-chat](https://developers.cloudflare.com/workers-ai/models/qwen1.5-0.5b-chat/) is 1500 requests per minute\n* [@cf/qwen/qwen1.5-1.8b-chat](https://developers.cloudflare.com/workers-ai/models/qwen1.5-1.8b-chat/) is 720 requests per minute\n* [@cf/qwen/qwen1.5-14b-chat-awq](https://developers.cloudflare.com/workers-ai/models/qwen1.5-14b-chat-awq/) is 150 requests per minute\n* [@cf/tinyllama/tinyllama-1.1b-chat-v1.0](https://developers.cloudflare.com/workers-ai/models/tinyllama-1.1b-chat-v1.0/) is 720 requests per minute\n\n### [Text-to-Image](https://developers.cloudflare.com/workers-ai/models/)\n\n* 720 requests per minute\n* [@cf/runwayml/stable-diffusion-v1-5-img2img](https://developers.cloudflare.com/workers-ai/models/stable-diffusion-v1-5-img2img/) is 1500 requests per minute\n\n### [Translation](https://developers.cloudflare.com/workers-ai/models/)\n\n* 720 requests per minute\n\n<page>\n---\ntitle: Pricing · Cloudflare Workers AI docs\ndescription: Workers AI is included in both the Free and Paid Workers plans and\n  is priced at $0.011 per 1,000 Neurons.\nlastUpdated: 2025-12-03T18:03:48.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/platform/pricing/\n  md: https://developers.cloudflare.com/workers-ai/platform/pricing/index.md\n---\n\nWorkers AI has updated pricing to be more granular, with per-model unit-based pricing presented, but still billing in neurons in the back end.\n\nWorkers AI is included in both the [Free and Paid Workers plans](https://developers.cloudflare.com/workers/platform/pricing/) and is priced at **$0.011 per 1,000 Neurons**.\n\nOur free allocation allows anyone to use a total of **10,000 Neurons per day at no charge**. To use more than 10,000 Neurons per day, you need to sign up for the [Workers Paid plan](https://developers.cloudflare.com/workers/platform/pricing/#workers). On Workers Paid, you will be charged at $0.011 / 1,000 Neurons for any usage above the free allocation of 10,000 Neurons per day.\n\nYou can monitor your Neuron usage in the [Cloudflare Workers AI dashboard](https://dash.cloudflare.com/?to=/:account/ai/workers-ai).\n\nAll limits reset daily at 00:00 UTC. If you exceed any one of the above limits, further operations will fail with an error.\n\n| | Free allocation | Pricing |\n| - | - | - |\n| Workers Free | 10,000 Neurons per day | N/A - Upgrade to Workers Paid |\n| Workers Paid | 10,000 Neurons per day | $0.011 / 1,000 Neurons |\n\nNeurons are our way of measuring AI outputs across different models, representing the GPU compute needed to perform your request. Our serverless model allows you to pay only for what you use without having to worry about renting, managing, or scaling GPUs.\n\nThe Price in Tokens column is equivalent to the Price in Neurons column - the different units are displayed so you can easily compare and understand pricing.\n\n| Model | Price in Tokens | Price in Neurons |\n| - | - | - |\n| @cf/meta/llama-3.2-1b-instruct | $0.027 per M input tokens $0.201 per M output tokens | 2457 neurons per M input tokens 18252 neurons per M output tokens |\n| @cf/meta/llama-3.2-3b-instruct | $0.051 per M input tokens $0.335 per M output tokens | 4625 neurons per M input tokens 30475 neurons per M output tokens |\n| @cf/meta/llama-3.1-8b-instruct-fp8-fast | $0.045 per M input tokens $0.384 per M output tokens | 4119 neurons per M input tokens 34868 neurons per M output tokens |\n| @cf/meta/llama-3.2-11b-vision-instruct | $0.049 per M input tokens $0.676 per M output tokens | 4410 neurons per M input tokens 61493 neurons per M output tokens |\n| @cf/meta/llama-3.1-70b-instruct-fp8-fast | $0.293 per M input tokens $2.253 per M output tokens | 26668 neurons per M input tokens 204805 neurons per M output tokens |\n| @cf/meta/llama-3.3-70b-instruct-fp8-fast | $0.293 per M input tokens $2.253 per M output tokens | 26668 neurons per M input tokens 204805 neurons per M output tokens |\n| @cf/deepseek-ai/deepseek-r1-distill-qwen-32b | $0.497 per M input tokens $4.881 per M output tokens | 45170 neurons per M input tokens 443756 neurons per M output tokens |\n| @cf/mistral/mistral-7b-instruct-v0.1 | $0.110 per M input tokens $0.190 per M output tokens | 10000 neurons per M input tokens 17300 neurons per M output tokens |\n| @cf/mistralai/mistral-small-3.1-24b-instruct | $0.351 per M input tokens $0.555 per M output tokens | 31876 neurons per M input tokens 50488 neurons per M output tokens |\n| @cf/meta/llama-3.1-8b-instruct | $0.282 per M input tokens $0.827 per M output tokens | 25608 neurons per M input tokens 75147 neurons per M output tokens |\n| @cf/meta/llama-3.1-8b-instruct-fp8 | $0.152 per M input tokens $0.287 per M output tokens | 13778 neurons per M input tokens 26128 neurons per M output tokens |\n| @cf/meta/llama-3.1-8b-instruct-awq | $0.123 per M input tokens $0.266 per M output tokens | 11161 neurons per M input tokens 24215 neurons per M output tokens |\n| @cf/meta/llama-3-8b-instruct | $0.282 per M input tokens $0.827 per M output tokens | 25608 neurons per M input tokens 75147 neurons per M output tokens |\n| @cf/meta/llama-3-8b-instruct-awq | $0.123 per M input tokens $0.266 per M output tokens | 11161 neurons per M input tokens 24215 neurons per M output tokens |\n| @cf/meta/llama-2-7b-chat-fp16 | $0.556 per M input tokens $6.667 per M output tokens | 50505 neurons per M input tokens 606061 neurons per M output tokens |\n| @cf/meta/llama-guard-3-8b | $0.484 per M input tokens $0.030 per M output tokens | 44003 neurons per M input tokens 2730 neurons per M output tokens |\n| @cf/meta/llama-4-scout-17b-16e-instruct | $0.270 per M input tokens $0.850 per M output tokens | 24545 neurons per M input tokens 77273 neurons per M output tokens |\n| @cf/google/gemma-3-12b-it | $0.345 per M input tokens $0.556 per M output tokens | 31371 neurons per M input tokens 50560 neurons per M output tokens |\n| @cf/qwen/qwq-32b | $0.660 per M input tokens $1.000 per M output tokens | 60000 neurons per M input tokens 90909 neurons per M output tokens |\n| @cf/qwen/qwen2.5-coder-32b-instruct | $0.660 per M input tokens $1.000 per M output tokens | 60000 neurons per M input tokens 90909 neurons per M output tokens |\n| @cf/qwen/qwen3-30b-a3b-fp8 | $0.051 per M input tokens $0.335 per M output tokens | 4625 neurons per M input tokens 30475 neurons per M output tokens |\n| @cf/openai/gpt-oss-120b | $0.350 per M input tokens $0.750 per M output tokens | 31818 neurons per M input tokens 68182 neurons per M output tokens |\n| @cf/openai/gpt-oss-20b | $0.200 per M input tokens $0.300 per M output tokens | 18182 neurons per M input tokens 27273 neurons per M output tokens |\n| @cf/aisingapore/gemma-sea-lion-v4-27b-it | $0.351 per M input tokens $0.555 per M output tokens | 31876 neurons per M input tokens 50488 neurons per M output tokens |\n| @cf/ibm-granite/granite-4.0-h-micro | $0.017 per M input tokens $0.112 per M output tokens | 1542 neurons per M input tokens 10158 neurons per M output tokens |\n\n## Embeddings model pricing\n\n| Model | Price in Tokens | Price in Neurons |\n| - | - | - |\n| @cf/baai/bge-small-en-v1.5 | $0.020 per M input tokens | 1841 neurons per M input tokens |\n| @cf/baai/bge-base-en-v1.5 | $0.067 per M input tokens | 6058 neurons per M input tokens |\n| @cf/baai/bge-large-en-v1.5 | $0.204 per M input tokens | 18582 neurons per M input tokens |\n| @cf/baai/bge-m3 | $0.012 per M input tokens | 1075 neurons per M input tokens |\n| @cf/pfnet/plamo-embedding-1b | $0.019 per M input tokens | 1689 neurons per M input tokens |\n| @cf/qwen/qwen3-embedding-0.6b | $0.012 per M input tokens | 1075 neurons per M input tokens |\n\n## Image model pricing\n\n| Model | Price in Tokens | Price in Neurons |\n| - | - | - |\n| @cf/black-forest-labs/flux-1-schnell | $0.0000528 per 512x512 tile $0.0001056 per step | 4.80 neurons per 512x512 tile 9.60 neurons per step |\n| @cf/leonardo/lucid-origin | $0.006996 per 512x512 tile $0.000132 per step | 636.00 neurons per 512x512 tile 12.00 neurons per step |\n| @cf/leonardo/phoenix-1.0 | $0.005830 per 512x512 tile $0.000110 per step | 530.00 neurons per 512x512 tile 10.00 neurons per step |\n| @cf/black-forest-labs/flux-2-dev | $0.00021 per input 512x512 tile, per step $0.00041 per output 512x512 tile, per step | 18.75 neurons per input 512x512 tile, per step 37.50 neurons per output 512x512 tile, per step |\n\n## Audio model pricing\n\n| Model | Price in Tokens | Price in Neurons |\n| - | - | - |\n| @cf/openai/whisper | $0.0005 per audio minute | 41.14 neurons per audio minute |\n| @cf/openai/whisper-large-v3-turbo | $0.0005 per audio minute | 46.63 neurons per audio minute |\n| @cf/myshell-ai/melotts | $0.0002 per audio minute | 18.63 neurons per audio minute |\n| @cf/deepgram/aura-1 | $0.015 per 1k characters input  | 1,363.64 neurons per 1k characters input  |\n| @cf/deepgram/nova-3 | $0.0052 per audio minute input  | 472.73 neurons per audio minute input  |\n| @cf/deepgram/nova-3 (WebSocket) | $0.0092 per audio minute input  | 836.36 neurons per audio minute input  |\n| @cf/pipecat-ai/smart-turn-v2 | $0.00033795 per audio minute input  | 0.51 neurons per audio minute input  |\n| @cf/deepgram/aura-2-en | $0.030 per 1k characters input  | 2727.27 neurons per 1k characters input  |\n| @cf/deepgram/aura-2-es | $0.030 per 1k characters input  | 2727.27 neurons per 1k characters input  |\n| @cf/deepgram/flux (WebSocket) | $0.0077 per audio minute  | 700.00 neurons per audio minute  |\n\n## Other model pricing\n\n| Model | Price in Tokens | Price in Neurons |\n| - | - | - |\n| @cf/huggingface/distilbert-sst-2-int8 | $0.026 per M input tokens | 2394 neurons per M input tokens |\n| @cf/baai/bge-reranker-base | $0.003 per M input tokens | 283 neurons per M input tokens |\n| @cf/meta/m2m100-1.2b | $0.342 per M input tokens $0.342 per M output tokens | 31050 neurons per M input tokens 31050 neurons per M output tokens |\n| @cf/microsoft/resnet-50 | $2.51 per M images | 228055 neurons per M images |\n| @cf/ai4bharat/indictrans2-en-indic-1B | $0.342 per M input tokens $0.342 per M output tokens | 31050 neurons per M input tokens 31050 neurons per M output tokens |\n\n<page>\n---\ntitle: Choose a data or storage product · Cloudflare Workers AI docs\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/platform/storage-options/\n  md: https://developers.cloudflare.com/workers-ai/platform/storage-options/index.md\n---\n\n<page>\n---\ntitle: Cloudflare Tunnel · Cloudflare Workers VPC\ndescription: Cloudflare Tunnel creates secure connections from your\n  infrastructure to Cloudflare's global network, providing the network\n  connectivity that allows Workers to access your private resources.\nlastUpdated: 2025-11-14T21:25:44.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-vpc/configuration/tunnel/\n  md: https://developers.cloudflare.com/workers-vpc/configuration/tunnel/index.md\n---\n\nCloudflare Tunnel creates secure connections from your infrastructure to Cloudflare's global network, providing the network connectivity that allows Workers to access your private resources.\n\nWhen you create a VPC Service, you specify a tunnel ID and target service. Workers VPC then routes requests from your Worker to the specified tunnel, which establishes a connection to the specified hostname or IP address, such that the target service receives the request and returns a response back to your Worker.\n\nTo allow members to create VPC Services that represent a target service reachable via a tunnel, you must assign them the **Connectivity Directory Admin** role. Members must possess **Connectivity Directory Bind** role to bind to existing VPC Services from worker.\n\nThe tunnel maintains persistent connections to Cloudflare, eliminating the need for inbound firewall rules or public IP addresses.\n\nThis section provides tunnel configuration specific to Workers VPC use cases. For comprehensive tunnel documentation including monitoring and advanced configurations, refer to the [full Cloudflare Tunnel documentation](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/).\n\n## Create and run tunnel (`cloudflared`)\n\nCloudflare Tunnel requires the installation of a lightweight and highly scalable server-side daemon, `cloudflared`, to connect your infrastructure to Cloudflare.\n\nVersion and Configuration\n\nEnsure you are running `cloudflared` version 2025.7.0 or later (latest version recommended) to ensure proper DNS resolution and connectivity. Older versions are not supported.\n\nWorkers VPC also requires Cloudflare Tunnel to connect using the [QUIC transport protocol](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/configure-tunnels/cloudflared-parameters/run-parameters/#protocol) using `auto` or `quic`. Ensure outbound UDP traffic on port 7844 is allowed through your firewall for QUIC connections.\n\nCloudflare Tunnels can be created one of two ways:\n\n1. **Remotely-managed tunnels (recommended):** Remotely-managed configurations are stored on Cloudflare, allowing you to manage the tunnel from any machine using the dashboard, API, or Terraform.\n2. **Locally-managed tunnels:** A locally-managed tunnel is created by running `cloudflared tunnel create <NAME>` on the command line. Tunnel configuration is stored in your local cloudflared directory.\n\nFor Workers VPC, we recommend creating a remotely-managed tunnel through the dashboard. Follow the [Tunnels for Workers VPC dashboard setup guide](https://developers.cloudflare.com/workers-vpc/get-started/) to create your tunnel with provided installation commands shown in the dashboard.\n\nFor locally-managed tunnels, refer to the [`cloudflared` locally-managed tunnels](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/do-more-with-tunnels/local-management/) guide. For manual installation, refer to the [`cloudflared` downloads page](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/downloads/) for platform-specific installation instructions.\n\nCloudflare Tunnels can either be configured for usage with [Cloudflare Zero Trust](https://developers.cloudflare.com/cloudflare-one/) or [Workers VPC](https://developers.cloudflare.com/workers-vpc/).\n\nUse Tunnels with Zero Trust when you are exposing internal applications securely to your employees with Cloudflare Access and hostnames.\n\nUse Tunnels with Workers VPC when you want to access private APIs, private databases, internal services or other HTTP services within your cloud or on-premise private network from Workers.\n\nThe same `cloudflared` instance can be used to cover both Zero Trust and Workers VPC use cases simultaneously.\n\n[Ingress configurations](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/do-more-with-tunnels/local-management/configuration-file/) for locally-managed tunnels are only relevant when using tunnels to expose services to the public internet, and are not required for Workers VPC as routing is handled by the VPC Service configuration.\n\n## Cloud platform setup guides\n\nFor platform-specific tunnel deployment instructions for production workloads:\n\n* [AWS](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/deployment-guides/aws/) - Deploy tunnels in Amazon Web Services\n* [Azure](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/deployment-guides/azure/) - Deploy tunnels in Microsoft Azure\n* [Google Cloud](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/deployment-guides/google-cloud-platform/) - Deploy tunnels in Google Cloud Platform\n* [Kubernetes](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/deployment-guides/kubernetes/) - Deploy tunnels in Kubernetes clusters\n* [Terraform](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/deployment-guides/terraform/) - Deploy tunnels using Infrastructure as Code\n\nRefer to the full Cloudflare Tunnel documentation on [how to setup Tunnels for high availability and failover with replicas](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/configure-tunnels/tunnel-availability/).\n\nWe do not recommend using `cloudflared` in autoscaling setups because downscaling (removing replicas) will break existing user connections to that replica. Additionally, `cloudflared` does not load balance across replicas; replicas are strictly for high availability and requests are routed to the nearest replica.\n\n* Configure [VPC Services](https://developers.cloudflare.com/workers-vpc/configuration/vpc-services/) to connect your tunnels to Workers\n* Review [hardware requirements](https://developers.cloudflare.com/workers-vpc/configuration/tunnel/hardware-requirements/) for capacity planning\n* Review the [complete Cloudflare Tunnel documentation](https://developers.cloudflare.com/cloudflare-one/networks/connectors/cloudflare-tunnel/) for advanced features\n\n<page>\n---\ntitle: VPC Services · Cloudflare Workers VPC\ndescription: VPC Services are the core building block of Workers VPC. They\n  represent specific resources in your private network that Workers can access\n  through Cloudflare Tunnel.\nlastUpdated: 2025-11-13T14:53:42.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-vpc/configuration/vpc-services/\n  md: https://developers.cloudflare.com/workers-vpc/configuration/vpc-services/index.md\n---\n\nVPC Services are the core building block of Workers VPC. They represent specific resources in your private network that Workers can access through Cloudflare Tunnel.\n\nYou can use bindings to connect to VPC Services from Workers. Every request made to a VPC Service using its `fetch` function will be securely routed to the configured service in the private network.\n\nVPC Services enforce that requests are routed to their intended service without exposing the entire network, securing your workloads and preventing server-side request forgery (SSRF).\n\nMembers must possess **Connectivity Directory Bind** role to bind to existing VPC Services from Workers. Creating VPC Services requires members to possess the **Connectivity Directory Admin** role.\n\nWorkers VPC is currently in beta. Features and APIs may change before general availability. While in beta, Workers VPC is available for free to all Workers plans.\n\n## VPC Service configuration\n\nA VPC Service consists of:\n\n* **Type**: Currently only `http` is supported (support for `tcp` coming soon)\n* **Tunnel ID**: The Cloudflare Tunnel that provides network connectivity\n* **Hostname or IPv4/IPv6 addresses**: The hostname, or IPv4 and/or IPv6 addresses to use to route to your service from the tunnel in your private network\n* **Ports**: HTTP and/or HTTPS port configuration (optional, defaults to 80/443)\n* **Resolver IPs**: Optionally, a specific resolver IP can be provided -- when not provided, `cloudflared` will direct DNS traffic to the currently configured default system resolver.\n\nRequests are encrypted in flight until they reach your network via a tunnel, regardless of the scheme used in the URL provided to `fetch`. If the `http` scheme is used, a plaintext connection is established to the service from the tunnel.\n\nThe `https` scheme can be used for an encrypted connection within your network, between the tunnel and your service. When the `https` scheme is specified, a hostname provided to the `fetch()` operation is utilized as the Server Name Indication (SNI) value.\n\nVPC Services default to allowing both `http` and `https` schemes to be used. You can provide values for only one of `http_port` or `https_port` to enforce the use of a particular scheme.\n\nWhen Workers VPC is unable to establish a connection to your service, `fetch()` will throw an exception.\n\nThe [VPC Service configuration](https://developers.cloudflare.com/workers-vpc/configuration/vpc-services/#vpc-service-configuration) host and port(s) will always be used to connect and route requests to your services, even if a different host or port is present in the URL provided to the `fetch()` operation in the Worker code.\n\nThe host provided in the `fetch()` operation is not used to route requests, and instead only populates the `Host` field for a HTTP request, or `Host` and the Server Name Indication (SNI) value presented to your service for a HTTPS request.\n\nThe port provided in the `fetch()` operation is ignored — the port specified in the VPC Service configuration for the provided scheme will be used.\n\n## Configuration example\n\nThe following is an example of a VPC Service for a service using custom HTTP and HTTPS ports, and both IPv4 and IPv6 addresses. These configurations represent the expected contract of the [REST API for creating a VPC Service](https://developers.cloudflare.com/api/resources/connectivity/subresources/directory/subresources/services/), a type of service within the broader connectivity directory.\n\nThe following is an example of a VPC Service for a service using custom HTTP and HTTPS ports as well, using a hostname. Note that since we are using a hostname, we must provide our service with a `resolver_network` that optionally has `resolver_ips`.\n\n## Workers binding configuration\n\nOnce you have created a VPC Service, you can bind it to your Worker:\n\nYou can have multiple VPC service bindings:\n\n* Set up [Cloudflare Tunnel](https://developers.cloudflare.com/workers-vpc/configuration/tunnel/) for your environment\n* Learn about the [Service Binding API](https://developers.cloudflare.com/workers-vpc/api/)\n* Refer to [examples](https://developers.cloudflare.com/workers-vpc/examples/) of common use cases\n\n<page>\n---\ntitle: Access a private API or website · Cloudflare Workers VPC\ndescription: This example demonstrates how to access a private REST API that is\n  not exposed to the public internet. In this guide, we will configure a VPC\n  Service for an internal API, create a Worker that makes requests to that API,\n  and deploy the Worker to validate our changes.\nlastUpdated: 2025-11-12T17:52:51.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-vpc/examples/private-api/\n  md: https://developers.cloudflare.com/workers-vpc/examples/private-api/index.md\n---\n\nThis example demonstrates how to access a private REST API that is not exposed to the public internet. In this guide, we will configure a VPC Service for an internal API, create a Worker that makes requests to that API, and deploy the Worker to validate our changes.\n\n* A virtual machine/EC2 instance running in your VPC/virtual network\n* A private API or website running in your VPC/virtual network with security rules allowing access to the virtual machine that will be running `cloudflared`\n* Workers account with Workers VPC access\n\n## 1. Set up Cloudflare Tunnel\n\nA Cloudflare Tunnel creates a secure connection from your private network to Cloudflare. This tunnel will allow Workers to securely access your private resources.\n\n1. Navigate to the [Workers VPC dashboard](https://dash.cloudflare.com/?to=/:account/workers/vpc/tunnels) and select the **Tunnels** tab.\n\n2. Select **Create** to create a new tunnel.\n\n3. Enter a name for your tunnel (for example, `private-api-tunnel`) and select **Save tunnel**.\n\n4. Choose your operating system and architecture. The dashboard will provide specific installation instructions for your environment.\n\n5. Follow the provided commands to download and install `cloudflared` on your VM, and execute the service installation command with your unique token.\n\nThe dashboard will confirm when your tunnel is successfully connected. Note the tunnel ID for the next step.\n\n## 2. Create the Workers VPC Service\n\nFirst, create a Workers VPC Service for your internal API:\n\nYou can also create a VPC Service for a service using its hostname:\n\nNote the service ID returned for the next step.\n\n## 3. Configure your Worker\n\nUpdate your `wrangler.toml`:\n\n## 4. Implement the Worker\n\nIn your Workers code, use the VPC Service binding in order to send requests to the service:\n\nThis guide demonstrates how you could create a simple proxy in your Workers. However, you could use VPC Services to fetch APIs directly and manipulate the responses to enable you to build more full-stack and backend functionality on Workers.\n\n## 5. Deploy and test\n\nNow, you can deploy and test your Worker that you have created:",
  "code_samples": [
    {
      "code": "export interface Env {\n  AI: Ai;\n}\n\n\nexport default {\n  async fetch(request, env): Promise<Response> {\n\n\n    const messages = [\n      { role: \"system\", content: \"You are a friendly assistant\" },\n      {\n        role: \"user\",\n        content: \"What is the origin of the phrase Hello, World\",\n      },\n    ];\n\n\n    const stream = await env.AI.run(\"@hf/thebloke/zephyr-7b-beta-awq\", {\n      messages,\n      stream: true,\n    });\n\n\n    return new Response(stream, {\n      headers: { \"content-type\": \"text/event-stream\" },\n    });\n  },\n} satisfies ExportedHandler<Env>;",
      "language": "ts"
    },
    {
      "code": "export interface Env {\n  AI: Ai;\n}\n\n\nexport default {\n  async fetch(request, env): Promise<Response> {\n\n\n    const messages = [\n      { role: \"system\", content: \"You are a friendly assistant\" },\n      {\n        role: \"user\",\n        content: \"What is the origin of the phrase Hello, World\",\n      },\n    ];\n    const response = await env.AI.run(\"@hf/thebloke/zephyr-7b-beta-awq\", { messages });\n\n\n    return Response.json(response);\n  },\n} satisfies ExportedHandler<Env>;",
      "language": "ts"
    },
    {
      "code": "import os\nimport requests\n\n\nACCOUNT_ID = \"your-account-id\"\nAUTH_TOKEN = os.environ.get(\"CLOUDFLARE_AUTH_TOKEN\")\n\n\nprompt = \"Tell me all about PEP-8\"\nresponse = requests.post(\n  f\"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@hf/thebloke/zephyr-7b-beta-awq\",\n    headers={\"Authorization\": f\"Bearer {AUTH_TOKEN}\"},\n    json={\n      \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a friendly assistant\"},\n        {\"role\": \"user\", \"content\": prompt}\n      ]\n    }\n)\nresult = response.json()\nprint(result)",
      "language": "py"
    },
    {
      "code": "curl https://api.cloudflare.com/client/v4/accounts/$CLOUDFLARE_ACCOUNT_ID/ai/run/@hf/thebloke/zephyr-7b-beta-awq \\\n  -X POST \\\n  -H \"Authorization: Bearer $CLOUDFLARE_AUTH_TOKEN\" \\\n  -d '{ \"messages\": [{ \"role\": \"system\", \"content\": \"You are a friendly assistant\" }, { \"role\": \"user\", \"content\": \"Why is pizza so good\" }]}'",
      "language": "sh"
    },
    {
      "code": "{\n      \"type\": \"object\",\n      \"oneOf\": [\n          {\n              \"title\": \"Prompt\",\n              \"properties\": {\n                  \"prompt\": {\n                      \"type\": \"string\",\n                      \"minLength\": 1,\n                      \"description\": \"The input text prompt for the model to generate a response.\"\n                  },\n                  \"lora\": {\n                      \"type\": \"string\",\n                      \"description\": \"Name of the LoRA (Low-Rank Adaptation) model to fine-tune the base model.\"\n                  },\n                  \"response_format\": {\n                      \"title\": \"JSON Mode\",\n                      \"type\": \"object\",\n                      \"properties\": {\n                          \"type\": {\n                              \"type\": \"string\",\n                              \"enum\": [\n                                  \"json_object\",\n                                  \"json_schema\"\n                              ]\n                          },\n                          \"json_schema\": {}\n                      }\n                  },\n                  \"raw\": {\n                      \"type\": \"boolean\",\n                      \"default\": false,\n                      \"description\": \"If true, a chat template is not applied and you must adhere to the specific model's expected formatting.\"\n                  },\n                  \"stream\": {\n                      \"type\": \"boolean\",\n                      \"default\": false,\n                      \"description\": \"If true, the response will be streamed back incrementally using SSE, Server Sent Events.\"\n                  },\n                  \"max_tokens\": {\n                      \"type\": \"integer\",\n                      \"default\": 256,\n                      \"description\": \"The maximum number of tokens to generate in the response.\"\n                  },\n                  \"temperature\": {\n                      \"type\": \"number\",\n                      \"default\": 0.6,\n                      \"minimum\": 0,\n                      \"maximum\": 5,\n                      \"description\": \"Controls the randomness of the output; higher values produce more random results.\"\n                  },\n                  \"top_p\": {\n                      \"type\": \"number\",\n                      \"minimum\": 0.001,\n                      \"maximum\": 1,\n                      \"description\": \"Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.\"\n                  },\n                  \"top_k\": {\n                      \"type\": \"integer\",\n                      \"minimum\": 1,\n                      \"maximum\": 50,\n                      \"description\": \"Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.\"\n                  },\n                  \"seed\": {\n                      \"type\": \"integer\",\n                      \"minimum\": 1,\n                      \"maximum\": 9999999999,\n                      \"description\": \"Random seed for reproducibility of the generation.\"\n                  },\n                  \"repetition_penalty\": {\n                      \"type\": \"number\",\n                      \"minimum\": 0,\n                      \"maximum\": 2,\n                      \"description\": \"Penalty for repeated tokens; higher values discourage repetition.\"\n                  },\n                  \"frequency_penalty\": {\n                      \"type\": \"number\",\n                      \"minimum\": -2,\n                      \"maximum\": 2,\n                      \"description\": \"Decreases the likelihood of the model repeating the same lines verbatim.\"\n                  },\n                  \"presence_penalty\": {\n                      \"type\": \"number\",\n                      \"minimum\": -2,\n                      \"maximum\": 2,\n                      \"description\": \"Increases the likelihood of the model introducing new topics.\"\n                  }\n              },\n              \"required\": [\n                  \"prompt\"\n              ]\n          },\n          {\n              \"title\": \"Messages\",\n              \"properties\": {\n                  \"messages\": {\n                      \"type\": \"array\",\n                      \"description\": \"An array of message objects representing the conversation history.\",\n                      \"items\": {\n                          \"type\": \"object\",\n                          \"properties\": {\n                              \"role\": {\n                                  \"type\": \"string\",\n                                  \"description\": \"The role of the message sender (e.g., 'user', 'assistant', 'system', 'tool').\"\n                              },\n                              \"content\": {\n                                  \"type\": \"string\",\n                                  \"description\": \"The content of the message as a string.\"\n                              }\n                          },\n                          \"required\": [\n                              \"role\",\n                              \"content\"\n                          ]\n                      }\n                  },\n                  \"functions\": {\n                      \"type\": \"array\",\n                      \"items\": {\n                          \"type\": \"object\",\n                          \"properties\": {\n                              \"name\": {\n                                  \"type\": \"string\"\n                              },\n                              \"code\": {\n                                  \"type\": \"string\"\n                              }\n                          },\n                          \"required\": [\n                              \"name\",\n                              \"code\"\n                          ]\n                      }\n                  },\n                  \"tools\": {\n                      \"type\": \"array\",\n                      \"description\": \"A list of tools available for the assistant to use.\",\n                      \"items\": {\n                          \"type\": \"object\",\n                          \"oneOf\": [\n                              {\n                                  \"properties\": {\n                                      \"name\": {\n                                          \"type\": \"string\",\n                                          \"description\": \"The name of the tool. More descriptive the better.\"\n                                      },\n                                      \"description\": {\n                                          \"type\": \"string\",\n                                          \"description\": \"A brief description of what the tool does.\"\n                                      },\n                                      \"parameters\": {\n                                          \"type\": \"object\",\n                                          \"description\": \"Schema defining the parameters accepted by the tool.\",\n                                          \"properties\": {\n                                              \"type\": {\n                                                  \"type\": \"string\",\n                                                  \"description\": \"The type of the parameters object (usually 'object').\"\n                                              },\n                                              \"required\": {\n                                                  \"type\": \"array\",\n                                                  \"description\": \"List of required parameter names.\",\n                                                  \"items\": {\n                                                      \"type\": \"string\"\n                                                  }\n                                              },\n                                              \"properties\": {\n                                                  \"type\": \"object\",\n                                                  \"description\": \"Definitions of each parameter.\",\n                                                  \"additionalProperties\": {\n                                                      \"type\": \"object\",\n                                                      \"properties\": {\n                                                          \"type\": {\n                                                              \"type\": \"string\",\n                                                              \"description\": \"The data type of the parameter.\"\n                                                          },\n                                                          \"description\": {\n                                                              \"type\": \"string\",\n                                                              \"description\": \"A description of the expected parameter.\"\n                                                          }\n                                                      },\n                                                      \"required\": [\n                                                          \"type\",\n                                                          \"description\"\n                                                      ]\n                                                  }\n                                              }\n                                          },\n                                          \"required\": [\n                                              \"type\",\n                                              \"properties\"\n                                          ]\n                                      }\n                                  },\n                                  \"required\": [\n                                      \"name\",\n                                      \"description\",\n                                      \"parameters\"\n                                  ]\n                              },\n                              {\n                                  \"properties\": {\n                                      \"type\": {\n                                          \"type\": \"string\",\n                                          \"description\": \"Specifies the type of tool (e.g., 'function').\"\n                                      },\n                                      \"function\": {\n                                          \"type\": \"object\",\n                                          \"description\": \"Details of the function tool.\",\n                                          \"properties\": {\n                                              \"name\": {\n                                                  \"type\": \"string\",\n                                                  \"description\": \"The name of the function.\"\n                                              },\n                                              \"description\": {\n                                                  \"type\": \"string\",\n                                                  \"description\": \"A brief description of what the function does.\"\n                                              },\n                                              \"parameters\": {\n                                                  \"type\": \"object\",\n                                                  \"description\": \"Schema defining the parameters accepted by the function.\",\n                                                  \"properties\": {\n                                                      \"type\": {\n                                                          \"type\": \"string\",\n                                                          \"description\": \"The type of the parameters object (usually 'object').\"\n                                                      },\n                                                      \"required\": {\n                                                          \"type\": \"array\",\n                                                          \"description\": \"List of required parameter names.\",\n                                                          \"items\": {\n                                                              \"type\": \"string\"\n                                                          }\n                                                      },\n                                                      \"properties\": {\n                                                          \"type\": \"object\",\n                                                          \"description\": \"Definitions of each parameter.\",\n                                                          \"additionalProperties\": {\n                                                              \"type\": \"object\",\n                                                              \"properties\": {\n                                                                  \"type\": {\n                                                                      \"type\": \"string\",\n                                                                      \"description\": \"The data type of the parameter.\"\n                                                                  },\n                                                                  \"description\": {\n                                                                      \"type\": \"string\",\n                                                                      \"description\": \"A description of the expected parameter.\"\n                                                                  }\n                                                              },\n                                                              \"required\": [\n                                                                  \"type\",\n                                                                  \"description\"\n                                                              ]\n                                                          }\n                                                      }\n                                                  },\n                                                  \"required\": [\n                                                      \"type\",\n                                                      \"properties\"\n                                                  ]\n                                              }\n                                          },\n                                          \"required\": [\n                                              \"name\",\n                                              \"description\",\n                                              \"parameters\"\n                                          ]\n                                      }\n                                  },\n                                  \"required\": [\n                                      \"type\",\n                                      \"function\"\n                                  ]\n                              }\n                          ]\n                      }\n                  },\n                  \"response_format\": {\n                      \"title\": \"JSON Mode\",\n                      \"type\": \"object\",\n                      \"properties\": {\n                          \"type\": {\n                              \"type\": \"string\",\n                              \"enum\": [\n                                  \"json_object\",\n                                  \"json_schema\"\n                              ]\n                          },\n                          \"json_schema\": {}\n                      }\n                  },\n                  \"raw\": {\n                      \"type\": \"boolean\",\n                      \"default\": false,\n                      \"description\": \"If true, a chat template is not applied and you must adhere to the specific model's expected formatting.\"\n                  },\n                  \"stream\": {\n                      \"type\": \"boolean\",\n                      \"default\": false,\n                      \"description\": \"If true, the response will be streamed back incrementally using SSE, Server Sent Events.\"\n                  },\n                  \"max_tokens\": {\n                      \"type\": \"integer\",\n                      \"default\": 256,\n                      \"description\": \"The maximum number of tokens to generate in the response.\"\n                  },\n                  \"temperature\": {\n                      \"type\": \"number\",\n                      \"default\": 0.6,\n                      \"minimum\": 0,\n                      \"maximum\": 5,\n                      \"description\": \"Controls the randomness of the output; higher values produce more random results.\"\n                  },\n                  \"top_p\": {\n                      \"type\": \"number\",\n                      \"minimum\": 0.001,\n                      \"maximum\": 1,\n                      \"description\": \"Adjusts the creativity of the AI's responses by controlling how many possible words it considers. Lower values make outputs more predictable; higher values allow for more varied and creative responses.\"\n                  },\n                  \"top_k\": {\n                      \"type\": \"integer\",\n                      \"minimum\": 1,\n                      \"maximum\": 50,\n                      \"description\": \"Limits the AI to choose from the top 'k' most probable words. Lower values make responses more focused; higher values introduce more variety and potential surprises.\"\n                  },\n                  \"seed\": {\n                      \"type\": \"integer\",\n                      \"minimum\": 1,\n                      \"maximum\": 9999999999,\n                      \"description\": \"Random seed for reproducibility of the generation.\"\n                  },\n                  \"repetition_penalty\": {\n                      \"type\": \"number\",\n                      \"minimum\": 0,\n                      \"maximum\": 2,\n                      \"description\": \"Penalty for repeated tokens; higher values discourage repetition.\"\n                  },\n                  \"frequency_penalty\": {\n                      \"type\": \"number\",\n                      \"minimum\": -2,\n                      \"maximum\": 2,\n                      \"description\": \"Decreases the likelihood of the model repeating the same lines verbatim.\"\n                  },\n                  \"presence_penalty\": {\n                      \"type\": \"number\",\n                      \"minimum\": -2,\n                      \"maximum\": 2,\n                      \"description\": \"Increases the likelihood of the model introducing new topics.\"\n                  }\n              },\n              \"required\": [\n                  \"messages\"\n              ]\n          }\n      ]\n  }",
      "language": "json"
    },
    {
      "code": "{\n      \"oneOf\": [\n          {\n              \"type\": \"object\",\n              \"properties\": {\n                  \"response\": {\n                      \"type\": \"string\",\n                      \"description\": \"The generated text response from the model\"\n                  },\n                  \"usage\": {\n                      \"type\": \"object\",\n                      \"description\": \"Usage statistics for the inference request\",\n                      \"properties\": {\n                          \"prompt_tokens\": {\n                              \"type\": \"number\",\n                              \"description\": \"Total number of tokens in input\",\n                              \"default\": 0\n                          },\n                          \"completion_tokens\": {\n                              \"type\": \"number\",\n                              \"description\": \"Total number of tokens in output\",\n                              \"default\": 0\n                          },\n                          \"total_tokens\": {\n                              \"type\": \"number\",\n                              \"description\": \"Total number of input and output tokens\",\n                              \"default\": 0\n                          }\n                      }\n                  },\n                  \"tool_calls\": {\n                      \"type\": \"array\",\n                      \"description\": \"An array of tool calls requests made during the response generation\",\n                      \"items\": {\n                          \"type\": \"object\",\n                          \"properties\": {\n                              \"arguments\": {\n                                  \"type\": \"object\",\n                                  \"description\": \"The arguments passed to be passed to the tool call request\"\n                              },\n                              \"name\": {\n                                  \"type\": \"string\",\n                                  \"description\": \"The name of the tool to be called\"\n                              }\n                          }\n                      }\n                  }\n              },\n              \"required\": [\n                  \"response\"\n              ]\n          },\n          {\n              \"type\": \"string\",\n              \"format\": \"binary\"\n          }\n      ]\n  }",
      "language": "json"
    },
    {
      "code": "{\n  \"type\": \"cf.workersAi.model.batch.queued\",\n  \"source\": {\n    \"type\": \"workersAi.model\",\n    \"modelName\": \"@cf/baai/bge-base-en-v1.5\"\n  },\n  \"payload\": {\n    \"requestId\": \"req-12345678-90ab-cdef-1234-567890abcdef\"\n  },\n  \"metadata\": {\n    \"accountId\": \"f9f79265f388666de8122cfb508d7776\",\n    \"eventSubscriptionId\": \"1830c4bb612e43c3af7f4cada31fbf3f\",\n    \"eventSchemaVersion\": 1,\n    \"eventTimestamp\": \"2025-05-01T02:48:57.132Z\"\n  }\n}",
      "language": "json"
    },
    {
      "code": "{\n  \"type\": \"cf.workersAi.model.batch.succeeded\",\n  \"source\": {\n    \"type\": \"workersAi.model\",\n    \"modelName\": \"@cf/baai/bge-base-en-v1.5\"\n  },\n  \"payload\": {\n    \"requestId\": \"req-12345678-90ab-cdef-1234-567890abcdef\"\n  },\n  \"metadata\": {\n    \"accountId\": \"f9f79265f388666de8122cfb508d7776\",\n    \"eventSubscriptionId\": \"1830c4bb612e43c3af7f4cada31fbf3f\",\n    \"eventSchemaVersion\": 1,\n    \"eventTimestamp\": \"2025-05-01T02:48:57.132Z\"\n  }\n}",
      "language": "json"
    },
    {
      "code": "{\n  \"type\": \"cf.workersAi.model.batch.failed\",\n  \"source\": {\n    \"type\": \"workersAi.model\",\n    \"modelName\": \"@cf/baai/bge-base-en-v1.5\"\n  },\n  \"payload\": {\n    \"requestId\": \"req-12345678-90ab-cdef-1234-567890abcdef\",\n    \"message\": \"Model execution failed\",\n    \"internalCode\": 5001,\n    \"httpCode\": 500\n  },\n  \"metadata\": {\n    \"accountId\": \"f9f79265f388666de8122cfb508d7776\",\n    \"eventSubscriptionId\": \"1830c4bb612e43c3af7f4cada31fbf3f\",\n    \"eventSchemaVersion\": 1,\n    \"eventTimestamp\": \"2025-05-01T02:48:57.132Z\"\n  }\n}",
      "language": "json"
    },
    {
      "code": "{\n  \"type\": \"http\",\n  \"name\": \"human-readable-name\",\n\n\n  // Port configuration (optional - defaults to 80/443)\n  \"http_port\": 80,\n  \"https_port\": 443,\n\n\n  // Host configuration\n  \"host\": {\n    \"ipv4\": \"10.0.0.1\",\n    \"ipv6\": \"fe80::\",\n    \"network\": {\n      \"tunnel_id\": \"0191dce4-9ab4-7fce-b660-8e5dec5172da\"\n    }\n  }\n}",
      "language": "json"
    },
    {
      "code": "{\n  \"type\": \"http\",\n  \"name\": \"human-readable-name\",\n\n\n  // Port configuration (optional - defaults to 80/443)\n  \"http_port\": 80,\n  \"https_port\": 443,\n\n\n  // Hostname Host (with DNS resolver)\n  \"host\": {\n    \"hostname\": \"example.com\",\n    \"resolver_network\": {\n      \"tunnel_id\": \"0191dce4-9ab4-7fce-b660-8e5dec5172da\",\n      \"resolver_ips\": [\"10.0.0.1\"] // Optional\n    }\n  }\n}",
      "language": "json"
    },
    {
      "code": "{\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"my-worker\",\n    \"main\": \"src/index.js\",\n    \"vpc_services\": [\n      {\n        \"binding\": \"PRIVATE_API\",\n        \"service_id\": \"e6a0817c-79c5-40ca-9776-a1c019defe70\",\n        \"remote\": true\n      }\n    ]\n  }",
      "language": "jsonc"
    },
    {
      "code": "name = \"my-worker\"\n  main = \"src/index.js\"\n\n\n  [[vpc_services]]\n  binding = \"PRIVATE_API\"\n  service_id = \"e6a0817c-79c5-40ca-9776-a1c019defe70\"\n  remote = true # When true, utilizes [remote bindings](/workers/development-testing/#remote-bindings) to allow access to the VPC Service during local development.",
      "language": "toml"
    },
    {
      "code": "{\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"vpc_services\": [\n      {\n        \"binding\": \"PRIVATE_API\",\n        \"service_id\": \"daf43e8c-a81a-4242-9912-4a2ebe4fdd79\",\n        \"remote\": true\n      },\n      {\n        \"binding\": \"PRIVATE_DATABASE\",\n        \"service_id\": \"453b6067-1327-420d-89b3-2b6ad16e6551\",\n        \"remote\": true\n      },\n      {\n        \"binding\": \"INTERNAL_CACHE\",\n        \"service_id\": \"6c39b574-237e-49f4-852a-cea5a93ed8f9\",\n        \"remote\": true\n      }\n    ]\n  }",
      "language": "jsonc"
    },
    {
      "code": "[[vpc_services]]\n  binding = \"PRIVATE_API\"\n  service_id = \"daf43e8c-a81a-4242-9912-4a2ebe4fdd79\"\n  remote = true\n\n\n  [[vpc_services]]\n  binding = \"PRIVATE_DATABASE\"\n  service_id = \"453b6067-1327-420d-89b3-2b6ad16e6551\"\n  remote = true\n\n\n  [[vpc_services]]\n  binding = \"INTERNAL_CACHE\"\n  service_id = \"6c39b574-237e-49f4-852a-cea5a93ed8f9\"\n  remote = true",
      "language": "toml"
    },
    {
      "code": "npx wrangler vpc service create api-service \\\n  --type http \\\n  --tunnel-id <YOUR_TUNNEL_ID> \\\n  --ipv4 10.0.1.50 \\\n  --http-port 8080",
      "language": "bash"
    },
    {
      "code": "npx wrangler vpc service create api-service \\\n  --type http \\\n  --tunnel-id <YOUR_TUNNEL_ID> \\\n  --hostname internal-hostname.example.com",
      "language": "bash"
    },
    {
      "code": "{\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"private-api-gateway\",\n    \"main\": \"src/index.js\",\n    \"compatibility_date\": \"2024-01-01\",\n    \"vpc_services\": [\n      {\n        \"binding\": \"INTERNAL_API\",\n        \"service_id\": \"<YOUR_SERVICE_ID>\",\n        \"remote\": true\n      }\n    ]\n  }",
      "language": "jsonc"
    },
    {
      "code": "name = \"private-api-gateway\"\n  main = \"src/index.js\"\n  compatibility_date = \"2024-01-01\"\n\n\n  [[vpc_services]]\n  binding = \"INTERNAL_API\"\n  service_id = \"<YOUR_SERVICE_ID>\"\n  remote = true",
      "language": "toml"
    },
    {
      "code": "export default {\n  async fetch(request, env, ctx) {\n    try {\n      // Fetch data from internal API and process it before returning\n      const response = await env.INTERNAL_API.fetch(\"http://10.0.1.50:8080/api/data\");\n\n\n      // Use the response of the private API to perform more logic in Workers, before returning the final response\n      return response;\n    } catch (error) {\n      return new Response(\"Service unavailable\", { status: 503 });\n    }\n  },\n};",
      "language": "js"
    },
    {
      "code": "npx wrangler deploy",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Playground",
      "id": "playground"
    },
    {
      "level": "h2",
      "text": "Usage",
      "id": "usage"
    },
    {
      "level": "h2",
      "text": "Parameters",
      "id": "parameters"
    },
    {
      "level": "h3",
      "text": "Input",
      "id": "input"
    },
    {
      "level": "h3",
      "text": "Output",
      "id": "output"
    },
    {
      "level": "h2",
      "text": "API Schemas",
      "id": "api-schemas"
    },
    {
      "level": "h2",
      "text": "Available Workers AI events",
      "id": "available-workers-ai-events"
    },
    {
      "level": "h2",
      "text": "Rate limits by task type",
      "id": "rate-limits-by-task-type"
    },
    {
      "level": "h3",
      "text": "[Automatic Speech Recognition](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[automatic-speech-recognition](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Image Classification](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[image-classification](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Image-to-Text](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[image-to-text](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Object Detection](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[object-detection](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Summarization](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[summarization](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Text Classification](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[text-classification](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Text Embeddings](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[text-embeddings](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Text Generation](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[text-generation](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Text-to-Image](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[text-to-image](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h3",
      "text": "[Translation](https://developers.cloudflare.com/workers-ai/models/)",
      "id": "[translation](https://developers.cloudflare.com/workers-ai/models/)"
    },
    {
      "level": "h2",
      "text": "What are Neurons?",
      "id": "what-are-neurons?"
    },
    {
      "level": "h2",
      "text": "LLM model pricing",
      "id": "llm-model-pricing"
    },
    {
      "level": "h2",
      "text": "Embeddings model pricing",
      "id": "embeddings-model-pricing"
    },
    {
      "level": "h2",
      "text": "Image model pricing",
      "id": "image-model-pricing"
    },
    {
      "level": "h2",
      "text": "Audio model pricing",
      "id": "audio-model-pricing"
    },
    {
      "level": "h2",
      "text": "Other model pricing",
      "id": "other-model-pricing"
    },
    {
      "level": "h2",
      "text": "Create and run tunnel (`cloudflared`)",
      "id": "create-and-run-tunnel-(`cloudflared`)"
    },
    {
      "level": "h2",
      "text": "Cloud platform setup guides",
      "id": "cloud-platform-setup-guides"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "VPC Service configuration",
      "id": "vpc-service-configuration"
    },
    {
      "level": "h2",
      "text": "Configuration example",
      "id": "configuration-example"
    },
    {
      "level": "h2",
      "text": "Workers binding configuration",
      "id": "workers-binding-configuration"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Set up Cloudflare Tunnel",
      "id": "1.-set-up-cloudflare-tunnel"
    },
    {
      "level": "h2",
      "text": "2. Create the Workers VPC Service",
      "id": "2.-create-the-workers-vpc-service"
    },
    {
      "level": "h2",
      "text": "3. Configure your Worker",
      "id": "3.-configure-your-worker"
    },
    {
      "level": "h2",
      "text": "4. Implement the Worker",
      "id": "4.-implement-the-worker"
    },
    {
      "level": "h2",
      "text": "5. Deploy and test",
      "id": "5.-deploy-and-test"
    }
  ],
  "url": "llms-txt#zephyr-7b-beta-awq-beta",
  "links": []
}