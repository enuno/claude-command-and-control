{
  "title": "Rewrite data files with a target file size (e.g., 512 MB)",
  "content": "spark.sql(\"\"\"\n  CALL r2dc.system.rewrite_data_files(\n    table => 'r2dc.namespace_name.table_name',\n    options => map('target-file-size-bytes', '536870912')\n  )\n\"\"\")\nplaintext\n     npx wrangler login\n     plaintext\n     npx wrangler r2 bucket create r2-data-catalog-tutorial\n     plaintext\n  npx wrangler r2 bucket catalog enable r2-data-catalog-tutorial\n  plaintext\n   mkdir r2-data-catalog-notebook\n   plaintext\n   cd r2-data-catalog-notebook\n   plaintext\n   uv init\n   py\n   uv add marimo pyiceberg pyarrow pandas\n   py\n   import marimo\n\n__generated_with = \"0.11.31\"\n   app = marimo.App(width=\"medium\")\n\n@app.cell\n   def _():\n       import marimo as mo\n       return (mo,)\n\n@app.cell\n   def _():\n       import pandas\n       import pyarrow as pa\n       import pyarrow.compute as pc\n       import pyarrow.parquet as pq\n\nfrom pyiceberg.catalog.rest import RestCatalog\n\n# Define catalog connection details (replace variables)\n       WAREHOUSE = \"<WAREHOUSE>\"\n       TOKEN = \"<TOKEN>\"\n       CATALOG_URI = \"<CATALOG_URI>\"\n\n# Connect to R2 Data Catalog\n       catalog = RestCatalog(\n           name=\"my_catalog\",\n           warehouse=WAREHOUSE,\n           uri=CATALOG_URI,\n           token=TOKEN,\n       )\n       return (\n           CATALOG_URI,\n           RestCatalog,\n           TOKEN,\n           WAREHOUSE,\n           catalog,\n           pa,\n           pandas,\n           pc,\n           pq,\n       )\n\n@app.cell\n   def _(catalog):\n       # Create default namespace if needed\n       catalog.create_namespace_if_not_exists(\"default\")\n       return\n\n@app.cell\n   def _(pa):\n       # Create simple PyArrow table\n       df = pa.table({\n           \"id\": [1, 2, 3],\n           \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n           \"score\": [80.0, 92.5, 88.0],\n       })\n       return (df,)\n\n@app.cell\n   def _(catalog, df):\n       # Create or load Iceberg table\n       test_table = (\"default\", \"people\")\n       if not catalog.table_exists(test_table):\n           print(f\"Creating table: {test_table}\")\n           table = catalog.create_table(\n               test_table,\n               schema=df.schema,\n           )\n       else:\n           table = catalog.load_table(test_table)\n       return table, test_table\n\n@app.cell\n   def _(df, table):\n       # Append data\n       table.append(df)\n       return\n\n@app.cell\n   def _(table):\n       print(\"Table contents:\")\n       scanned = table.scan().to_arrow()\n       print(scanned.to_pandas())\n       return (scanned,)\n\n@app.cell\n   def _():\n       # Optional cleanup. To run uncomment and run cell\n       # print(f\"Deleting table: {test_table}\")\n       # catalog.drop_table(test_table)\n       # print(\"Table dropped.\")\n       return\n\nif __name__ == \"__main__\":\n       app.run()\n   plaintext\n   uv run marimo edit r2-data-catalog-tutorial.py\n   bash\n  npx wrangler r2 bucket catalog enable <BUCKET_NAME>\n  bash\n  npx wrangler r2 bucket catalog disable <BUCKET_NAME>\n  bash\n  # Enable catalog-level compaction (all tables)\n  npx wrangler r2 bucket catalog compaction enable <BUCKET_NAME> --target-size 128 --token <API_TOKEN>\n\n# Enable compaction for a specific table\n  npx wrangler r2 bucket catalog compaction enable <BUCKET_NAME> <NAMESPACE> <TABLE> --target-size 128\n  bash\n  # Disable catalog-level compaction (all tables)\n  npx wrangler r2 bucket catalog compaction disable <BUCKET_NAME>\n\n# Disable compaction for a specific table\n  npx wrangler r2 bucket catalog compaction disable <BUCKET_NAME> <NAMESPACE> <TABLE>\n  bash",
  "code_samples": [
    {
      "code": "## About Apache Iceberg metadata\n\nApache Iceberg uses a layered metadata structure to manage table data efficiently. Here are the key components and file structure:\n\n* **metadata.json**: Top-level JSON file pointing to the current snapshot\n* **snapshot-**\\*: Immutable table state for a given point in time\n* **manifest-list-\\*.avro**: An Avro file listing all manifest files for a given snapshot\n* **manifest-file-\\*.avro**: An Avro file tracking data files and their statistics\n* **data-\\*.parquet**: Parquet files containing actual table data\n* **Note**: Unchanged manifest files are reused across snapshots\n\nWarning\n\nManually modifying or deleting any of these files directly can lead to data catalog corruption.\n\n### What happens during deletion\n\nApache Iceberg supports two deletion modes: **Copy-on-Write (COW)** and **Merge-on-Read (MOR)**. Both create a new snapshot and mark old files for cleanup, but handle the deletion differently:\n\n| Aspect | Copy-on-Write (COW) | Merge-on-Read (MOR) |\n| - | - | - |\n| **How deletes work** | Rewrites data files without deleted rows | Creates delete files marking rows to skip |\n| **Query performance** | Fast (no merge needed) | Slower (requires read-time merge) |\n| **Write performance** | Slower (rewrites data files) | Fast (only writes delete markers) |\n| **Storage impact** | Creates new data files immediately | Accumulates delete files over time |\n| **Maintenance needs** | Snapshot expiration | Snapshot expiration + compaction (`rewrite_data_files`) |\n| **Best for** | Read-heavy workloads | Write-heavy workloads with frequent small mutations |\n\nImportant for all deletion modes\n\n* Deleted data is **not immediately removed** from R2 - files are marked for cleanup\n* Enable [snapshot expiration](https://developers.cloudflare.com/r2/data-catalog/table-maintenance) in R2 Data Catalog to automatically clean up old snapshots and files\n\n### Common deletion operations\n\nThese operations work the same way for both COW and MOR tables:\n\n| Operation | What it does | Data deleted? | Reversible? |\n| - | - | - | - |\n| `DELETE FROM` | Removes rows matching condition | No (marked for cleanup) | Via time travel[1](#user-content-fn-1) |\n| `DROP TABLE` | Removes table from catalog | No | Yes (if data files exist) |\n| `DROP TABLE ... PURGE` | Removes table and deletes data | **Yes** | **No** |\n| `expire_snapshots` | Cleans up old snapshots/files | **Yes** | **No** |\n| `remove_orphan_files` | Removes unreferenced files | **Yes** | **No** |\n\n### MOR-specific operations\n\nFor Merge-on-Read tables, you may need to manually apply deletes for performance:\n\n| Operation | What it does | When to use |\n| - | - | - |\n| `rewrite_data_files` (compaction) | Applies deletes and consolidates files | When query performance degrades due to many delete files |\n\nNote\n\nR2 Data Catalog can automate [rewriting data files](https://developers.cloudflare.com/r2/data-catalog/table-maintenance/) for you.\n\n## Related resources\n\n* [Table maintenance](https://developers.cloudflare.com/r2/data-catalog/table-maintenance) - Learn about automatic maintenance operations\n* [R2 Data Catalog](https://developers.cloudflare.com/r2/data-catalog/) - Overview and getting started guide\n* [Query data](https://developers.cloudflare.com/r2-sql/query-data) - Query tables with R2 SQL\n* [Apache Iceberg Maintenance](https://iceberg.apache.org/docs/latest/maintenance/) - Official Iceberg documentation on table maintenance\n\n## Footnotes\n\n1. Time travel available until `expire_snapshots` is called [↩](#user-content-fnref-1)\n\n</page>\n\n<page>\n---\ntitle: Getting started · Cloudflare R2 docs\ndescription: Learn how to enable the R2 Data Catalog on your bucket, load sample\n  data, and run your first query.\nlastUpdated: 2025-09-25T04:07:16.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-catalog/get-started/\n  md: https://developers.cloudflare.com/r2/data-catalog/get-started/index.md\n---\n\nThis guide will instruct you through:\n\n* Creating your first [R2 bucket](https://developers.cloudflare.com/r2/buckets/) and enabling its [data catalog](https://developers.cloudflare.com/r2/data-catalog/).\n* Creating an [API token](https://developers.cloudflare.com/r2/api/tokens/) needed for query engines to authenticate with your data catalog.\n* Using [PyIceberg](https://py.iceberg.apache.org/) to create your first Iceberg table in a [marimo](https://marimo.io/) Python notebook.\n* Using [PyIceberg](https://py.iceberg.apache.org/) to load sample data into your table and query it.\n\n## Prerequisites\n\n1. Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages).\n2. Install [`Node.js`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).\n\nNode.js version manager\n\nUse a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm) to avoid permission issues and change Node.js versions. [Wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/), discussed later in this guide, requires a Node version of `16.17.0` or later.\n\n## 1. Create an R2 bucket\n\n* Wrangler CLI\n\n  1. If not already logged in, run:",
      "language": "unknown"
    },
    {
      "code": "2. Create an R2 bucket:",
      "language": "unknown"
    },
    {
      "code": "* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select **Create bucket**.\n\n  3. Enter the bucket name: r2-data-catalog-tutorial\n\n  4. Select **Create bucket**.\n\n## 2. Enable the data catalog for your bucket\n\n* Wrangler CLI\n\n  Then, enable the catalog on your chosen R2 bucket:",
      "language": "unknown"
    },
    {
      "code": "When you run this command, take note of the \"Warehouse\" and \"Catalog URI\". You will need these later.\n\n* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket: r2-data-catalog-tutorial.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, and select **Enable**.\n\n  4. Once enabled, note the **Catalog URI** and **Warehouse name**.\n\n## 3. Create an API token\n\nIceberg clients (including [PyIceberg](https://py.iceberg.apache.org/)) must authenticate to the catalog with an [R2 API token](https://developers.cloudflare.com/r2/api/tokens/) that has both R2 and catalog permissions.\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Select **Manage API tokens**.\n\n3. Select **Create API token**.\n\n4. Select the **R2 Token** text to edit your API token name.\n\n5. Under **Permissions**, choose the **Admin Read & Write** permission.\n\n6. Select **Create API Token**.\n\n7. Note the **Token value**.\n\n## 4. Install uv\n\nYou need to install a Python package manager. In this guide, use [uv](https://docs.astral.sh/uv/). If you do not already have uv installed, follow the [installing uv guide](https://docs.astral.sh/uv/getting-started/installation/).\n\n## 5. Install marimo and set up your project with uv\n\nWe will use [marimo](https://github.com/marimo-team/marimo) as a Python notebook.\n\n1. Create a directory where our notebook will be stored:",
      "language": "unknown"
    },
    {
      "code": "2. Change into our new directory:",
      "language": "unknown"
    },
    {
      "code": "3. Initialize a new uv project (this creates a `.venv` and a `pyproject.toml`):",
      "language": "unknown"
    },
    {
      "code": "4. Add marimo and required dependencies:",
      "language": "unknown"
    },
    {
      "code": "## 6. Create a Python notebook to interact with the data warehouse\n\n1. Create a file called `r2-data-catalog-tutorial.py`.\n\n2. Paste the following code snippet into your `r2-data-catalog-tutorial.py` file:",
      "language": "unknown"
    },
    {
      "code": "3. Replace the `CATALOG_URI`, `WAREHOUSE`, and `TOKEN` variables with your values from sections **2** and **3** respectively.\n\n4. Launch the notebook editor in your browser:",
      "language": "unknown"
    },
    {
      "code": "Once your notebook connects to the catalog, you'll see the catalog along with its namespaces and tables appear in marimo's Datasources panel.\n\nIn the Python notebook above, you:\n\n1. Connect to your catalog.\n2. Create the `default` namespace.\n3. Create a simple PyArrow table.\n4. Create (or load) the `people` table in the `default` namespace.\n5. Append sample data to the table.\n6. Print the contents of the table.\n7. (Optional) Drop the `people` table we created for this tutorial.\n\n## Learn more\n\n[Managing catalogs ](https://developers.cloudflare.com/r2/data-catalog/manage-catalogs/)Enable or disable R2 Data Catalog on your bucket, retrieve configuration details, and authenticate your Iceberg engine.\n\n[Connect to Iceberg engines ](https://developers.cloudflare.com/r2/data-catalog/config-examples/)Find detailed setup instructions for Apache Spark and other common query engines.\n\n</page>\n\n<page>\n---\ntitle: Manage catalogs · Cloudflare R2 docs\ndescription: Understand how to manage Iceberg REST catalogs associated with R2 buckets\nlastUpdated: 2025-12-18T17:16:51.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-catalog/manage-catalogs/\n  md: https://developers.cloudflare.com/r2/data-catalog/manage-catalogs/index.md\n---\n\nLearn how to:\n\n* Enable and disable [R2 Data Catalog](https://developers.cloudflare.com/r2/data-catalog/) on your buckets.\n* Enable and disable [table maintenance](https://developers.cloudflare.com/r2/data-catalog/table-maintenance/) features like compaction and snapshot expiration.\n* Authenticate Iceberg engines using API tokens.\n\n## Enable R2 Data Catalog on a bucket\n\nEnabling the catalog on a bucket turns on the REST catalog interface and provides a **Catalog URI** and **Warehouse name** required by Iceberg clients. Once enabled, you can create and manage Iceberg tables in that bucket.\n\n* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket you want to enable as a data catalog.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, and select **Enable**.\n\n  4. Once enabled, note the **Catalog URI** and **Warehouse name**.\n\n* Wrangler CLI\n\n  To enable the catalog on your bucket, run the [`r2 bucket catalog enable command`](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-catalog-enable):",
      "language": "unknown"
    },
    {
      "code": "After enabling, Wrangler will return your catalog URI and warehouse name.\n\n## Disable R2 Data Catalog on a bucket\n\nWhen you disable the catalog on a bucket, it immediately stops serving requests from the catalog interface. Any Iceberg table references stored in that catalog become inaccessible until you re-enable it.\n\n* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket where you want to disable the data catalog.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, and select **Disable**.\n\n* Wrangler CLI\n\n  To disable the catalog on your bucket, run the [`r2 bucket catalog disable command`](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-catalog-disable):",
      "language": "unknown"
    },
    {
      "code": "## Enable compaction\n\nCompaction improves query performance by combining the many small files created during data ingestion into fewer, larger files according to the set `target file size`. For more information about compaction and why it's valuable, refer to [About compaction](https://developers.cloudflare.com/r2/data-catalog/table-maintenance/).\n\n* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket you want to enable compaction on.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, and click on the **Edit** icon next to the compaction card.\n\n  4. Enable compaction and optionally set a target file size. The default is 128 MB.\n\n  5. (Optional) Provide a Cloudflare API token for compaction to access and rewrite files in your bucket.\n\n  6. Select **Save**.\n\n* Wrangler CLI\n\n  To enable the compaction on your catalog, run the [`r2 bucket catalog compaction enable` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-catalog-compaction-enable):",
      "language": "unknown"
    },
    {
      "code": "Table-level vs Catalog-level compaction\n\n  * **Catalog-level**: Applies to all tables in the bucket; requires an API token as a service credential.\n  * **Table-level**: Applies to a specific table only.\n\nAPI token permission requirements\n\nCompaction requires a Cloudflare API token with both R2 storage and R2 Data Catalog read/write permissions to act as a service credential. The compaction process uses this token to read files, combine them, and update table metadata.\n\nRefer to [Authenticate your Iceberg engine](#authenticate-your-iceberg-engine) for details on creating a token with the required permissions.\n\nOnce enabled, compaction applies retroactively to all existing tables (for catalog-level compaction) or the specified table (for table-level compaction). During open beta, we currently compact up to 2 GB worth of files once per hour for each table.\n\n## Disable compaction\n\nDisabling compaction will prevent the process from running for all tables (catalog level) or a specific table (table level). You can re-enable it at any time.\n\n* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket you want to enable compaction on.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, and click on the **edit** icon next to the compaction card.\n\n  4. Disable compaction.\n\n  5. Select **Save**.\n\n* Wrangler CLI\n\n  To disable the compaction on your catalog, run the [`r2 bucket catalog compaction disable` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-catalog-compaction-disable):",
      "language": "unknown"
    },
    {
      "code": "## Enable snapshot expiration\n\nSnapshot expiration automatically removes old table snapshots to reduce metadata bloat and storage costs. For more information about snapshot expiration and why it is valuable, refer to [Table maintenance](https://developers.cloudflare.com/r2/data-catalog/table-maintenance/).\n\nTo enable snapshot expiration on your catalog, run the [`r2 bucket catalog snapshot-expiration enable` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-catalog-snapshot-expiration-enable):",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "About Apache Iceberg metadata",
      "id": "about-apache-iceberg-metadata"
    },
    {
      "level": "h3",
      "text": "What happens during deletion",
      "id": "what-happens-during-deletion"
    },
    {
      "level": "h3",
      "text": "Common deletion operations",
      "id": "common-deletion-operations"
    },
    {
      "level": "h3",
      "text": "MOR-specific operations",
      "id": "mor-specific-operations"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Footnotes",
      "id": "footnotes"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Create an R2 bucket",
      "id": "1.-create-an-r2-bucket"
    },
    {
      "level": "h2",
      "text": "2. Enable the data catalog for your bucket",
      "id": "2.-enable-the-data-catalog-for-your-bucket"
    },
    {
      "level": "h2",
      "text": "3. Create an API token",
      "id": "3.-create-an-api-token"
    },
    {
      "level": "h2",
      "text": "4. Install uv",
      "id": "4.-install-uv"
    },
    {
      "level": "h2",
      "text": "5. Install marimo and set up your project with uv",
      "id": "5.-install-marimo-and-set-up-your-project-with-uv"
    },
    {
      "level": "h2",
      "text": "6. Create a Python notebook to interact with the data warehouse",
      "id": "6.-create-a-python-notebook-to-interact-with-the-data-warehouse"
    },
    {
      "level": "h2",
      "text": "Learn more",
      "id": "learn-more"
    },
    {
      "level": "h2",
      "text": "Enable R2 Data Catalog on a bucket",
      "id": "enable-r2-data-catalog-on-a-bucket"
    },
    {
      "level": "h2",
      "text": "Disable R2 Data Catalog on a bucket",
      "id": "disable-r2-data-catalog-on-a-bucket"
    },
    {
      "level": "h2",
      "text": "Enable compaction",
      "id": "enable-compaction"
    },
    {
      "level": "h2",
      "text": "Disable compaction",
      "id": "disable-compaction"
    },
    {
      "level": "h2",
      "text": "Enable snapshot expiration",
      "id": "enable-snapshot-expiration"
    }
  ],
  "url": "llms-txt#rewrite-data-files-with-a-target-file-size-(e.g.,-512-mb)",
  "links": []
}