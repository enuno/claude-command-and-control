{
  "title": "Disable snapshot expiration for a catalog",
  "content": "npx wrangler r2 bucket catalog snapshot-expiration disable my-bucket\nsh\nnpx wrangler r2 bucket sippy enable <BUCKET_NAME>\nsh\nnpx wrangler r2 bucket sippy disable <BUCKET_NAME>\njson\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Action\": [\"s3:ListBucket*\", \"s3:GetObject*\"],\n         \"Resource\": [\n           \"arn:aws:s3:::<BUCKET_NAME>\",\n           \"arn:aws:s3:::<BUCKET_NAME>/*\"\n         ]\n       }\n     ]\n   }\n   json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\"s3:Get*\", \"s3:List*\"],\n      \"Resource\": [\"arn:aws:s3:::<BUCKET_NAME>\", \"arn:aws:s3:::<BUCKET_NAME>/*\"]\n    }\n  ]\n}\nsh\nexport AWS_REGION=auto\nexport AWS_ENDPOINT_URL=https://<account_id>.r2.cloudflarestorage.com\nexport AWS_ACCESS_KEY_ID=your_access_key_id\nexport AWS_SECRET_ACCESS_KEY=your_secret_access_key\nsh\n    npm i @aws-sdk/client-s3\n    sh\n    yarn add @aws-sdk/client-s3\n    sh\n    pnpm add @aws-sdk/client-s3\n    javascript\n  import { GetObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\n\nconst s3 = new S3Client();\n\nconst Bucket = \"<YOUR_BUCKET_NAME>\";\n  const Key = \"pfp.jpg\";\n\nconst object = await s3.send(\n    new GetObjectCommand({\n      Bucket,\n      Key,\n    }),\n  );\n\nconsole.log(\"Successfully fetched the object\", object.$metadata);\n\n// Process the data as needed\n  // For example, to get the content as a Buffer:\n  // const content = data.Body;\n\n// Or to save the file (requires 'fs' module):\n  // import { writeFile } from \"node:fs/promises\";\n  // await writeFile('ingested_0001.parquet', data.Body);\n  sh\n  pip install boto3\n  python\n  import boto3\n  from botocore.client import Config\n\n# Configure the S3 client for Cloudflare R2\n  s3_client = boto3.client('s3',\n    config=Config(signature_version='s3v4')\n  )\n\n# Specify the object key\n  #\n  bucket = '<YOUR_BUCKET_NAME>'\n  object_key = '2024/08/02/ingested_0001.parquet'\n\ntry:\n    # Fetch the object\n    response = s3_client.get_object(Bucket=bucket, Key=object_key)\n\nprint('Successfully fetched the object')\n\n# Process the response content as needed\n    # For example, to read the content:\n    # object_content = response['Body'].read()\n\n# Or to save the file:\n    # with open('ingested_0001.parquet', 'wb') as f:\n    #     f.write(response['Body'].read())\n\nexcept Exception as e:\n    print(f'Failed to fetch the object. Error: {str(e)}')\n  sh\n  go get github.com/aws/aws-sdk-go-v2\n  go get github.com/aws/aws-sdk-go-v2/config\n  go get github.com/aws/aws-sdk-go-v2/credentials\n  go get github.com/aws/aws-sdk-go-v2/service/s3\n  go\n  package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"io\"\n    \"log\"\n    \"github.com/aws/aws-sdk-go-v2/aws\"\n    \"github.com/aws/aws-sdk-go-v2/config\"\n    \"github.com/aws/aws-sdk-go-v2/service/s3\"\n  )\n\nfunc main() {\n      cfg, err := config.LoadDefaultConfig(context.TODO())\n      if err != nil {\n        log.Fatalf(\"Unable to load SDK config, %v\", err)\n      }\n\n// Create an S3 client\n      client := s3.NewFromConfig(cfg)\n\n// Specify the object key\n      bucket := \"<YOUR_BUCKET_NAME>\"\n      objectKey := \"pfp.jpg\"\n\n// Fetch the object\n      output, err := client.GetObject(context.TODO(), &s3.GetObjectInput{\n        Bucket: aws.String(bucket),\n        Key:    aws.String(objectKey),\n      })\n      if err != nil {\n        log.Fatalf(\"Unable to fetch object, %v\", err)\n      }\n      defer output.Body.Close()\n\nfmt.Println(\"Successfully fetched the object\")\n\n// Process the object content as needed\n      // For example, to save the file:\n      // file, err := os.Create(\"ingested_0001.parquet\")\n      // if err != nil {\n      //   log.Fatalf(\"Unable to create file, %v\", err)\n      // }\n      // defer file.Close()\n      // _, err = io.Copy(file, output.Body)\n      // if err != nil {\n      //   log.Fatalf(\"Unable to write file, %v\", err)\n      // }\n\n// Or to read the content:\n      content, err := io.ReadAll(output.Body)\n      if err != nil {\n        log.Fatalf(\"Unable to read object content, %v\", err)\n      }\n      fmt.Printf(\"Object content length: %d bytes\\n\", len(content))\n  }\n  sh\n  npm i @aws-sdk/client-s3\n  sh\n  yarn add @aws-sdk/client-s3\n  sh\n  pnpm add @aws-sdk/client-s3\n  js\nexport default {\n  async fetch(request, env, context) {\n    try {\n      const url = new URL(request.url);\n\n// Construct the cache key from the cache URL\n      const cacheKey = new Request(url.toString(), request);\n      const cache = caches.default;\n\n// Check whether the value is already available in the cache\n      // if not, you will need to fetch it from R2, and store it in the cache\n      // for future access\n      let response = await cache.match(cacheKey);\n\nif (response) {\n        console.log(`Cache hit for: ${request.url}.`);\n        return response;\n      }\n\nconsole.log(\n        `Response for request url: ${request.url} not present in cache. Fetching and caching request.`\n      );\n\n// If not in cache, get it from R2\n      const objectKey = url.pathname.slice(1);\n      const object = await env.MY_BUCKET.get(objectKey);\n      if (object === null) {\n        return new Response('Object Not Found', { status: 404 });\n      }\n\n// Set the appropriate object headers\n      const headers = new Headers();\n      object.writeHttpMetadata(headers);\n      headers.set('etag', object.httpEtag);\n\n// Cache API respects Cache-Control headers. Setting s-max-age to 10\n      // will limit the response to be in cache for 10 seconds max\n      // Any changes made to the response here will be reflected in the cached value\n      headers.append('Cache-Control', 's-maxage=10');\n\nresponse = new Response(object.body, {\n        headers,\n      });\n\n// Store the fetched response as cacheKey\n      // Use waitUntil so you can return the response without blocking on\n      // writing to cache\n      context.waitUntil(cache.put(cacheKey, response.clone()));\n\nreturn response;\n    } catch (e) {\n      return new Response('Error thrown ' + e.message);\n    }\n  },\n};\nsh\nrclone config file",
  "code_samples": [
    {
      "code": "### Choose the right retention policy\n\nDifferent workloads require different snapshot retention strategies:\n\n* **Development/testing tables**: Shorter retention (2-7 days, 5 snapshots) to minimize storage costs\n* **Production analytics tables**: Medium retention (7-30 days, 10-20 snapshots) for debugging and analysis\n* **Compliance/audit tables**: Longer retention (30-90 days, 50+ snapshots) to meet regulatory requirements\n* **High-frequency ingest**: Higher minimum snapshot count to preserve more granular history\n\nThese are generic recommendations, make sure to consider:\n\n* Time travel requirements\n* Compliance requirements\n* Storage costs\n\n## Current limitations\n\n* During open beta, compaction will compact up to 2 GB worth of files once per hour for each table.\n* Only data files stored in parquet format are currently supported with compaction.\n* Orphan file cleanup is not supported yet.\n* Minimum target file size for compaction is 64 MB and maximum is 512 MB.\n\n</page>\n\n<page>\n---\ntitle: Migration Strategies · Cloudflare R2 docs\ndescription: You can use a combination of Super Slurper and Sippy to effectively\n  migrate all objects with minimal downtime.\nlastUpdated: 2025-10-21T17:09:06.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-migration/migration-strategies/\n  md: https://developers.cloudflare.com/r2/data-migration/migration-strategies/index.md\n---\n\nYou can use a combination of Super Slurper and Sippy to effectively migrate all objects with minimal downtime.\n\n### When the source bucket is actively being read from / written to\n\n1. Enable Sippy and start using the R2 bucket in your application.\n\n   * This copies objects from your previous bucket into the R2 bucket on demand when they are requested by the application.\n   * New uploads will go to the R2 bucket.\n\n2. Use Super Slurper to trigger a one-off migration to copy the remaining objects into the R2 bucket.\n   * In the **Destination R2 bucket** > **Overwrite files?**, select \"Skip existing\".\n\n### When the source bucket is not being read often\n\n1. Use Super Slurper to copy all objects to the R2 bucket.\n   * Note that Super Slurper may skip some objects if they are uploaded after it lists the objects to be copied.\n\n2. Enable Sippy on your R2 bucket, then start using the R2 bucket in your application.\n\n   * New uploads will go to the R2 bucket.\n   * Objects which were uploaded while Super Slurper was copying the objects will be copied on-demand (by Sippy) when they are requested by the application.\n\n### Optimizing your Slurper data migration performance\n\nFor an account, you can run three concurrent Slurper migration jobs at any given time, and each Slurper migration job can process a set amount of requests per second.\n\nTo increase overall throughput and reliability, we recommend splitting your migration into smaller, concurrent jobs using the prefix (or bucket subpath) option.\n\nWhen creating a migration job:\n\n1. Go to the **Source bucket** step.\n2. Under **Define rules**, in **Bucket subpath**, specify subpaths to divide your data by prefix.\n3. Complete the data migration set up.\n\nFor example, suppose your source bucket contains:\n\nYou can create separate jobs with prefixes such as:\n\n* `/photos/2024` to migrate all 2024 files\n* `/photos/202` to migrate all files from 2023 and 2024\n\nEach prefix runs as an independent migration job, allowing Slurper to transfer data in parallel. This improves total transfer speed and ensures that a failure in one job does not interrupt the others.\n\n</page>\n\n<page>\n---\ntitle: Sippy · Cloudflare R2 docs\ndescription: Sippy is a data migration service that allows you to copy data from\n  other cloud providers to R2 as the data is requested, without paying\n  unnecessary cloud egress fees typically associated with moving large amounts\n  of data.\nlastUpdated: 2025-09-03T16:40:54.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-migration/sippy/\n  md: https://developers.cloudflare.com/r2/data-migration/sippy/index.md\n---\n\nSippy is a data migration service that allows you to copy data from other cloud providers to R2 as the data is requested, without paying unnecessary cloud egress fees typically associated with moving large amounts of data.\n\nMigration-specific egress fees are reduced by leveraging requests within the flow of your application where you would already be paying egress fees to simultaneously copy objects to R2.\n\n## How it works\n\nWhen enabled for an R2 bucket, Sippy implements the following migration strategy across [Workers](https://developers.cloudflare.com/r2/api/workers/), [S3 API](https://developers.cloudflare.com/r2/api/s3/), and [public buckets](https://developers.cloudflare.com/r2/buckets/public-buckets/):\n\n* When an object is requested, it is served from your R2 bucket if it is found.\n* If the object is not found in R2, the object will simultaneously be returned from your source storage bucket and copied to R2.\n* All other operations, including put and delete, continue to work as usual.\n\n## When is Sippy useful?\n\nUsing Sippy as part of your migration strategy can be a good choice when:\n\n* You want to start migrating your data, but you want to avoid paying upfront egress fees to facilitate the migration of your data all at once.\n* You want to experiment by serving frequently accessed objects from R2 to eliminate egress fees, without investing time in data migration.\n* You have frequently changing data and are looking to conduct a migration while avoiding downtime. Sippy can be used to serve requests while [Super Slurper](https://developers.cloudflare.com/r2/data-migration/super-slurper/) can be used to migrate your remaining data.\n\nIf you are looking to migrate all of your data from an existing cloud provider to R2 at one time, we recommend using [Super Slurper](https://developers.cloudflare.com/r2/data-migration/super-slurper/).\n\n## Get started with Sippy\n\nBefore getting started, you will need:\n\n* An existing R2 bucket. If you don't already have one, refer to [Create buckets](https://developers.cloudflare.com/r2/buckets/create-buckets/).\n* [API credentials](https://developers.cloudflare.com/r2/data-migration/sippy/#create-credentials-for-storage-providers) for your source object storage bucket.\n* (Wrangler only) Cloudflare R2 Access Key ID and Secret Access Key with read and write permissions. For more information, refer to [Authentication](https://developers.cloudflare.com/r2/api/tokens/).\n\n### Enable Sippy via the Dashboard\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Select the bucket you'd like to migrate objects to.\n\n3. Switch to the **Settings** tab, then scroll down to the **On Demand Migration** card.\n\n4. Select **Enable** and enter details for the AWS / GCS bucket you'd like to migrate objects from. The credentials you enter must have permissions to read from this bucket. Cloudflare also recommends scoping your credentials to only allow reads from this bucket.\n\n5. Select **Enable**.\n\n### Enable Sippy via Wrangler\n\n#### Set up Wrangler\n\nTo begin, install [`npm`](https://docs.npmjs.com/getting-started). Then [install Wrangler, the Developer Platform CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/).\n\n#### Enable Sippy on your R2 bucket\n\nLog in to Wrangler with the [`wrangler login` command](https://developers.cloudflare.com/workers/wrangler/commands/#login). Then run the [`r2 bucket sippy enable` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-sippy-enable):",
      "language": "unknown"
    },
    {
      "code": "This will prompt you to select between supported object storage providers and lead you through setup.\n\n### Enable Sippy via API\n\nFor information on required parameters and examples of how to enable Sippy, refer to the [API documentation](https://developers.cloudflare.com/api/resources/r2/subresources/buckets/subresources/sippy/methods/update/). For information about getting started with the Cloudflare API, refer to [Make API calls](https://developers.cloudflare.com/fundamentals/api/how-to/make-api-calls/).\n\nNote\n\nIf your bucket is setup with [jurisdictional restrictions](https://developers.cloudflare.com/r2/reference/data-location/#jurisdictional-restrictions), you will need to pass a `cf-r2-jurisdiction` request header with that jurisdiction. For example, `cf-r2-jurisdiction: eu`.\n\n### View migration metrics\n\nWhen enabled, Sippy exposes metrics that help you understand the progress of your ongoing migrations.\n\n| Metric | Description |\n| - | - |\n| Requests served by Sippy | The percentage of overall requests served by R2 over a period of time. A higher percentage indicates that fewer requests need to be made to the source bucket. |\n| Data migrated by Sippy | The amount of data that has been copied from the source bucket to R2 over a period of time. Reported in bytes. |\n\nTo view current and historical metrics:\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Select your bucket.\n\n3. Select the **Metrics** tab.\n\nYou can optionally select a time window to query. This defaults to the last 24 hours.\n\n## Disable Sippy on your R2 bucket\n\n### Dashboard\n\n1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n   [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n2. Select the bucket you'd like to disable Sippy for.\n\n3. Switch to the **Settings** tab and scroll down to the **On Demand Migration** card.\n\n4. Press **Disable**.\n\n### Wrangler\n\nTo disable Sippy, run the [`r2 bucket sippy disable` command](https://developers.cloudflare.com/workers/wrangler/commands/#r2-bucket-sippy-disable):",
      "language": "unknown"
    },
    {
      "code": "### API\n\nFor more information on required parameters and examples of how to disable Sippy, refer to the [API documentation](https://developers.cloudflare.com/api/resources/r2/subresources/buckets/subresources/sippy/methods/delete/).\n\n## Supported cloud storage providers\n\nCloudflare currently supports copying data from the following cloud object storage providers to R2:\n\n* Amazon S3\n* Google Cloud Storage (GCS)\n\n## R2 API interactions\n\nWhen Sippy is enabled, it changes the behavior of certain actions on your R2 bucket across [Workers](https://developers.cloudflare.com/r2/api/workers/), [S3 API](https://developers.cloudflare.com/r2/api/s3/), and [public buckets](https://developers.cloudflare.com/r2/buckets/public-buckets/).\n\n| Action | New behavior |\n| - | - |\n| GetObject | Calls to GetObject will first attempt to retrieve the object from your R2 bucket. If the object is not present, the object will be served from the source storage bucket and simultaneously uploaded to the requested R2 bucket. Additional considerations:- Modifications to objects in the source bucket will not be reflected in R2 after the initial copy. Once an object is stored in R2, it will not be re-retrieved and updated.\n- Only user-defined metadata that is prefixed by `x-amz-meta-` in the HTTP response will be migrated. Remaining metadata will be omitted.\n- For larger objects (greater than 199 MiB), multiple GET requests may be required to fully copy the object to R2.\n- If there are multiple simultaneous GET requests for an object which has not yet been fully copied to R2, Sippy may fetch the object from the source storage bucket multiple times to serve those requests. |\n| HeadObject | Behaves similarly to GetObject, but only retrieves object metadata. Will not copy objects to the requested R2 bucket. |\n| PutObject | No change to behavior. Calls to PutObject will add objects to the requested R2 bucket. |\n| DeleteObject | No change to behavior. Calls to DeleteObject will delete objects in the requested R2 bucket. Additional considerations:- If deletes to objects in R2 are not also made in the source storage bucket, subsequent GetObject requests will result in objects being retrieved from the source bucket and copied to R2. |\n\nActions not listed above have no change in behavior. For more information, refer to [Workers API reference](https://developers.cloudflare.com/r2/api/workers/workers-api-reference/) or [S3 API compatibility](https://developers.cloudflare.com/r2/api/s3/api/).\n\n## Create credentials for storage providers\n\n### Amazon S3\n\nTo copy objects from Amazon S3, Sippy requires access permissions to your bucket. While you can use any AWS Identity and Access Management (IAM) user credentials with the correct permissions, Cloudflare recommends you create a user with a narrow set of permissions.\n\nTo create credentials with the correct permissions:\n\n1. Log in to your AWS IAM account.\n\n2. Create a policy with the following format and replace `<BUCKET_NAME>` with the bucket you want to grant access to:",
      "language": "unknown"
    },
    {
      "code": "3. Create a new user and attach the created policy to that user.\n\nYou can now use both the Access Key ID and Secret Access Key when enabling Sippy.\n\n### Google Cloud Storage\n\nTo copy objects from Google Cloud Storage (GCS), Sippy requires access permissions to your bucket. Cloudflare recommends using the Google Cloud predefined `Storage Object Viewer` role.\n\nTo create credentials with the correct permissions:\n\n1. Log in to your Google Cloud console.\n2. Go to **IAM & Admin** > **Service Accounts**.\n3. Create a service account with the predefined `Storage Object Viewer` role.\n4. Go to the **Keys** tab of the service account you created.\n5. Select **Add Key** > **Create a new key** and download the JSON key file.\n\nYou can now use this JSON key file when enabling Sippy via Wrangler or API.\n\n## Caveats\n\n### ETags\n\nWhile R2's ETag generation is compatible with S3's during the regular course of operations, ETags are not guaranteed to be equal when an object is migrated using Sippy. Sippy makes autonomous decisions about the operations it uses when migrating objects to optimize for performance and network usage. It may choose to migrate an object in multiple parts, which affects [ETag calculation](https://developers.cloudflare.com/r2/objects/multipart-objects#etags).\n\nFor example, a 320 MiB object originally uploaded to S3 using a single `PutObject` operation might be migrated to R2 via multipart operations. In this case, its ETag on R2 will not be the same as its ETag on S3. Similarly, an object originally uploaded to S3 using multipart operations might also have a different ETag on R2 if the part sizes Sippy chooses for its migration differ from the part sizes this object was originally uploaded with.\n\nRelying on matching ETags before and after the migration is therefore discouraged.\n\n</page>\n\n<page>\n---\ntitle: Super Slurper · Cloudflare R2 docs\ndescription: Super Slurper allows you to quickly and easily copy objects from\n  other cloud providers to an R2 bucket of your choice.\nlastUpdated: 2025-09-03T16:40:54.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/data-migration/super-slurper/\n  md: https://developers.cloudflare.com/r2/data-migration/super-slurper/index.md\n---\n\nSuper Slurper allows you to quickly and easily copy objects from other cloud providers to an R2 bucket of your choice.\n\nMigration jobs:\n\n* Preserve custom object metadata from source bucket by copying them on the migrated objects on R2.\n* Do not delete any objects from source bucket.\n* Use TLS encryption over HTTPS connections for safe and private object transfers.\n\n## When to use Super Slurper\n\nUsing Super Slurper as part of your strategy can be a good choice if the cloud storage bucket you are migrating consists primarily of objects less than 1 TB. Objects greater than 1 TB will be skipped and need to be copied separately.\n\nFor migration use cases that do not meet the above criteria, we recommend using tools such as [rclone](https://developers.cloudflare.com/r2/examples/rclone/).\n\n## Use Super Slurper to migrate data to R2\n\n1. In the Cloudflare dashboard, go to the **R2 data migration** page.\n\n   [Go to **Data migration**](https://dash.cloudflare.com/?to=/:account/r2/slurper)\n\n2. Select **Migrate files**.\n\n3. Select the source cloud storage provider that you will be migrating data from.\n\n4. Enter your source bucket name and associated credentials and select **Next**.\n\n5. Enter your R2 bucket name and associated credentials and select **Next**.\n\n6. After you finish reviewing the details of your migration, select **Migrate files**.\n\nYou can view the status of your migration job at any time by selecting your migration from **Data Migration** page.\n\n### Source bucket options\n\n#### Bucket sub path (optional)\n\nThis setting specifies the prefix within the source bucket where objects will be copied from.\n\n### Destination R2 bucket options\n\n#### Overwrite files?\n\nThis setting determines what happens when an object being copied from the source storage bucket matches the path of an existing object in the destination R2 bucket. There are two options:\n\n* Overwrite (default)\n* Skip\n\n## Supported cloud storage providers\n\nCloudflare currently supports copying data from the following cloud object storage providers to R2:\n\n* Amazon S3\n* Cloudflare R2\n* Google Cloud Storage (GCS)\n* All S3-compatible storage providers\n\n### Tested S3-compatible storage providers\n\nThe following S3-compatible storage providers have been tested and verified to work with Super Slurper:\n\n* Backblaze B2\n* DigitalOcean Spaces\n* Scaleway Object Storage\n* Wasabi Cloud Object Storage\n\nSuper Slurper should support transfers from all S3-compatible storage providers, but the ones listed have been explicitly tested.\n\nNote\n\nHave you tested and verified another S3-compatible provider? [Open a pull request](https://github.com/cloudflare/cloudflare-docs/edit/production/src/content/docs/r2/data-migration/super-slurper.mdx) or [create a GitHub issue](https://github.com/cloudflare/cloudflare-docs/issues/new).\n\n## Create credentials for storage providers\n\n### Amazon S3\n\nTo copy objects from Amazon S3, Super Slurper requires access permissions to your S3 bucket. While you can use any AWS Identity and Access Management (IAM) user credentials with the correct permissions, Cloudflare recommends you create a user with a narrow set of permissions.\n\nTo create credentials with the correct permissions:\n\n1. Log in to your AWS IAM account.\n2. Create a policy with the following format and replace `<BUCKET_NAME>` with the bucket you want to grant access to:",
      "language": "unknown"
    },
    {
      "code": "1. Create a new user and attach the created policy to that user.\n\nYou can now use both the Access Key ID and Secret Access Key when defining your source bucket.\n\n### Google Cloud Storage\n\nTo copy objects from Google Cloud Storage (GCS), Super Slurper requires access permissions to your GCS bucket. You can use the Google Cloud predefined `Storage Admin` role, but Cloudflare recommends creating a custom role with a narrower set of permissions.\n\nTo create a custom role with the necessary permissions:\n\n1. Log in to your Google Cloud console.\n2. Go to **IAM & Admin** > **Roles**.\n3. Find the `Storage Object Viewer` role and select **Create role from this role**.\n4. Give your new role a name.\n5. Select **Add permissions** and add the `storage.buckets.get` permission.\n6. Select **Create**.\n\nTo create credentials with your custom role:\n\n1. Log in to your Google Cloud console.\n2. Go to **IAM & Admin** > **Service Accounts**.\n3. Create a service account with the your custom role.\n4. Go to the **Keys** tab of the service account you created.\n5. Select **Add Key** > **Create a new key** and download the JSON key file.\n\nYou can now use this JSON key file when enabling Super Slurper.\n\n## Caveats\n\n### ETags\n\nWhile R2's ETag generation is compatible with S3's during the regular course of operations, ETags are not guaranteed to be equal when an object is migrated using Super Slurper. Super Slurper makes autonomous decisions about the operations it uses when migrating objects to optimize for performance and network usage. It may choose to migrate an object in multiple parts, which affects [ETag calculation](https://developers.cloudflare.com/r2/objects/multipart-objects#etags).\n\nFor example, a 320 MiB object originally uploaded to S3 using a single `PutObject` operation might be migrated to R2 via multipart operations. In this case, its ETag on R2 will not be the same as its ETag on S3. Similarly, an object originally uploaded to S3 using multipart operations might also have a different ETag on R2 if the part sizes Super Slurper chooses for its migration differ from the part sizes this object was originally uploaded with.\n\nRelying on matching ETags before and after the migration is therefore discouraged.\n\n### Archive storage classes\n\nObjects stored using AWS S3 [archival storage classes](https://aws.amazon.com/s3/storage-classes/#Archive) will be skipped and need to be copied separately. Specifically:\n\n* Files stored using S3 Glacier tiers (not including Glacier Instant Retrieval) will be skipped and logged in the migration log.\n* Files stored using S3 Intelligent Tiering and placed in Deep Archive tier will be skipped and logged in the migration log.\n\n</page>\n\n<page>\n---\ntitle: Authenticate against R2 API using auth tokens · Cloudflare R2 docs\ndescription: The following example shows how to authenticate against R2 using\n  the S3 API and an API token.\nlastUpdated: 2025-09-24T08:37:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens/\n  md: https://developers.cloudflare.com/r2/examples/authenticate-r2-auth-tokens/index.md\n---\n\nThe following example shows how to authenticate against R2 using the S3 API and an API token.\n\nNote\n\nFor providing secure access to bucket objects for anonymous users, we recommend using [pre-signed URLs](https://developers.cloudflare.com/r2/api/s3/presigned-urls/) instead.\n\nPre-signed URLs do not require users to be a member of your organization and enable direct programmatic access to R2.\n\nEnsure you have set the following environment variables prior to running either example. Refer to [Authentication](https://developers.cloudflare.com/r2/api/tokens/) for more information.",
      "language": "unknown"
    },
    {
      "code": "* JavaScript\n\n  Install the `@aws-sdk/client-s3` package for the S3 API:\n\n  * npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Run the following Node.js script with `node index.js`. Ensure you change `Bucket` to the name of your bucket, and `Key` to point to an existing file in your R2 bucket.\n\n  Note, tutorial below should function for TypeScript as well.",
      "language": "unknown"
    },
    {
      "code": "* Python\n\n  Install the `boto3` S3 API client:",
      "language": "unknown"
    },
    {
      "code": "Run the following Python script with `python3 get_r2_object.py`. Ensure you change `bucket` to the name of your bucket, and `object_key` to point to an existing file in your R2 bucket.",
      "language": "unknown"
    },
    {
      "code": "* Go\n\n  Use `go get` to add the `aws-sdk-go-v2` packages to your Go project:",
      "language": "unknown"
    },
    {
      "code": "Run the following Go application as a script with `go run main.go`. Ensure you change `bucket` to the name of your bucket, and `objectKey` to point to an existing file in your R2 bucket.",
      "language": "unknown"
    },
    {
      "code": "* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: S3 SDKs · Cloudflare R2 docs\nlastUpdated: 2024-09-29T02:09:56.000Z\nchatbotDeprioritize: true\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/aws/\n  md: https://developers.cloudflare.com/r2/examples/aws/index.md\n---\n\n* [aws CLI](https://developers.cloudflare.com/r2/examples/aws/aws-cli/)\n* [aws-sdk-go](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-go/)\n* [aws-sdk-java](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-java/)\n* [aws-sdk-js](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-js/)\n* [aws-sdk-js-v3](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-js-v3/)\n* [aws-sdk-net](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-net/)\n* [aws-sdk-php](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-php/)\n* [aws-sdk-ruby](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-ruby/)\n* [aws-sdk-rust](https://developers.cloudflare.com/r2/examples/aws/aws-sdk-rust/)\n* [aws4fetch](https://developers.cloudflare.com/r2/examples/aws/aws4fetch/)\n* [boto3](https://developers.cloudflare.com/r2/examples/aws/boto3/)\n* [Configure custom headers](https://developers.cloudflare.com/r2/examples/aws/custom-header/)\n\n</page>\n\n<page>\n---\ntitle: Use the Cache API · Cloudflare R2 docs\ndescription: Use the Cache API to store R2 objects in Cloudflare's cache.\nlastUpdated: 2024-08-21T16:27:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/cache-api/\n  md: https://developers.cloudflare.com/r2/examples/cache-api/index.md\n---\n\nUse the [Cache API](https://developers.cloudflare.com/workers/runtime-apis/cache/) to store R2 objects in Cloudflare's cache.\n\nNote\n\nYou will need to [connect a custom domain](https://developers.cloudflare.com/workers/configuration/routing/custom-domains/) or [route](https://developers.cloudflare.com/workers/configuration/routing/routes/) to your Worker in order to use the Cache API. Cache API operations in the Cloudflare Workers dashboard editor, Playground previews, and any `*.workers.dev` deployments will have no impact.",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Multi-cloud setup · Cloudflare R2 docs\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/multi-cloud/\n  md: https://developers.cloudflare.com/r2/examples/multi-cloud/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Rclone · Cloudflare R2 docs\ndescription: You must generate an Access Key before getting started. All\n  examples will utilize access_key_id and access_key_secret variables which\n  represent the Access Key ID and Secret Access Key values you generated.\nlastUpdated: 2025-08-20T18:25:25.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/examples/rclone/\n  md: https://developers.cloudflare.com/r2/examples/rclone/index.md\n---\n\nYou must [generate an Access Key](https://developers.cloudflare.com/r2/api/tokens/) before getting started. All examples will utilize `access_key_id` and `access_key_secret` variables which represent the **Access Key ID** and **Secret Access Key** values you generated.\n\n\n\nRclone is a command-line tool which manages files on cloud storage. You can use rclone to upload objects to R2 concurrently.\n\n## Configure rclone\n\nWith [`rclone`](https://rclone.org/install/) installed, you may run [`rclone config`](https://rclone.org/s3/) to configure a new S3 storage provider. You will be prompted with a series of questions for the new provider details.\n\nRecommendation\n\nIt is recommended that you choose a unique provider name and then rely on all default answers to the prompts.\n\nThis will create a `rclone` configuration file, which you can then modify with the preset configuration given below.\n\n1. Create new remote by selecting `n`.\n2. Select a name for the new remote. For example, use `r2`.\n3. Select the `Amazon S3 Compliant Storage Providers` storage type.\n4. Select `Cloudflare R2 storage` for the provider.\n5. Select whether you would like to enter AWS credentials manually, or get it from the runtime environment.\n6. Enter the AWS Access Key ID.\n7. Enter AWS Secret Access Key (password).\n8. Select the region to connect to (optional).\n9. Select the S3 API endpoint.\n\nNote\n\nEnsure you are running `rclone` v1.59 or greater ([rclone downloads](https://beta.rclone.org/)). Versions prior to v1.59 may return `HTTP 401: Unauthorized` errors, as earlier versions of `rclone` do not strictly align to the S3 specification in all cases.\n\n### Edit an existing rclone configuration\n\nIf you have already configured `rclone` in the past, you may run `rclone config file` to print the location of your `rclone` configuration file:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Choose the right retention policy",
      "id": "choose-the-right-retention-policy"
    },
    {
      "level": "h2",
      "text": "Current limitations",
      "id": "current-limitations"
    },
    {
      "level": "h3",
      "text": "When the source bucket is actively being read from / written to",
      "id": "when-the-source-bucket-is-actively-being-read-from-/-written-to"
    },
    {
      "level": "h3",
      "text": "When the source bucket is not being read often",
      "id": "when-the-source-bucket-is-not-being-read-often"
    },
    {
      "level": "h3",
      "text": "Optimizing your Slurper data migration performance",
      "id": "optimizing-your-slurper-data-migration-performance"
    },
    {
      "level": "h2",
      "text": "How it works",
      "id": "how-it-works"
    },
    {
      "level": "h2",
      "text": "When is Sippy useful?",
      "id": "when-is-sippy-useful?"
    },
    {
      "level": "h2",
      "text": "Get started with Sippy",
      "id": "get-started-with-sippy"
    },
    {
      "level": "h3",
      "text": "Enable Sippy via the Dashboard",
      "id": "enable-sippy-via-the-dashboard"
    },
    {
      "level": "h3",
      "text": "Enable Sippy via Wrangler",
      "id": "enable-sippy-via-wrangler"
    },
    {
      "level": "h3",
      "text": "Enable Sippy via API",
      "id": "enable-sippy-via-api"
    },
    {
      "level": "h3",
      "text": "View migration metrics",
      "id": "view-migration-metrics"
    },
    {
      "level": "h2",
      "text": "Disable Sippy on your R2 bucket",
      "id": "disable-sippy-on-your-r2-bucket"
    },
    {
      "level": "h3",
      "text": "Dashboard",
      "id": "dashboard"
    },
    {
      "level": "h3",
      "text": "Wrangler",
      "id": "wrangler"
    },
    {
      "level": "h3",
      "text": "API",
      "id": "api"
    },
    {
      "level": "h2",
      "text": "Supported cloud storage providers",
      "id": "supported-cloud-storage-providers"
    },
    {
      "level": "h2",
      "text": "R2 API interactions",
      "id": "r2-api-interactions"
    },
    {
      "level": "h2",
      "text": "Create credentials for storage providers",
      "id": "create-credentials-for-storage-providers"
    },
    {
      "level": "h3",
      "text": "Amazon S3",
      "id": "amazon-s3"
    },
    {
      "level": "h3",
      "text": "Google Cloud Storage",
      "id": "google-cloud-storage"
    },
    {
      "level": "h2",
      "text": "Caveats",
      "id": "caveats"
    },
    {
      "level": "h3",
      "text": "ETags",
      "id": "etags"
    },
    {
      "level": "h2",
      "text": "When to use Super Slurper",
      "id": "when-to-use-super-slurper"
    },
    {
      "level": "h2",
      "text": "Use Super Slurper to migrate data to R2",
      "id": "use-super-slurper-to-migrate-data-to-r2"
    },
    {
      "level": "h3",
      "text": "Source bucket options",
      "id": "source-bucket-options"
    },
    {
      "level": "h3",
      "text": "Destination R2 bucket options",
      "id": "destination-r2-bucket-options"
    },
    {
      "level": "h2",
      "text": "Supported cloud storage providers",
      "id": "supported-cloud-storage-providers"
    },
    {
      "level": "h3",
      "text": "Tested S3-compatible storage providers",
      "id": "tested-s3-compatible-storage-providers"
    },
    {
      "level": "h2",
      "text": "Create credentials for storage providers",
      "id": "create-credentials-for-storage-providers"
    },
    {
      "level": "h3",
      "text": "Amazon S3",
      "id": "amazon-s3"
    },
    {
      "level": "h3",
      "text": "Google Cloud Storage",
      "id": "google-cloud-storage"
    },
    {
      "level": "h2",
      "text": "Caveats",
      "id": "caveats"
    },
    {
      "level": "h3",
      "text": "ETags",
      "id": "etags"
    },
    {
      "level": "h3",
      "text": "Archive storage classes",
      "id": "archive-storage-classes"
    },
    {
      "level": "h2",
      "text": "Configure rclone",
      "id": "configure-rclone"
    },
    {
      "level": "h3",
      "text": "Edit an existing rclone configuration",
      "id": "edit-an-existing-rclone-configuration"
    }
  ],
  "url": "llms-txt#disable-snapshot-expiration-for-a-catalog",
  "links": []
}