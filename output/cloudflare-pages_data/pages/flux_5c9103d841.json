{
  "title": "flux",
  "content": "Automatic Speech Recognition • Deepgram\n\nFlux is the first conversational speech recognition model built specifically for voice agents.\n\n| Model Info | |\n| - | - |\n| Terms and License | [link](https://deepgram.com/terms) |\n| Partner | Yes |\n| Real-time | Yes |\n| Unit Pricing | $0.0077 per audio minute (websocket) |\n\nStep 1: Create a Worker that establishes a WebSocket connection\n\nStep 2: Deploy your Worker\n\nStep 3: Write a client script to connect to your Worker and send audio\n\n\\* indicates a required field\n\n* `encoding` string required\n\nEncoding of the audio stream. Currently only supports raw signed little-endian 16-bit PCM.\n\n* `sample_rate` string required\n\nSample rate of the audio stream in Hz.\n\n* `eager_eot_threshold` string\n\nEnd-of-turn confidence required to fire an eager end-of-turn event. When set, enables EagerEndOfTurn and TurnResumed events. Valid Values 0.3 - 0.9.\n\n* `eot_threshold` string default 0.7\n\nEnd-of-turn confidence required to finish a turn. Valid Values 0.5 - 0.9.\n\n* `eot_timeout_ms` string default 5000\n\nA turn will be finished when this much time has passed after speech, regardless of EOT confidence.\n\nKeyterm prompting can improve recognition of specialized terminology. Pass multiple keyterm query parameters to boost multiple keyterms.\n\n* `mip_opt_out` string default false\n\nOpts out requests from the Deepgram Model Improvement Program. Refer to Deepgram Docs for pricing impacts before setting this to true. https\\://dpgr.am/deepgram-mip\n\nLabel your requests for the purpose of identification during usage reporting\n\n* `request_id` string\n\nThe unique identifier of the request (uuid)\n\n* `sequence_id` integer min 0\n\nStarts at 0 and increments for each message the server sends to the client.\n\nThe type of event being reported.\n\n* `turn_index` integer min 0\n\nThe index of the current turn\n\n* `audio_window_start` number\n\nStart time in seconds of the audio range that was transcribed\n\n* `audio_window_end` number\n\nEnd time in seconds of the audio range that was transcribed\n\n* `transcript` string\n\nText that was said over the course of the current turn\n\nThe words in the transcript\n\n* `word` string required\n\nThe individual punctuated, properly-cased word from the transcript\n\n* `confidence` number required\n\nConfidence that this word was transcribed correctly\n\n* `end_of_turn_confidence` number\n\nConfidence that no more speech is coming in this turn\n\nThe following schemas are based on JSON Schema\n\n<page>\n---\ntitle: flux-1-schnell · Cloudflare Workers AI docs\ndescription: \"FLUX.1 [schnell] is a 12 billion parameter rectified flow\n  transformer capable of generating images from text descriptions. \"\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/models/flux-1-schnell/\n  md: https://developers.cloudflare.com/workers-ai/models/flux-1-schnell/index.md\n---\n\n![Black Forest Labs logo](https://developers.cloudflare.com/_astro/blackforestlabs.Ccs-Y4-D.svg)",
  "code_samples": [
    {
      "code": "export default {\n  async fetch(request, env, ctx): Promise<Response> {\n    const resp = await env.AI.run(\"@cf/deepgram/flux\", {\n      encoding: \"linear16\",\n      sample_rate: \"16000\"\n    }, {\n      websocket: true\n    });\n    return resp;\n  },\n} satisfies ExportedHandler<Env>;",
      "language": "ts"
    },
    {
      "code": "npx wrangler deploy",
      "language": "sh"
    },
    {
      "code": "const ws = new WebSocket('wss://<your-worker-url.com>');\n\n\nws.onopen = () => {\n  console.log('Connected to WebSocket');\n\n\n  // Generate and send random audio bytes\n  // You can replace this part with a function\n  // that reads from your mic or other audio source\n  const audioData = generateRandomAudio();\n  ws.send(audioData);\n  console.log('Audio data sent');\n};\n\n\nws.onmessage = (event) => {\n  // Transcription will be received here\n  // Add your custom logic to parse the data\n  console.log('Received:', event.data);\n};\n\n\nws.onerror = (error) => {\n  console.error('WebSocket error:', error);\n};\n\n\nws.onclose = () => {\n  console.log('WebSocket closed');\n};\n\n\n// Generate random audio data (1 second of noise at 44.1kHz, mono)\nfunction generateRandomAudio() {\n  const sampleRate = 44100;\n  const duration = 1;\n  const numSamples = sampleRate * duration;\n  const buffer = new ArrayBuffer(numSamples * 2);\n  const view = new Int16Array(buffer);\n\n\n  for (let i = 0; i < numSamples; i++) {\n    view[i] = Math.floor(Math.random() * 65536 - 32768);\n  }\n\n\n  return buffer;\n}",
      "language": "js"
    },
    {
      "code": "{\n      \"type\": \"object\",\n      \"properties\": {\n          \"encoding\": {\n              \"type\": \"string\",\n              \"description\": \"Encoding of the audio stream. Currently only supports raw signed little-endian 16-bit PCM.\",\n              \"enum\": [\n                  \"linear16\"\n              ]\n          },\n          \"sample_rate\": {\n              \"type\": \"string\",\n              \"description\": \"Sample rate of the audio stream in Hz.\",\n              \"pattern\": \"^[0-9]+$\"\n          },\n          \"eager_eot_threshold\": {\n              \"type\": \"string\",\n              \"description\": \"End-of-turn confidence required to fire an eager end-of-turn event. When set, enables EagerEndOfTurn and TurnResumed events. Valid Values 0.3 - 0.9.\"\n          },\n          \"eot_threshold\": {\n              \"type\": \"string\",\n              \"description\": \"End-of-turn confidence required to finish a turn. Valid Values 0.5 - 0.9.\",\n              \"default\": \"0.7\"\n          },\n          \"eot_timeout_ms\": {\n              \"type\": \"string\",\n              \"description\": \"A turn will be finished when this much time has passed after speech, regardless of EOT confidence.\",\n              \"default\": \"5000\",\n              \"pattern\": \"^[0-9]+$\"\n          },\n          \"keyterm\": {\n              \"type\": \"string\",\n              \"description\": \"Keyterm prompting can improve recognition of specialized terminology. Pass multiple keyterm query parameters to boost multiple keyterms.\"\n          },\n          \"mip_opt_out\": {\n              \"type\": \"string\",\n              \"description\": \"Opts out requests from the Deepgram Model Improvement Program. Refer to Deepgram Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip\",\n              \"enum\": [\n                  \"true\",\n                  \"false\"\n              ],\n              \"default\": \"false\"\n          },\n          \"tag\": {\n              \"type\": \"string\",\n              \"description\": \"Label your requests for the purpose of identification during usage reporting\"\n          }\n      },\n      \"required\": [\n          \"sample_rate\",\n          \"encoding\"\n      ]\n  }",
      "language": "json"
    },
    {
      "code": "{\n      \"type\": \"object\",\n      \"description\": \"Output will be returned as websocket messages.\",\n      \"properties\": {\n          \"request_id\": {\n              \"type\": \"string\",\n              \"description\": \"The unique identifier of the request (uuid)\"\n          },\n          \"sequence_id\": {\n              \"type\": \"integer\",\n              \"description\": \"Starts at 0 and increments for each message the server sends to the client.\",\n              \"minimum\": 0\n          },\n          \"event\": {\n              \"type\": \"string\",\n              \"description\": \"The type of event being reported.\",\n              \"enum\": [\n                  \"Update\",\n                  \"StartOfTurn\",\n                  \"EagerEndOfTurn\",\n                  \"TurnResumed\",\n                  \"EndOfTurn\"\n              ]\n          },\n          \"turn_index\": {\n              \"type\": \"integer\",\n              \"description\": \"The index of the current turn\",\n              \"minimum\": 0\n          },\n          \"audio_window_start\": {\n              \"type\": \"number\",\n              \"description\": \"Start time in seconds of the audio range that was transcribed\"\n          },\n          \"audio_window_end\": {\n              \"type\": \"number\",\n              \"description\": \"End time in seconds of the audio range that was transcribed\"\n          },\n          \"transcript\": {\n              \"type\": \"string\",\n              \"description\": \"Text that was said over the course of the current turn\"\n          },\n          \"words\": {\n              \"type\": \"array\",\n              \"description\": \"The words in the transcript\",\n              \"items\": {\n                  \"type\": \"object\",\n                  \"required\": [\n                      \"word\",\n                      \"confidence\"\n                  ],\n                  \"properties\": {\n                      \"word\": {\n                          \"type\": \"string\",\n                          \"description\": \"The individual punctuated, properly-cased word from the transcript\"\n                      },\n                      \"confidence\": {\n                          \"type\": \"number\",\n                          \"description\": \"Confidence that this word was transcribed correctly\"\n                      }\n                  }\n              }\n          },\n          \"end_of_turn_confidence\": {\n              \"type\": \"number\",\n              \"description\": \"Confidence that no more speech is coming in this turn\"\n          }\n      }\n  }",
      "language": "json"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Usage",
      "id": "usage"
    },
    {
      "level": "h2",
      "text": "Parameters",
      "id": "parameters"
    },
    {
      "level": "h3",
      "text": "Input",
      "id": "input"
    },
    {
      "level": "h3",
      "text": "Output",
      "id": "output"
    },
    {
      "level": "h2",
      "text": "API Schemas",
      "id": "api-schemas"
    }
  ],
  "url": "llms-txt#flux",
  "links": []
}