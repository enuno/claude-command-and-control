{
  "title": "Make sure to replace the placeholder with your shared secret",
  "content": "curl -XPOST \"https://YOUR_WORKER.YOUR_ACCOUNT.workers.dev\" --data '{\"messages\": [{\"msg\":\"hello world\"}]}'\nsh\n{\"success\":true}\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"my-worker\",\n    \"queues\": {\n      \"producers\": [\n        {\n          \"queue\": \"my-queue\",\n          \"binding\": \"ERROR_QUEUE\"\n        }\n      ],\n      \"consumers\": [\n        {\n          \"queue\": \"my-queue\",\n          \"max_batch_size\": 100,\n          \"max_batch_timeout\": 30\n        }\n      ]\n    },\n    \"r2_buckets\": [\n      {\n        \"bucket_name\": \"my-bucket\",\n        \"binding\": \"ERROR_BUCKET\"\n      }\n    ]\n  }\n  toml\n  name = \"my-worker\"\n\n[[queues.producers]]\n    queue = \"my-queue\"\n    binding = \"ERROR_QUEUE\"\n\n[[queues.consumers]]\n    queue = \"my-queue\"\n    max_batch_size = 100\n    max_batch_timeout = 30\n\n[[r2_buckets]]\n    bucket_name = \"my-bucket\"\n    binding = \"ERROR_BUCKET\"\n  ts\ntype Environment = {\n  readonly ERROR_QUEUE: Queue<Error>;\n  readonly ERROR_BUCKET: R2Bucket;\n};\n\nexport default {\n  async fetch(req, env): Promise<Response> {\n    try {\n      return doRequest(req);\n    } catch (error) {\n      await env.ERROR_QUEUE.send(error);\n      return new Response(error.message, { status: 500 });\n    }\n  },\n  async queue(batch, env): Promise<void> {\n    let file = '';\n    for (const message of batch.messages) {\n      const error = message.body;\n      file += error.stack || error.message || String(error);\n      file += '\\r\\n';\n    }\n    await env.ERROR_BUCKET.put(`errors/${Date.now()}.log`, file);\n  },\n} satisfies ExportedHandler<Environment, Error>;\n\nfunction doRequest(request: Request): Promise<Response> {\n  if (Math.random() > 0.5) {\n    return new Response('Success!');\n  }\n  throw new Error('Failed!');\n}\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"my-worker\",\n    \"queues\": {\n      \"producers\": [\n        {\n          \"queue\": \"my-queue\",\n          \"binding\": \"YOUR_QUEUE\"\n        }\n      ]\n    },\n    \"durable_objects\": {\n      \"bindings\": [\n        {\n          \"name\": \"YOUR_DO_CLASS\",\n          \"class_name\": \"YourDurableObject\"\n        }\n      ]\n    },\n    \"migrations\": [\n      {\n        \"tag\": \"v1\",\n        \"new_sqlite_classes\": [\n          \"YourDurableObject\"\n        ]\n      }\n    ]\n  }\n  toml\n  name = \"my-worker\"\n\n[[queues.producers]]\n    queue = \"my-queue\"\n    binding = \"YOUR_QUEUE\"\n\n[durable_objects]\n  bindings = [\n    { name = \"YOUR_DO_CLASS\", class_name = \"YourDurableObject\" }\n  ]\n\n[[migrations]]\n  tag = \"v1\"\n  new_sqlite_classes = [\"YourDurableObject\"]\n  ts\ninterface Env {\n  YOUR_QUEUE: Queue;\n  YOUR_DO_CLASS: DurableObjectNamespace;\n}\n\nexport default {\n  async fetch(req, env): Promise<Response> {\n    // Assume each Durable Object is mapped to a userId in a query parameter\n    // In a production application, this will be a userId defined by your application\n    // that you validate (and/or authenticate) first.\n    let url = new URL(req.url)\n    let userIdParam = url.searchParams.get(\"userId\")\n\nif (userIdParam) {\n      // Get a stub that allows you to call that Durable Object\n      let durableObjectStub = env.YOUR_DO_CLASS.getByName(userIdParam);\n\n// Pass the request to that Durable Object and await the response\n      // This invokes the constructor once on your Durable Object class (defined further down)\n      // on the first initialization, and the fetch method on each request.\n      // We pass the original Request to the Durable Object's fetch method\n      let response = await durableObjectStub.fetch(req);\n\n// This would return \"wrote to queue\", but you could return any response.\n      return response;\n    }\n    return new Response(\"userId must be provided\", { status: 400 });\n  },\n} satisfies ExportedHandler<Env>;\n\nexport class YourDurableObject implements DurableObject {\n  constructor(private state: DurableObjectState, private env: Env) {}\n\nasync fetch(req: Request): Promise<Response> {\n    // Error handling elided for brevity.\n    // Publish to your queue\n    await this.env.YOUR_QUEUE.send({\n      id: this.state.id.toString() // Write the ID of the Durable Object to your queue\n      // Write any other properties to your queue\n    });\n\nreturn new Response(\"wrote to queue\")\n  }\ngraphql\nquery QueueBacklog(\n  $accountTag: string!\n  $queueId: string!\n  $datetimeStart: Time!\n  $datetimeEnd: Time!\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountTag }) {\n      queueBacklogAdaptiveGroups(\n        limit: 10000\n        filter: {\n          queueId: $queueId\n          datetime_geq: $datetimeStart\n          datetime_leq: $datetimeEnd\n        }\n      ) {\n        avg {\n          messages\n          bytes\n        }\n      }\n    }\n  }\n}\ngraphql\nquery QueueConcurrencyByHour(\n  $accountTag: string!\n  $queueId: string!\n  $datetimeStart: Time!\n  $datetimeEnd: Time!\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountTag }) {\n      queueConsumerMetricsAdaptiveGroups(\n        limit: 10000\n        filter: {\n          queueId: $queueId\n          datetime_geq: $datetimeStart\n          datetime_leq: $datetimeEnd\n        }\n        orderBy: [datetimeHour_DESC]\n      ) {\n        avg {\n          concurrency\n        }\n        dimensions {\n          datetimeHour\n        }\n      }\n    }\n  }\n}\ngraphql\nquery QueueMessageOperationsByMinute(\n  $accountTag: string!\n  $queueId: string!\n  $datetimeStart: Date!\n  $datetimeEnd: Date!\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountTag }) {\n      queueMessageOperationsAdaptiveGroups(\n        limit: 10000\n        filter: {\n          queueId: $queueId\n          datetime_geq: $datetimeStart\n          datetime_leq: $datetimeEnd\n        }\n        orderBy: [datetimeMinute_DESC]\n      ) {\n        count\n        sum {\n          bytes\n        }\n        dimensions {\n          datetimeMinute\n        }\n      }\n    }\n  }\n}\njsonc\n  {\n    // ...rest of your configuration...\n    \"limits\": {\n      \"cpu_ms\": 300000, // 300,000 milliseconds = 5 minutes\n    },\n    // ...rest of your configuration...\n  }\n  toml\n  [limits]\n  cpu_ms = 300_000\n  txt\n((Number of Messages * 3) - 1,000,000) / 1,000,000  * $0.40\nts\ntype Environment = {\n  readonly MY_FIRST_QUEUE: Queue;\n};\n\nexport default {\n  async fetch(req, env, context): Promise<Response> {\n    let message = {\n      url: req.url,\n      method: req.method,\n      headers: Object.fromEntries(req.headers),\n    };\n\nawait env.MY_FIRST_QUEUE.send(message); // This will throw an exception if the send fails for any reason\n  },\n} satisfies ExportedHandler<Environment>;\nts\ntype Environment = {\n  readonly MY_FIRST_QUEUE: Queue;\n};\n\nexport default {\n  async fetch(req, env): Promise<Response> {\n    let message = {\n      url: req.url,\n      method: req.method,\n      headers: Object.fromEntries(req.headers),\n    };\n    try {\n      await env.MY_FIRST_QUEUE.send(message, { contentType: \"json\" }); // \"json\" is the default\n    } catch (e) {\n      // Catch cases where send fails, including due to a mismatched content type\n      console.log(e)\n      return Response.json({\"msg\": e}, { status: 500 })\n    }\n  },\n} satisfies ExportedHandler<Environment>;\nts\n    try {\n      // This will throw an exception (error) if you write to pass a non-string to the queue, such as a\n      // native JavaScript object or ArrayBuffer.\n      await env.MY_FIRST_QUEUE.send(\"hello there\", { contentType: \"text\" }); // explicitly set 'text'\n    } catch (e) {\n      console.log(e)\n      return Response.json({\"msg\": e}, { status: 500 })\nts\nexport default {\n  async queue(batch: MessageBatch<Error>, env: Environment): Promise<void> {\n    // Do something with messages in the batch\n    // i.e. write to R2 storage, D1 database, or POST to an external API\n    // You can also iterate over each message in the batch by looping over batch.messages\n  },\n};\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"queues\": {\n      \"consumers\": [\n        {\n          \"queue\": \"<your-queue-name>\",\n          \"max_batch_size\": 100,\n          \"max_batch_timeout\": 30\n        }\n      ]\n    }\n  }\n  toml\n  [[queues.consumers]]\n    queue = \"<your-queue-name>\"\n    max_batch_size = 100 # optional\n    max_batch_timeout = 30 # optional\n  ts\nexport default {\n  async queue(batch: MessageBatch<Error>, env: Environment): Promise<void> {\n    // MessageBatch has a `queue` property we can switch on\n    switch (batch.queue) {\n      case 'log-queue':\n        // Write the batch to R2\n        break;\n      case 'debug-queue':\n        // Write the message to the console or to another queue\n        break;\n      case 'email-reset':\n        // Trigger a password reset email via an external API\n        break;\n      default:\n      // Handle messages we haven't mentioned explicitly (write a log, push to a DLQ)\n    }\n  },\n};\nsh\n  npx wrangler queues list\n  sh\n  pnpm wrangler queues list\n  sh\n  yarn wrangler queues list\n  sh\n  npx wrangler queues create [NAME]\n  sh\n  pnpm wrangler queues create [NAME]\n  sh\n  yarn wrangler queues create [NAME]\n  sh\n  npx wrangler queues update [NAME]\n  sh\n  pnpm wrangler queues update [NAME]\n  sh\n  yarn wrangler queues update [NAME]\n  sh\n  npx wrangler queues delete [NAME]\n  sh\n  pnpm wrangler queues delete [NAME]\n  sh\n  yarn wrangler queues delete [NAME]\n  sh\n  npx wrangler queues info [NAME]\n  sh\n  pnpm wrangler queues info [NAME]\n  sh\n  yarn wrangler queues info [NAME]\n  sh\n  npx wrangler queues consumer add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  pnpm wrangler queues consumer add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  yarn wrangler queues consumer add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  npx wrangler queues consumer remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  pnpm wrangler queues consumer remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  yarn wrangler queues consumer remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  npx wrangler queues consumer http add [QUEUE-NAME]\n  sh\n  pnpm wrangler queues consumer http add [QUEUE-NAME]\n  sh\n  yarn wrangler queues consumer http add [QUEUE-NAME]\n  sh\n  npx wrangler queues consumer http remove [QUEUE-NAME]\n  sh\n  pnpm wrangler queues consumer http remove [QUEUE-NAME]\n  sh\n  yarn wrangler queues consumer http remove [QUEUE-NAME]\n  sh\n  npx wrangler queues consumer worker add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  pnpm wrangler queues consumer worker add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  yarn wrangler queues consumer worker add [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  npx wrangler queues consumer worker remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  pnpm wrangler queues consumer worker remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  yarn wrangler queues consumer worker remove [QUEUE-NAME] [SCRIPT-NAME]\n  sh\n  npx wrangler queues pause-delivery [NAME]\n  sh\n  pnpm wrangler queues pause-delivery [NAME]\n  sh\n  yarn wrangler queues pause-delivery [NAME]\n  sh\n  npx wrangler queues resume-delivery [NAME]\n  sh\n  pnpm wrangler queues resume-delivery [NAME]\n  sh\n  yarn wrangler queues resume-delivery [NAME]\n  sh\n  npx wrangler queues purge [NAME]\n  sh\n  pnpm wrangler queues purge [NAME]\n  sh\n  yarn wrangler queues purge [NAME]\n  sh\n  npx wrangler queues subscription create [QUEUE]\n  sh\n  pnpm wrangler queues subscription create [QUEUE]\n  sh\n  yarn wrangler queues subscription create [QUEUE]\n  sh\n  npx wrangler queues subscription list [QUEUE]\n  sh\n  pnpm wrangler queues subscription list [QUEUE]\n  sh\n  yarn wrangler queues subscription list [QUEUE]\n  sh\n  npx wrangler queues subscription get [QUEUE]\n  sh\n  pnpm wrangler queues subscription get [QUEUE]\n  sh\n  yarn wrangler queues subscription get [QUEUE]\n  sh\n  npx wrangler queues subscription delete [QUEUE]\n  sh\n  pnpm wrangler queues subscription delete [QUEUE]\n  sh\n  yarn wrangler queues subscription delete [QUEUE]\n  sh\n  npx wrangler queues subscription update [QUEUE]\n  sh\n  pnpm wrangler queues subscription update [QUEUE]\n  sh\n  yarn wrangler queues subscription update [QUEUE]\n  sh\n  npm create cloudflare@latest -- resend-rate-limit-queue\n  sh\n  yarn create cloudflare resend-rate-limit-queue\n  sh\n  pnpm create cloudflare@latest resend-rate-limit-queue\n  sh\ncd resend-rate-limit-queue\nsh\nnpx wrangler queues create rate-limit-queue\nsh\nCreating queue rate-limit-queue.\nCreated queue rate-limit-queue.\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"queues\": {\n      \"producers\": [\n        {\n          \"binding\": \"EMAIL_QUEUE\",\n          \"queue\": \"rate-limit-queue\"\n        }\n      ],\n      \"consumers\": [\n        {\n          \"queue\": \"rate-limit-queue\",\n          \"max_batch_size\": 2,\n          \"max_batch_timeout\": 10,\n          \"max_retries\": 3\n        }\n      ]\n    }\n  }\n  toml\n  [[queues.producers]]\n  binding = \"EMAIL_QUEUE\"\n  queue = \"rate-limit-queue\"\n\n[[queues.consumers]]\n  queue = \"rate-limit-queue\"\n  max_batch_size = 2\n  max_batch_timeout = 10\n  max_retries = 3\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"resend-rate-limit-queue\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-09-09\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"queues\": {\n      \"producers\": [\n        {\n          \"binding\": \"EMAIL_QUEUE\",\n          \"queue\": \"rate-limit-queue\"\n        }\n      ],\n      \"consumers\": [\n        {\n          \"queue\": \"rate-limit-queue\",\n          \"max_batch_size\": 2,\n          \"max_batch_timeout\": 10,\n          \"max_retries\": 3\n        }\n      ]\n    }\n  }\n  toml\n  #:schema node_modules/wrangler/config-schema.json\n  name = \"resend-rate-limit-queue\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-09-09\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n[[queues.producers]]\n  binding = \"EMAIL_QUEUE\"\n  queue = \"rate-limit-queue\"\n\n[[queues.consumers]]\n  queue = \"rate-limit-queue\"\n  max_batch_size = 2\n  max_batch_timeout = 10\n  max_retries = 3\n  ts\ninterface Env {\n  EMAIL_QUEUE: Queue<any>;\n}\nts\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    try {\n      await env.EMAIL_QUEUE.send(\n        { email: await req.text() },\n        { delaySeconds: 1 },\n      );\n      return new Response(\"Success!\");\n    } catch (e) {\n      return new Response(\"Error!\", { status: 500 });\n    }\n  },\n};\nts\ninterface Message {\n  email: string;\n}\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    try {\n      await env.EMAIL_QUEUE.send(\n        { email: await req.text() },\n        { delaySeconds: 1 },\n      );\n      return new Response(\"Success!\");\n    } catch (e) {\n      return new Response(\"Error!\", { status: 500 });\n    }\n  },\n  async queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n    for (const message of batch.messages) {\n      try {\n        console.log(message.body.email);\n        // After configuring Resend, you can send email\n        message.ack();\n      } catch (e) {\n        console.error(e);\n        message.retry({ delaySeconds: 5 });\n      }\n    }\n  },\n};\nsh\nnpm run dev\nsh\ncurl -X POST -d \"test@example.com\" http://localhost:8787/\nsh\n[wrangler:inf] POST / 200 OK (2ms)\nQueueMessage {\n  attempts: 1,\n  body: { email: 'test@example.com' },\n  timestamp: 2024-09-12T13:48:07.236Z,\n  id: '72a25ff18dd441f5acb6086b9ce87c8c'\n}\ntxt\nRESEND_API_KEY='your-resend-api-key'\nts\ninterface Env {\n  EMAIL_QUEUE: Queue<any>;\n  RESEND_API_KEY: string;\n}\nsh\n  npm i resend\n  sh\n  yarn add resend\n  sh\n  pnpm add resend\n  ts\nimport { Resend } from \"resend\";\n\ninterface Message {\n  email: string;\n}\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    try {\n      await env.EMAIL_QUEUE.send(\n        { email: await req.text() },\n        { delaySeconds: 1 },\n      );\n      return new Response(\"Success!\");\n    } catch (e) {\n      return new Response(\"Error!\", { status: 500 });\n    }\n  },\n  async queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n    // Initialize Resend\n    const resend = new Resend(env.RESEND_API_KEY);\n    for (const message of batch.messages) {\n      try {\n        console.log(message.body.email);\n        // send email\n        const sendEmail = await resend.emails.send({\n          from: \"onboarding@resend.dev\",\n          to: [message.body.email],\n          subject: \"Hello World\",\n          html: \"<strong>Sending an email from Worker!</strong>\",\n        });\n\n// check if the email failed\n        if (sendEmail.error) {\n          console.error(sendEmail.error);\n          message.retry({ delaySeconds: 5 });\n        } else {\n          // if success, ack the message\n          message.ack();\n        }\n        message.ack();\n      } catch (e) {\n        console.error(e);\n        message.retry({ delaySeconds: 5 });\n      }\n    }\n  },\n};\nts\nimport { Resend } from \"resend\";\n\ninterface Message {\n  email: string;\n}\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    try {\n      await env.EMAIL_QUEUE.send(\n        { email: await req.text() },\n        { delaySeconds: 1 },\n      );\n      return new Response(\"Success!\");\n    } catch (e) {\n      return new Response(\"Error!\", { status: 500 });\n    }\n  },\n  async queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n    // Initialize Resend\n    const resend = new Resend(env.RESEND_API_KEY);\n    for (const message of batch.messages) {\n      try {\n        // send email\n        const sendEmail = await resend.emails.send({\n          from: \"onboarding@resend.dev\",\n          to: [message.body.email],\n          subject: \"Hello World\",\n          html: \"<strong>Sending an email from Worker!</strong>\",\n        });\n\n// check if the email failed\n        if (sendEmail.error) {\n          console.error(sendEmail.error);\n          message.retry({ delaySeconds: 5 });\n        } else {\n          // if success, ack the message\n          message.ack();\n        }\n      } catch (e) {\n        console.error(e);\n        message.retry({ delaySeconds: 5 });\n      }\n    }\n  },\n};\nsh\nnpm run dev\nsh\ncurl -X POST -d \"delivered@resend.dev\" http://localhost:8787/\nsh\nnpx wrangler deploy\nsh\nnpx wrangler secret put RESEND_API_KEY\nbash\ncurl -X POST -d \"delivered@resend.dev\" <YOUR_WORKER_URL>\nsh\n  npm create cloudflare@latest -- queues-web-crawler\n  sh\n  yarn create cloudflare queues-web-crawler\n  sh\n  pnpm create cloudflare@latest queues-web-crawler\n  sh\ncd queues-web-crawler\nsh\n  npx wrangler kv namespace create crawler_links\n  sh\n  yarn wrangler kv namespace create crawler_links\n  sh\n  pnpm wrangler kv namespace create crawler_links\n  sh\n  npx wrangler kv namespace create crawler_screenshots\n  sh\n  yarn wrangler kv namespace create crawler_screenshots\n  sh\n  pnpm wrangler kv namespace create crawler_screenshots\n  sh\nðŸŒ€ Creating namespace with title \"web-crawler-crawler-links\"\nâœ¨ Success!\nAdd the following to your configuration file in your kv_namespaces array:\n[[kv_namespaces]]\nbinding = \"crawler_links\"\nid = \"<GENERATED_NAMESPACE_ID>\"\n\nðŸŒ€ Creating namespace with title \"web-crawler-crawler-screenshots\"\nâœ¨ Success!\nAdd the following to your configuration file in your kv_namespaces array:\n[[kv_namespaces]]\nbinding = \"crawler_screenshots\"\nid = \"<GENERATED_NAMESPACE_ID>\"\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"kv_namespaces\": [\n      {\n        \"binding\": \"CRAWLER_SCREENSHOTS_KV\",\n        \"id\": \"<GENERATED_NAMESPACE_ID>\"\n      },\n      {\n        \"binding\": \"CRAWLER_LINKS_KV\",\n        \"id\": \"<GENERATED_NAMESPACE_ID>\"\n      }\n    ]\n  }\n  toml\n  kv_namespaces = [\n    { binding = \"CRAWLER_SCREENSHOTS_KV\", id = \"<GENERATED_NAMESPACE_ID>\" },\n    { binding = \"CRAWLER_LINKS_KV\", id = \"<GENERATED_NAMESPACE_ID>\" }\n  ]\n  sh\n  npm i -D @cloudflare/puppeteer\n  sh\n  yarn add -D @cloudflare/puppeteer\n  sh\n  pnpm add -D @cloudflare/puppeteer\n  sh\n  npm i robots-parser\n  sh\n  yarn add robots-parser\n  sh\n  pnpm add robots-parser\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"browser\": {\n      \"binding\": \"CRAWLER_BROWSER\"\n    }\n  }\n  toml\n  browser = { binding = \"CRAWLER_BROWSER\" }\n  sh\n  npx wrangler queues create queues-web-crawler\n  sh\n  yarn wrangler queues create queues-web-crawler\n  sh\n  pnpm wrangler queues create queues-web-crawler\n  txt\nCreating queue queues-web-crawler.\nCreated queue queues-web-crawler.\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"queues\": {\n      \"consumers\": [\n        {\n          \"queue\": \"queues-web-crawler\",\n          \"max_batch_timeout\": 60\n        }\n      ],\n      \"producers\": [\n        {\n          \"queue\": \"queues-web-crawler\",\n          \"binding\": \"CRAWLER_QUEUE\"\n        }\n      ]\n    }\n  }\n  toml\n  [[queues.consumers]]\n  queue = \"queues-web-crawler\"\n  max_batch_timeout = 60\n\n[[queues.producers]]\n  queue = \"queues-web-crawler\"\n  binding = \"CRAWLER_QUEUE\"\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"web-crawler\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-07-25\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"kv_namespaces\": [\n      {\n        \"binding\": \"CRAWLER_SCREENSHOTS_KV\",\n        \"id\": \"<GENERATED_NAMESPACE_ID>\"\n      },\n      {\n        \"binding\": \"CRAWLER_LINKS_KV\",\n        \"id\": \"<GENERATED_NAMESPACE_ID>\"\n      }\n    ],\n    \"browser\": {\n      \"binding\": \"CRAWLER_BROWSER\"\n    },\n    \"queues\": {\n      \"consumers\": [\n        {\n          \"queue\": \"queues-web-crawler\",\n          \"max_batch_timeout\": 60\n        }\n      ],\n      \"producers\": [\n        {\n          \"queue\": \"queues-web-crawler\",\n          \"binding\": \"CRAWLER_QUEUE\"\n        }\n      ]\n    }\n  }\n  toml\n  #:schema node_modules/wrangler/config-schema.json\n  name = \"web-crawler\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-07-25\"\n  compatibility_flags = [\"nodejs_compat\"]\n\nkv_namespaces = [\n    { binding = \"CRAWLER_SCREENSHOTS_KV\", id = \"<GENERATED_NAMESPACE_ID>\" },\n    { binding = \"CRAWLER_LINKS_KV\", id = \"<GENERATED_NAMESPACE_ID>\" }\n  ]\n\nbrowser = { binding = \"CRAWLER_BROWSER\" }\n\n[[queues.consumers]]\n  queue = \"queues-web-crawler\"\n  max_batch_timeout = 60\n\n[[queues.producers]]\n  queue = \"queues-web-crawler\"\n  binding = \"CRAWLER_QUEUE\"\n  ts\nimport { BrowserWorker } from \"@cloudflare/puppeteer\";\n\nexport interface Env {\n  CRAWLER_QUEUE: Queue<any>;\n  CRAWLER_SCREENSHOTS_KV: KVNamespace;\n  CRAWLER_LINKS_KV: KVNamespace;\n  CRAWLER_BROWSER: BrowserWorker;\n}\nts\ntype Message = {\n  url: string;\n};\n\nexport interface Env {\n  CRAWLER_QUEUE: Queue<Message>;\n  // ... etc.\n}\n\nexport default {\n  async fetch(req, env): Promise<Response> {\n    await env.CRAWLER_QUEUE.send({ url: await req.text() });\n    return new Response(\"Success!\");\n  },\n} satisfies ExportedHandler<Env>;\nts\nimport puppeteer from \"@cloudflare/puppeteer\";\nimport robotsParser from \"robots-parser\";\n\nasync queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n  let browser: puppeteer.Browser | null = null;\n  try {\n    browser = await puppeteer.launch(env.CRAWLER_BROWSER);\n  } catch {\n    batch.retryAll();\n  return;\n  }\n\nfor (const message of batch.messages) {\n    const { url } = message.body;\n\nlet isAllowed = true;\n    try {\n      const robotsTextPath = new URL(url).origin + \"/robots.txt\";\n      const response = await fetch(robotsTextPath);\n\nconst robots = robotsParser(robotsTextPath, await response.text());\n      isAllowed = robots.isAllowed(url) ?? true; // respect robots.txt!\n    } catch {}\n\nif (!isAllowed) {\n      message.ack();\n      continue;\n    }\n\n// TODO: crawl!\n    message.ack();\n  }\n\nawait browser.close();\n},\nts\ntype Result = {\n  numCloudflareLinks: number;\n  screenshot: ArrayBuffer;\n};\n\nconst crawlPage = async (url: string): Promise<Result> => {\n  const page = await (browser as puppeteer.Browser).newPage();\n\nawait page.goto(url, {\n    waitUntil: \"load\",\n  });\n\nconst numCloudflareLinks = await page.$$eval(\"a\", (links) => {\n    links = links.filter((link) => {\n      try {\n        return new URL(link.href).hostname.includes(\"cloudflare.com\");\n      } catch {\n        return false;\n      }\n    });\n    return links.length;\n  });\n\nawait page.setViewport({\n    width: 1920,\n    height: 1080,\n    deviceScaleFactor: 1,\n  });\n\nreturn {\n    numCloudflareLinks,\n    screenshot: ((await page.screenshot({ fullPage: true })) as Buffer).buffer,\n  };\n};\nts\n// const numCloudflareLinks = await page.$$eval(\"a\", (links) => { ...\n\nawait page.$$eval(\"a\", async (links) => {\n  const urls: MessageSendRequest<Message>[] = links.map((link) => {\n    return {\n      body: {\n        url: link.href,\n      },\n    };\n  });\n  try {\n    await env.CRAWLER_QUEUE.sendBatch(urls);\n  } catch {} // do nothing, likely hit subrequest limit\n});\n\n// await page.setViewport({ ...\nts\n// in the `queue` handler:\n// ...\nif (!isAllowed) {\n  message.ack();\n  continue;\n}\n\ntry {\n  const { numCloudflareLinks, screenshot } = await crawlPage(url);\n  const timestamp = new Date().getTime();\n  const resultKey = `${encodeURIComponent(url)}-${timestamp}`;\n  await env.CRAWLER_LINKS_KV.put(resultKey, numCloudflareLinks.toString(), {\n    metadata: { date: timestamp },\n  });\n  await env.CRAWLER_SCREENSHOTS_KV.put(resultKey, screenshot, {\n    metadata: { date: timestamp },\n  });\n  message.ack();\n} catch {\n  message.retry();\n}\n\n// ...\nts\ntype KeyMetadata = {\n  date: number;\n};\n\n// in the `queue` handler:\n// ...\nfor (const message of batch.messages) {\n  const sameUrlCrawls = await env.CRAWLER_LINKS_KV.list({\n    prefix: `${encodeURIComponent(url)}`,\n  });\n\nlet shouldSkip = false;\n  for (const key of sameUrlCrawls.keys) {\n    if (timestamp - (key.metadata as KeyMetadata)?.date < 60 * 60 * 1000) {\n      // if crawled in last hour, skip\n      message.ack();\n      shouldSkip = true;\n      break;\n    }\n  }\n  if (shouldSkip) {\n    continue;\n  }\n\nlet isAllowed = true;\n  // ...\nts\nimport puppeteer, { BrowserWorker } from \"@cloudflare/puppeteer\";\nimport robotsParser from \"robots-parser\";\n\ntype Message = {\n  url: string;\n};\n\nexport interface Env {\n  CRAWLER_QUEUE: Queue<Message>;\n  CRAWLER_SCREENSHOTS_KV: KVNamespace;\n  CRAWLER_LINKS_KV: KVNamespace;\n  CRAWLER_BROWSER: BrowserWorker;\n}\n\ntype Result = {\n  numCloudflareLinks: number;\n  screenshot: ArrayBuffer;\n};\n\ntype KeyMetadata = {\n  date: number;\n};\n\nexport default {\n  async fetch(req: Request, env: Env): Promise<Response> {\n    // util endpoint for testing purposes\n    await env.CRAWLER_QUEUE.send({ url: await req.text() });\n    return new Response(\"Success!\");\n  },\n  async queue(batch: MessageBatch<Message>, env: Env): Promise<void> {\n    const crawlPage = async (url: string): Promise<Result> => {\n      const page = await (browser as puppeteer.Browser).newPage();\n\nawait page.goto(url, {\n        waitUntil: \"load\",\n      });\n\nconst numCloudflareLinks = await page.$$eval(\"a\", (links) => {\n        links = links.filter((link) => {\n          try {\n            return new URL(link.href).hostname.includes(\"cloudflare.com\");\n          } catch {\n            return false;\n          }\n        });\n        return links.length;\n      });\n\n// to crawl recursively - uncomment this!\n      /*await page.$$eval(\"a\", async (links) => {\n        const urls: MessageSendRequest<Message>[] = links.map((link) => {\n          return {\n            body: {\n              url: link.href,\n            },\n          };\n        });\n        try {\n          await env.CRAWLER_QUEUE.sendBatch(urls);\n        } catch {} // do nothing, might've hit subrequest limit\n      });*/\n\nawait page.setViewport({\n        width: 1920,\n        height: 1080,\n        deviceScaleFactor: 1,\n      });\n\nreturn {\n        numCloudflareLinks,\n        screenshot: ((await page.screenshot({ fullPage: true })) as Buffer)\n          .buffer,\n      };\n    };\n\nlet browser: puppeteer.Browser | null = null;\n    try {\n      browser = await puppeteer.launch(env.CRAWLER_BROWSER);\n    } catch {\n      batch.retryAll();\n      return;\n    }\n\nfor (const message of batch.messages) {\n      const { url } = message.body;\n      const timestamp = new Date().getTime();\n      const resultKey = `${encodeURIComponent(url)}-${timestamp}`;\n\nconst sameUrlCrawls = await env.CRAWLER_LINKS_KV.list({\n        prefix: `${encodeURIComponent(url)}`,\n      });\n\nlet shouldSkip = false;\n      for (const key of sameUrlCrawls.keys) {\n        if (timestamp - (key.metadata as KeyMetadata)?.date < 60 * 60 * 1000) {\n          // if crawled in last hour, skip\n          message.ack();\n          shouldSkip = true;\n          break;\n        }\n      }\n      if (shouldSkip) {\n        continue;\n      }\n\nlet isAllowed = true;\n      try {\n        const robotsTextPath = new URL(url).origin + \"/robots.txt\";\n        const response = await fetch(robotsTextPath);\n\nconst robots = robotsParser(robotsTextPath, await response.text());\n        isAllowed = robots.isAllowed(url) ?? true; // respect robots.txt!\n      } catch {}\n\nif (!isAllowed) {\n        message.ack();\n        continue;\n      }\n\ntry {\n        const { numCloudflareLinks, screenshot } = await crawlPage(url);\n        await env.CRAWLER_LINKS_KV.put(\n          resultKey,\n          numCloudflareLinks.toString(),\n          { metadata: { date: timestamp } },\n        );\n        await env.CRAWLER_SCREENSHOTS_KV.put(resultKey, screenshot, {\n          metadata: { date: timestamp },\n        });\n        message.ack();\n      } catch {\n        message.retry();\n      }\n    }\n\nawait browser.close();\n  },\n};\nsh\n  npx wrangler deploy\n  sh\n  yarn wrangler deploy\n  sh\n  pnpm wrangler deploy\n  bash\ncurl <YOUR_WORKER_URL> \\\n  -H \"Content-Type: application/json\" \\\n  -d 'https://developers.cloudflare.com/queues/tutorials/web-crawler-with-browser-rendering/'\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/http/summary/device_type?dateRange=7d&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"summary_0\": {\n      \"desktop\": \"58.223483\",\n      \"mobile\": \"41.725833\",\n      \"other\": \"0.050684\"\n    },\n    \"meta\": {\n      \"dateRange\": {\n        \"startTime\": \"2022-10-26T14:00:00Z\",\n        \"endTime\": \"2022-11-02T14:00:00Z\"\n      },\n      \"normalization\": \"PERCENTAGE\",\n      ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/http/summary/device_type?dateRange=7d&botClass=LIKELY_AUTOMATED&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\npython\nimport io\nimport requests\nimport pandas as pd\n\ncf_api_url = \"https://api.cloudflare.com/client/v4\"\nparams = \"dateRange=7d&format=csv\"\nmy_token = \"xxx\" # TODO replace\nr = requests.get(f\"{cf_api_url}/radar/http/summary/device_type?{params}\",\n                 headers={\"Authorization\": f\"Bearer {my_token}\"})\ndf = pd.read_csv(io.StringIO(r.text))\ndf.plot(kind=\"bar\", stacked=True)\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/netflows/timeseries?name=us_data&dateRange=7d&location=US&name=pt_data&dateRange=7d&location=PT&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"us_data\": {\n      \"timestamps\": [ \"2022-10-26T17:00:00Z\", \"2022-11-02T15:00:00Z\" ],\n      \"values\": [ \"0.871752\", \"1\" ]\n    },\n    \"pt_data\": {\n      \"timestamps\": [ \"2022-10-26T17:00:00Z\", \"2022-11-02T15:00:00Z\" ],\n      \"values\": [ \"0.020457\", \"0.012313\" ]\n    },\n    \"meta\": {\n      \"dateRange\": {\n        \"startTime\": \"2022-10-26T17:00:00Z\",\n        \"endTime\": \"2022-11-02T17:00:00Z\"\n      },\n      \"aggInterval\": \"ONE_HOUR\",\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/netflows/timeseries?name=this_week&dateRange=7d&location=US&name=previous_week&dateRange=7dControl&location=US&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"this_week\": {\n    \"timestamps\": [ \"2022-10-27T13:00:00Z\", \"2022-10-27T14:00:00Z\", \"...\", \"2022-11-03T12:00:00Z\" ],\n    \"values\": [ \"0.794321\", \"1\", \"...\", \"0.718433\"]\n  },\n  \"previous_week\": {\n    \"timestamps\": [ \"2022-10-20T13:00:00Z\", \"2022-10-20T14:00:00Z\", \"...\", \"2022-10-27T12:00:00Z\" ],\n    \"values\": [ \"0.774392\", \"0.835071\", \"...\", \"0.720181\"]\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/netflows/timeseries?name=tonga&dateStart=2022-10-15T02%3A00%3A00Z&dateEnd=2022-10-15T05%3A00%3A00Z&location=TO&name=tonga_outage&dateStart=2022-01-15T02%3A00%3A00Z&dateEnd=2022-01-15T05%3A00%3A00Z&location=TO&format=json&aggInterval=1h\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"tonga\": {\n    \"timestamps\": [\"2022-10-15T02:00:00Z\", \"2022-10-15T03:00:00Z\", \"2022-10-15T04:00:00Z\", \"2022-10-15T05:00:00Z\"],\n    \"values\": [\"1.0\", \"0.832473\", \"0.820083\", \"0.79408\"]\n  },\n  \"tonga_outage\": {\n    \"timestamps\": [\"2022-01-15T02:00:00Z\", \"2022-01-15T03:00:00Z\", \"2022-01-15T04:00:00Z\", \"2022-01-15T05:00:00Z\"],\n    \"values\": [\"0.354105\", \"0.357287\", \"0.181811\", \"0.044198\"]\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/attacks/layer7/timeseries_groups/mitigation_product?aggInterval=1h&dateRange=1d&name=attacks&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"attacks\": {\n      \"timestamps\": [\"2022-11-05T11:00:00Z\", \"..\"],\n      \"ddos\": [\"53.824302\", \"54.305823\",  \"..\"],\n      \"waf\": [\"39.760956\", \"39.31228\",  \"..\"],\n      \"ip_reputation\": [\"5.623487\", \"5.485468\",  \"..\"],\n      \"access_rules\": [\"0.648368\", \"0.676456\",  \"..\"],\n      \"bot_management\": [\"0.139733\", \"0.217155\",  \"..\"],\n      \"api_shield\": [\"0.003154\", \"0.002819\",  \"..\"],\n      \"data_loss_prevention\": [\"0.0\", \"0.0\",  \"..\"]\n    },\n    \"meta\": {\n      \"dateRange\": {\n        \"startTime\": \"2022-11-05T11:00:00Z\",\n        \"endTime\": \"2022-11-06T11:00:00Z\"\n      },\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/attacks/layer7/summary?location=GB&name=attacks_gb&aggInterval=1h&dateRange=1d&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"attacks_gb\": {\n      \"waf\": \"75.012138\",\n      \"ddos\": \"18.539149\",\n      \"ip_reputation\": \"5.721021\",\n      \"access_rules\": \"0.592515\",\n      \"bot_management\": \"0.131998\",\n      \"api_shield\": \"0.003178\",\n      \"data_loss_prevention\": \"0.0\"\n    },\n    \"meta\": {\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/attacks/layer7/top/locations/target?name=attacks_target&limit=5&dateRange=1d&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"attacks_target\": [\n      {\n        \"targetCountryName\": \"Belgium\",\n        \"targetCountryAlpha2\": \"BE\",\n        \"value\": \"18.536740\",\n        \"rank\": 1\n      },\n      {\n        \"targetCountryName\": \"United States\",\n        \"targetCountryAlpha2\": \"US\",\n        \"value\": \"16.116210\",\n        \"rank\": 2\n      },\n      {\n        \"targetCountryName\": \"China\",\n        \"targetCountryAlpha2\": \"CN\",\n        \"value\": \"13.864696\",\n        \"rank\": 3\n      },\n      {\n        \"targetCountryName\": \"India\",\n        \"targetCountryAlpha2\": \"IN\",\n        \"value\": \"4.344139\",\n        \"rank\": 4\n      },\n      {\n        \"targetCountryName\": \"Germany\",\n        \"targetCountryAlpha2\": \"DE\",\n        \"value\": \"4.182777\",\n        \"rank\": 5\n      }\n    ],\n    \"meta\": {\n      \"dateRange\": {\n        \"startTime\": \"2022-11-05T12:00:00Z\",\n        \"endTime\": \"2022-11-06T12:00:00Z\"\n      },\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/attacks/layer7/top/attacks?limit=5&dateRange=1d&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"top_0\": [\n      {\n        \"originCountryName\": \"United States\",\n        \"originCountryAlpha2\": \"US\",\n        \"targetCountryName\": \"United States\",\n        \"targetCountryAlpha2\": \"US\",\n        \"value\": \"3.790724\",\n        \"rank\": 1\n      },\n      {\n        \"originCountryName\": \"United States\",\n        \"originCountryAlpha2\": \"US\",\n        \"targetCountryName\": \"Belgium\",\n        \"targetCountryAlpha2\": \"BE\",\n        \"value\": \"3.602177\",\n        \"rank\": 2\n      },\n      {\n        \"originCountryName\": \"China\",\n        \"originCountryAlpha2\": \"CN\",\n        \"targetCountryName\": \"Netherlands\",\n        \"targetCountryAlpha2\": \"NL\",\n        \"value\": \"3.017341\",\n        \"rank\": 3\n      },\n      {\n        \"originCountryName\": \"China\",\n        \"originCountryAlpha2\": \"CN\",\n        \"targetCountryName\": \"China\",\n        \"targetCountryAlpha2\": \"CN\",\n        \"value\": \"2.472068\",\n        \"rank\": 4\n      },\n      {\n        \"originCountryName\": \"Indonesia\",\n        \"originCountryAlpha2\": \"ID\",\n        \"targetCountryName\": \"China\",\n        \"targetCountryAlpha2\": \"CN\",\n        \"value\": \"2.056729\",\n        \"rank\": 5\n      }\n    ],\n    \"meta\": {\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/bgp/hijacks/events?invlovedAsn=64512&format=json&per_page=10\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"asn_info\": [\n      {\n        \"asn\": 64512,\n        \"org_name\": \"XXXXX\",\n        \"country_code\": \"XX\"\n      },\n      ...\n    ],\n    \"events\": [\n      {\n        \"duration\": 0,\n        \"event_type\": 0,\n        \"hijack_msgs_count\": 1,\n        \"hijacker_asn\": 64512,\n        \"id\": 1234,\n        \"is_stale\": false,\n        \"max_hijack_ts\": \"2023-04-27T14:01:55.952\",\n        \"max_msg_ts\": \"2023-04-27T14:01:55.952\",\n        \"min_hijack_ts\": \"2023-04-27T14:01:55.952\",\n        \"on_going_count\": 1,\n        \"peer_asns\": [\n          8455\n        ],\n        \"peer_ip_count\": 1,\n        \"prefixes\": [\n          \"192.0.2.0/24\"\n        ],\n        \"tags\": [\n          {\n            \"name\": \"irr_new_origin_invalid\",\n            \"score\": 4\n          },\n          {\n            \"name\": \"irr_old_origin_valid\",\n            \"score\": 0\n          },\n          ...\n        ],\n        \"victim_asns\": [\n          64513\n        ],\n        \"confidence_score\": 4\n      },\n    ],\n    \"total_monitors\": 163\n  },\n  ...\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/bgp/hijacks/events?invlovedAsn=64512&format=json&per_page=10&minConfidence=8\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/bgp/leaks/events?invlovedAsn=64512&format=json&per_page=10\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"asn_info\": [\n      {\n        \"asn\": 64512,\n        \"org_name\": \"XXXXXXX\",\n        \"country_code\": \"XX\"\n      },\n      ...\n    ],\n    \"events\": [\n      {\n        \"detected_ts\": \"2023-04-21T23:10:06\",\n        \"finished\": false,\n        \"id\": 1234,\n        \"leak_asn\": 64512,\n        \"leak_count\": 14,\n        \"leak_seg\": [\n          64514,\n          64512,\n          64513\n        ],\n        \"leak_type\": 1,\n        \"max_ts\": \"2023-04-21T23:10:56\",\n        \"min_ts\": \"2023-04-21T23:09:46\",\n        \"origin_count\": 1,\n        \"peer_count\": 13,\n        \"prefix_count\": 1\n      },\n      ...\n    ]\n  },\n  ...\n}\nsh\n  npm create cloudflare@latest -- hijack-alerts\n  sh\n  yarn create cloudflare hijack-alerts\n  sh\n  pnpm create cloudflare@latest hijack-alerts\n  sh\ncd hijack-alerts\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hijack-alerts\",\n    \"main\": \"src/index.js\",\n    \"compatibility_date\": \"2023-04-27\",\n    \"triggers\": {\n      \"crons\": [\n        \"*/5 * * * *\"\n      ]\n    }\n  }\n  toml\n  name = \"hijack-alerts\"\n  main = \"src/index.js\"\n  compatibility_date = \"2023-04-27\"\n\n[triggers]\n  crons = [ \"*/5 * * * *\" ]\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"kv_namespaces\": [\n      {\n        \"binding\": \"HIJACKS_KV\",\n        \"id\": \"KV_ID_FOR_PRODUCTION\",\n        \"preview_id\": \"TEMPORARY_KV_FOR_DEV_ENVIRONMENT\"\n      }\n    ]\n  }\n  toml\n  [[kv_namespaces]]\n  binding = \"HIJACKS_KV\"\n  id = \"KV_ID_FOR_PRODUCTION\"\n  preview_id = \"TEMPORARY_KV_FOR_DEV_ENVIRONMENT\"\n  javascript\nasync function apiFetch(env, paramsStr) {\n  const config = {\n    headers: {\n      Authorization: `Bearer ${env.CF_API_TOKEN}`,\n    },\n  };\n  const res = await fetch(\n    `https://api.cloudflare.com/client/v4/radar/bgp/hijacks/events?${paramsStr}`,\n    config,\n  );\n\nif (!res.ok) {\n    console.log(JSON.stringify(res));\n    return null;\n  }\n  return await res.json();\n}\njavascript\nexport default {\n    async scheduled(controller, env, ctx) {\n    ...\n    }\n}\njavascript\nlet kv_latest_id = parseInt(await env.HIJACKS_KV.get(\"latest_id\"));\nconst first_batch = isNaN(kv_latest_id);\njavascript\nlet new_events = [];\nlet page = 1;\nwhile (true) {\n  // query for events\n  const query_params = `per_page=10&page=${page}&involvedAsn=${env.TARGET_ASN}&sortBy=ID&sortOrder=DESC`;\n  const data = await apiFetch(env, query_params);\n\n// first batch, save KV value only\n  if (first_batch) {\n    await env.HIJACKS_KV.put(\"latest_id\", events[0].id.toString());\n    return;\n  }\n\n// some validation skipped\n  // ...\n\nlet reached_last = false;\n  for (const event of data.result.events) {\n    if (event.id <= kv_latest_id) {\n      // reached the latest events\n      reached_last = true;\n      break;\n    }\n    new_events.push(event);\n  }\n  if (reached_last) {\n    break;\n  }\n  page += 1;\n}\njavascript\n// sort events by increasing ID order\nnew_events.sort((a, b) => a.id - b.id);\nconst kv_latest_id = new_events[new_events.length - 1].id;\n// push new events\nfor (const event of new_events) {\n  await send_alert(env, event);\n}\n// update latest_id KV value\nawait env.HIJACKS_KV.put(\"latest_id\", kv_latest_id.toString());\njavascript\nasync function send_hangout_alert(env, event) {\n  const webhook_url = `${env.WEBHOOK_URL}&threadKey=bgp-hijacks-event-${event.id}`;\n\nconst data = JSON.stringify({\n    text: `Detected BGP hijack event (${event.id}):\nDetected time: *${event.min_hijack_ts} UTC*\nDetected ASN: *${event.hijacker_asn}*\nExpected ASN(s): *${event.victim_asns.join(\" \")}*\nPrefixes: *${event.prefixes.join(\" \")}*\nTags: *${event.tags.map((tag) => tag.name).join(\" \")}*\nPeer Count: *${event.peer_ip_count}*\n`,\n  });\n  await fetch(webhook_url, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json; charset=UTF-8\",\n    },\n    body: data,\n  });\n}\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"send_email\": [\n      {\n        \"type\": \"send_email\",\n        \"name\": \"SEND_EMAIL_BINDING\",\n        \"destination_address\": \"<YOUR_EMAIL>@example.com\"\n      }\n    ]\n  }\n  toml\n  send_email = [\n      {type = \"send_email\", name = \"SEND_EMAIL_BINDING\", destination_address = \"<YOUR_EMAIL>@example.com\"},\n  ]\n  javascript\nasync function send_email_alert(hijacker, prefixes, victims) {\n  const msg = createMimeMessage();\n  msg.setSender({\n    name: \"BGP Hijack Alerter\",\n    addr: \"<YOUR_APP>@<YOUR_APP_DOMAIN>\",\n  });\n  msg.setRecipient(\"<YOUR_EMAIL>@example.com\");\n  msg.setSubject(\"BGP hijack alert\");\n  msg.addMessage({\n    contentType: \"text/plain\",\n    data: `BGP hijack detected:\n    Detected origin: ${hijacker}\n    Expected origins: ${victims.join(\" \")}\n    Prefixes: ${prefixes.join(\" \")}\n    `,\n  });\n\nvar message = new EmailMessage(\n    \"<YOUR_APP>@<YOUR_APP_DOMAIN>\",\n    \"<YOUR_EMAIL>@example.com\",\n    msg.asRaw(),\n  );\n  try {\n    await env.SEND_EMAIL_BINDING.send(message);\n  } catch (e) {\n    return new Response(e.message);\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/dns/top/locations?domain=google.com&dateRange=1d&format=json&limit=2\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"top_0\": [\n      {\n        \"clientCountryAlpha2\": \"US\",\n        \"clientCountryName\": \"United States\",\n        \"value\": \"43.474518\"\n      },\n      {\n        \"clientCountryAlpha2\": \"BR\",\n        \"clientCountryName\": \"Brazil\",\n        \"value\": \"10.772799\"\n      }\n    ],\n    \"meta\": {\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/dns/top/locations?domain=yandex.ru&dateRange=1d&format=json&limit=2\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"top_0\": [\n      {\n        \"clientCountryAlpha2\": \"RU\",\n        \"clientCountryName\": \"Russian Federation\",\n        \"value\": \"73.710495\"\n      },\n      {\n        \"clientCountryAlpha2\": \"DE\",\n        \"clientCountryName\": \"Germany\",\n        \"value\": \"5.518052\"\n      }\n    ],\n    \"meta\": {\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/ranking/top?name=top&limit=5\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"top_0\": [\n      {\n        \"rank\": 1,\n        \"domain\": \"google.com\"\n      },\n      {\n        \"rank\": 2,\n        \"domain\": \"googleapis.com\"\n      },\n      {\n        \"rank\": 3,\n        \"domain\": \"facebook.com\"\n      },\n      {\n        \"rank\": 4,\n        \"domain\": \"gstatic.com\"\n      },\n      {\n        \"rank\": 5,\n        \"domain\": \"apple.com\"\n      }\n    ]\n  },\n  \"meta\": {\n    // ...\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/datasets?limit=10&datasetType=RANKING_BUCKET\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"datasets\": [\n      {\n        \"id\": 213,\n        \"title\": \"Top 1000000 ranking domains\",\n        \"description\": \"Unordered top 1000000 from 2023-01-02 to 2023-01-09\",\n        \"type\": \"RANKING_BUCKET\",\n        \"tags\": [\n          \"GLOBAL\",\n          \"top_1000000\"\n        ],\n        \"meta\": {\n          \"top\": 1000000\n        },\n        \"alias\": \"ranking_top_1000000\"\n      },\n      // ...\n    ]\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/datasets/download\" \\\n--header \"Authorization: Bearer <API_TOKEN>\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n  \"datasetId\": 213\n}'\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"dataset\": {\n      \"url\": \"https://example.com/download\"\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/datasets/ranking_top_1000\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\ncsv\ndomain\n1rx.io\n2mdn.net\n360yield.com\n3lift.com\na-msedge.net\na2z.com\n...\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/http/timeseries/device_type?name=human&botClass=LIKELY_HUMAN&dateRange=1d&name=bot&botClass=LIKELY_AUTOMATED&dateRange=1d&format=json&aggInterval=1h\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"human\": {\n      \"timestamps\": [\"2022-11-03T13:00:00Z\", \"2022-11-03T14:00:00Z\", \"..\"],\n      \"mobile\": [\"52.5532\", \"52.146628\", \"..\"],\n      \"desktop\": [\"47.394791\", \"47.800731\", \"..\"],\n      \"other\": [\"0.052009\", \"0.052642\", \"..\"]\n    },\n    \"bot\": {\n      \"timestamps\": [\"2022-11-03T13:00:00Z\", \"2022-11-03T14:00:00Z\", \"..\"],\n      \"desktop\": [\"83.833892\", \"84.017711\", \"..\"],\n      \"mobile\": [\"16.156748\", \"15.969936\", \"..\"],\n      \"other\": [\"0.00936\", \"0.012353\", \"..\"]\n    },\n    \"meta\": {\n      \"dateRange\": {\n        \"startTime\": \"2022-11-03T13:00:00Z\",\n        \"endTime\": \"2022-11-04T13:00:00Z\"\n      },\n      \"normalization\": \"PERCENTAGE\"\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/http/summary/device_type?name=human&botClass=LIKELY_HUMAN&dateRange=1d&name=bot&botClass=LIKELY_AUTOMATED&dateRange=1d&format=json&aggInterval=1h\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"human\": {\n      \"mobile\": \"54.967243\",\n      \"desktop\": \"44.974006\",\n      \"other\": \"0.058751\"\n    },\n    \"bot\": {\n      \"desktop\": \"83.275452\",\n      \"mobile\": \"16.707455\",\n      \"other\": \"0.017093\"\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/http/summary/ip_version?name=human&botClass=LIKELY_HUMAN&dateRange=1d&name=bot&botClass=LIKELY_AUTOMATED&dateRange=1d&format=json&aggInterval=1h\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"human\": {\n      \"IPv4\": \"76.213647\",\n      \"IPv6\": \"23.786353\"\n    },\n    \"bot\": {\n      \"IPv4\": \"91.492032\",\n      \"IPv6\": \"8.507968\"\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/http/top/locations/ip_version/IPv6?name=ipv6&botClass=LIKELY_HUMAN&dateRange=28d&format=json&limit=5\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"ipv6\": [\n      {\n        \"clientCountryAlpha2\": \"IN\",\n        \"clientCountryName\": \"India\",\n        \"value\": \"50.612747\"\n      },\n      {\n        \"clientCountryAlpha2\": \"MY\",\n        \"clientCountryName\": \"Malaysia\",\n        \"value\": \"46.233654\"\n      },\n      {\n        \"clientCountryAlpha2\": \"UY\",\n        \"clientCountryName\": \"Uruguay\",\n        \"value\": \"39.796762\"\n      },\n      {\n        \"clientCountryAlpha2\": \"LK\",\n        \"clientCountryName\": \"Sri Lanka\",\n        \"value\": \"39.709355\"\n      },\n      {\n        \"clientCountryAlpha2\": \"VN\",\n        \"clientCountryName\": \"Vietnam\",\n        \"value\": \"39.1514\"\n      }\n    ]\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/netflows/timeseries?name=meo_all&product=ALL&dateRange=1d&asn=3243&name=meo_http&product=HTTP&dateRange=1d&asn=3243&format=json&aggInterval=1h\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"AS3243_all\": {\n      \"timestamps\": [\"2022-11-08T14:00:00Z\", \"2022-11-08T15:00:00Z\", \"...\"],\n      \"values\": [\"0.565885\", \"0.586434\", \"...\"]\n    },\n    \"AS3243_http\": {\n      \"timestamps\": [\"2022-11-08T14:00:00Z\", \"2022-11-08T15:00:00Z\", \"...\"],\n      \"values\": [\"0.548564\", \"0.568329\", \"...\"]\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/netflows/timeseries?name=AS174_all&product=ALL&dateRange=1d&asn=174&name=AS174_http&product=HTTP&dateRange=1d&asn=174&format=json&aggInterval=1h\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"AS174_all\": {\n      \"timestamps\": [\"2022-11-08T14:00:00Z\", \"2022-11-08T15:00:00Z\", \"...\"],\n      \"values\": [\"0.917348\", \"1.0\", \"...\"]\n    },\n    \"AS174_http\": {\n      \"timestamps\": [\"2022-11-08T14:00:00Z\", \"2022-11-08T15:00:00Z\", \"...\"],\n      \"values\": [\"0.381777\", \"0.408091\", \"...\"]\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/netflows/timeseries?name=AS174_all&product=ALL&dateRange=1d&asn=174&name=AS174_http&product=HTTP&dateRange=1d&asn=174&name=AS3243_all&product=ALL&dateRange=1d&asn=3243&name=AS3243_http&product=HTTP&dateRange=1d&asn=3243&format=json&aggInterval=1h\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"AS174_all\": {\n      \"timestamps\": [\"2022-11-08T14:00:00Z\", \"2022-11-08T15:00:00Z\", \"...\"],\n      \"values\": [\"0.917348\", \"1.0\", \"...\"]\n    },\n    \"AS174_http\": {\n      \"timestamps\": [\"2022-11-08T14:00:00Z\", \"2022-11-08T15:00:00Z\", \"...\"],\n      \"values\": [\"0.381777\", \"0.408091\", \"...\"]\n    },\n    \"AS3243_all\": {\n      \"timestamps\": [\"2022-11-08T14:00:00Z\", \"2022-11-08T15:00:00Z\", \"...\"],\n      \"values\": [\"0.317136\", \"0.328652\", \"...\"]\n    },\n    \"AS3243_http\": {\n      \"timestamps\": [\"2022-11-08T14:00:00Z\", \"2022-11-08T15:00:00Z\", \"...\"],\n      \"values\": [\"0.307429\", \"0.318505\", \"...\"]\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/attacks/layer3/timeseries_groups?name=global&dateRange=1d&location=&name=singapore&location=SG&dateRange=1d&aggInterval=1h&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"global\": {\n      \"timestamps\": [\"2022-11-06T13:00:00Z\", \"2022-11-06T14:00:00Z\", \"...\"],\n      \"udp\": [\"50.784034\", \"51.055221\", \"...\"],\n      \"tcp\": [\"49.213944\", \"48.943769\", \"...\"],\n      \"icmp\": [\"0.002023\", \"0.001009\", \"...\"],\n      \"gre\": [\"0.0\", \"0.0\", \"0.0\", \"...\"]\n    },\n    \"singapore\": {\n      \"timestamps\": [\"2022-11-06T13:00:00Z\", \"2022-11-06T14:00:00Z\", \"...\"],\n      \"tcp\": [\"79.605287\", \"83.943885\", \"...\"],\n      \"udp\": [\"20.394594\", \"16.056115\", \"...\"],\n      \"icmp\": [\"0.000119\", \"0.0\", \"...\"],\n      \"gre\": [\"0.0\", \"0.0\", \"...\"]\n    },\n    \"meta\": {\n      \"dateRange\": {\n        \"startTime\": \"2022-11-06T13:00:00Z\",\n        \"endTime\": \"2022-11-07T13:00:00Z\"\n      },\n      \"normalization\": \"PERCENTAGE\",\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/attacks/layer3/summary?location=RU&name=attacks_ru&dateRange=1d&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"attacks_ru\": {\n      \"udp\": \"86.682356\",\n      \"tcp\": \"11.928664\",\n      \"gre\": \"1.381015\",\n      \"icmp\": \"0.007965\"\n    },\n    \"meta\": {\n      \"dateRange\": {\n        \"startTime\": \"2022-11-06T15:00:00Z\",\n        \"endTime\": \"2022-11-07T15:00:00Z\"\n      },\n      \"normalization\": \"PERCENTAGE\",\n      // ...\n    }\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/radar/annotations/outages?limit=5&offset=0&dateRange=7d&format=json\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\njson\n{\n  \"success\": true,\n  \"errors\": [],\n  \"result\": {\n    \"annotations\": [\n      {\n        \"dataSource\": \"ALL\",\n        \"description\": null,\n        \"scope\": \"Multiple regions/cities\",\n        \"startDate\": \"2022-10-25T00:00:00Z\",\n        \"endDate\": null,\n        \"locations\": [\"UA\"],\n        \"asns\": [],\n        \"eventType\": \"OUTAGE\",\n        \"linkedUrl\": \"https://www.npr.org/2022/10/22/1130742768/ukraine-power-grid-outages-record-damage\",\n        \"outage\": {\n          \"outageCause\": \"POWER_OUTAGE\",\n          \"outageType\": \"REGIONAL\"\n        }\n      },\n      {\n        \"dataSource\": \"ALL\",\n        \"description\": null,\n        \"scope\": \"Multiple cities in Florida\",\n        \"startDate\": \"2022-09-28T19:00:00Z\",\n        \"endDate\": \"2022-11-02T00:00:00Z\",\n        \"locations\": [\"US\"],\n        \"asns\": [],\n        \"eventType\": \"OUTAGE\",\n        \"linkedUrl\": \"https://x.com/CloudflareRadar/status/1575229448353349632\",\n        \"outage\": {\n          \"outageCause\": \"WEATHER\",\n          \"outageType\": \"REGIONAL\"\n        }\n      }\n    ]\n  }\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/accounts/{account_id}/urlscanner/v2/scan\" \\\n--header \"Authorization: Bearer <API_TOKEN>\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n  \"url\": \"https://www.example.com\"\n}'\njson\n{\n  \"uuid\": \"095be615-a8ad-4c33-8e9c-c7612fbf6c9f\",\n  \"api\": \"https://api.cloudflare.com/client/v4/accounts/<accountId>/urlscanner/v2/result/095be615-a8ad-4c33-8e9c-c7612fbf6c9f\",\n  \"visibility\": \"public\",\n  \"url\": \"https://www.example.com\",\n  \"message\": \"Submission successful\"\n}\njson\n{\n  \"url\": \"https://example.com\",\n  \"screenshotsResolutions\": [\n    \"desktop\", \"mobile\", \"tablet\"\n  ],\n  \"customagent\": \"XXX-my-user-agent\",\n  \"referer\": \"example\",\n  \"customHeaders\": {\n    \"Authorization\": \"xxx-token\"\n  },\n  \"visibility\": \"Unlisted\"\n}\nbash\ncurl 'https://api.cloudflare.com/client/v4/accounts/{account_id}/urlscanner/v2/search?q=page.domain:google.com' \\\n--header \"Authorization: Bearer <API_TOKEN>\"\nbash\ncurl \"https://api.cloudflare.com/client/v4/accounts/{account_id}/urlscanner/v2/search?q=domain:cdnjs.cloudflare.com\" \\\n--header \"Authorization: Bearer <API_TOKEN>\"\nsql\n-- Valid\nSELECT timestamp, user_id, status FROM my_table;\nSELECT * FROM my_table;\n\n-- Invalid\nSELECT user_id AS uid, timestamp AS ts FROM my_table;\nSELECT COUNT(*) FROM events FROM FROM my_table;\nSELECT json_field.property FROM my_table;\nSELECT 1 AS synthetic_column FROM my_table;\nsql\n-- Valid\nSELECT department, COUNT(*) FROM sales GROUP BY department;\nSELECT region, AVG(amount) FROM sales GROUP BY region;\nSELECT category, MIN(price), MAX(price) FROM products GROUP BY category;\nSELECT SUM(quantity) FROM sales GROUP BY department ORDER BY SUM(amount) DESC;\n\n-- Invalid\nSELECT COUNT(*) AS total FROM sales GROUP BY department; -- No aliases\nSELECT COUNT(department) FROM sales; -- Must use COUNT(*)\nSELECT COUNT(DISTINCT region) FROM sales; -- No DISTINCT support\nsql\nSELECT region, COUNT(*) FROM sales GROUP BY region;\nSELECT dept, category, COUNT(*) FROM sales GROUP BY dept, category;\nSELECT region, COUNT(*) FROM sales WHERE status = 'completed' GROUP BY region;\nSELECT dept, COUNT(*) FROM sales GROUP BY dept ORDER BY COUNT(*) DESC LIMIT 10;\nSELECT is_active, SUM(amount) FROM sales GROUP BY is_active;\nSELECT dept, SUM(amount) FROM sales GROUP BY dept ORDER BY SUM(amount) DESC;\nsql\nSELECT region, COUNT(*) FROM sales GROUP BY region HAVING COUNT(*) > 1000;\nSELECT dept, SUM(amount) FROM sales GROUP BY dept HAVING SUM(amount) > 100000; -- HAVING with SUM\nSELECT region, COUNT(*) FROM sales GROUP BY region HAVING COUNT(*) > 100 AND COUNT(*) < 1000;\nsql\n--Valid\nSELECT * FROM http_requests;\n\n--Invalid\nSELECT * FROM table1, table2;\nSELECT * FROM table1 JOIN table2 ON table1.id = table2.id;\nSELECT * FROM (SELECT * FROM events WHERE status = 200);\nsql\n--Valid\nSELECT * FROM events WHERE timestamp BETWEEN '2024-01-01' AND '2024-01-02';\nSELECT * FROM logs WHERE status = 200 AND user_type = 'premium';\nSELECT * FROM requests WHERE (method = 'GET' OR method = 'POST') AND response_time < 1000;\n\n--Invalid\nSELECT * FROM logs WHERE tags[0] = 'error'; -- Array filtering\nSELECT * FROM requests WHERE metadata.user_id = '123'; -- JSON field filtering\nSELECT * FROM events WHERE col_a = col_b; -- Column comparison\nSELECT * FROM logs WHERE response_time + latency > 5000; -- Arithmetic\nsql\n-- Valid\nSELECT * FROM table_name WHERE ... ORDER BY partitionKey;\nSELECT * FROM table_name WHERE ... ORDER BY partitionKey DESC;\nSELECT dept, COUNT(*) FROM table_name GROUP BY dept ORDER BY COUNT(*) DESC;\n\n-- Invalid\nSELECT * FROM table_name GROUP BY dept ORDER BY nonPartitionKey DESC --ORDER BY a non-grouped column\nsql\n-- Valid\nSELECT * FROM events LIMIT 100\nSELECT * FROM logs WHERE ... LIMIT 10000\n\n-- Invalid\nSELECT * FROM events LIMIT 100, 50; -- Pagination\nSELECT * FROM logs LIMIT COUNT(*); / 2 -- Functions\nSELECT * FROM events LIMIT 10 * 10; -- Arithmetic\nsh\n  npx wrangler r2 sql query [WAREHOUSE] [QUERY]\n  sh\n  pnpm wrangler r2 sql query [WAREHOUSE] [QUERY]\n  sh\n  yarn wrangler r2 sql query [WAREHOUSE] [QUERY]\n  bash\nexport WRANGLER_R2_SQL_AUTH_TOKEN= #paste your token here\nbash\nnpx wrangler login\nbash\n  npx wrangler r2 bucket create fraud-pipeline\n  bash\n  npx wrangler r2 bucket catalog enable fraud-pipeline\n  bash\nexport WAREHOUSE= #Paste your warehouse here\nbash\n  npx wrangler r2 bucket catalog compaction enable fraud-pipeline --token $WRANGLER_R2_SQL_AUTH_TOKEN\n  json\n  {\n    \"fields\": [\n      { \"name\": \"transaction_id\", \"type\": \"string\", \"required\": true },\n      { \"name\": \"user_id\", \"type\": \"int64\", \"required\": true },\n      { \"name\": \"amount\", \"type\": \"float64\", \"required\": false },\n      { \"name\": \"transaction_timestamp\", \"type\": \"string\", \"required\": false },\n      { \"name\": \"location\", \"type\": \"string\", \"required\": false },\n      { \"name\": \"merchant_category\", \"type\": \"string\", \"required\": false },\n      { \"name\": \"is_fraud\", \"type\": \"bool\", \"required\": false }\n    ]\n  }\n  bash\n  npx wrangler pipelines streams create raw_events_stream \\\n    --schema-file raw_transactions_schema.json \\\n    --http-enabled true \\\n    --http-auth false\n  bash\n  # The http ingest endpoint from the output (see example below)\n  export STREAM_ENDPOINT= #the http ingest endpoint from the output (see example below)\n  sh\n  ðŸŒ€ Creating stream 'raw_events_stream'...\n  âœ¨ Successfully created stream 'raw_events_stream' with id 'stream_id'.\n\nCreation Summary:\n  General:\n    Name:  raw_events_stream\n\nHTTP Ingest:\n    Enabled:         Yes\n    Authentication:  Yes\n    Endpoint:        https://stream_id.ingest.cloudflare.com\n    CORS Origins:    None\n\nInput Schema:\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚ Field Name            â”‚ Type   â”‚ Unit/Items â”‚ Required â”‚\n  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  â”‚ transaction_id        â”‚ string â”‚            â”‚ Yes      â”‚\n  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  â”‚ user_id               â”‚ int64  â”‚            â”‚ Yes      â”‚\n  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  â”‚ amount                â”‚float64 â”‚            â”‚ No       â”‚\n  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  â”‚ transaction_timestamp â”‚ string â”‚            â”‚ No       â”‚\n  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  â”‚ location              â”‚ string â”‚            â”‚ No       â”‚\n  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  â”‚ merchant_category     â”‚ string â”‚            â”‚ No       â”‚\n  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n  â”‚ is_fraud              â”‚ bool   â”‚            â”‚ No       â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n  bash\n  npx wrangler pipelines sinks create raw_events_sink \\\n    --type \"r2-data-catalog\" \\\n    --bucket \"fraud-pipeline\" \\\n    --roll-interval 30 \\\n    --namespace \"fraud_detection\" \\\n    --table \"transactions\" \\\n    --catalog-token $WRANGLER_R2_SQL_AUTH_TOKEN\n  bash\n  npx wrangler pipelines create raw_events_pipeline \\\n    --sql \"INSERT INTO raw_events_sink SELECT * FROM raw_events_stream\"\n  json\n       {\n         \"fields\": [\n           { \"name\": \"transaction_id\", \"type\": \"string\", \"required\": true },\n           { \"name\": \"user_id\", \"type\": \"int64\", \"required\": true },\n           { \"name\": \"amount\", \"type\": \"float64\", \"required\": false },\n           {\n             \"name\": \"transaction_timestamp\",\n             \"type\": \"string\",\n             \"required\": false\n           },\n           { \"name\": \"location\", \"type\": \"string\", \"required\": false },\n           { \"name\": \"merchant_category\", \"type\": \"string\", \"required\": false },\n           { \"name\": \"is_fraud\", \"type\": \"bool\", \"required\": false }\n         ]\n       }\n       sql\n       INSERT INTO raw_events_sink SELECT * FROM raw_events_stream;\n       python\nimport requests\nimport json\nimport uuid\nimport random\nimport time\nimport os\nfrom datetime import datetime, timezone, timedelta",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "This will issue a HTTP POST request, and if successful, return a HTTP 200 with a `success: true` response body.\n\n* If you receive a HTTP 400, this is because you attempted to send malformed JSON to your queue.\n* If you receive a HTTP 500, this is because the message was not written to your Queue successfully.\n\nYou can use [`wrangler tail`](https://developers.cloudflare.com/workers/observability/logs/real-time-logs/) to debug the output of `console.log`.\n\n</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Queues & R2 Â· Cloudflare Queues docs\ndescription: Example of how to use Queues to batch data and store it in an R2 bucket.\nlastUpdated: 2025-01-29T12:28:42.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/examples/send-errors-to-r2/\n  md: https://developers.cloudflare.com/queues/examples/send-errors-to-r2/index.md\n---\n\nThe following Worker will catch JavaScript errors and send them to a queue. The same Worker will receive those errors in batches and store them to a log file in an R2 bucket.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Sending messages from the dashboard Â· Cloudflare\n  Queues docs\ndescription: Use the dashboard to send messages to a queue.\nlastUpdated: 2025-09-04T11:41:09.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/examples/send-messages-from-dash/\n  md: https://developers.cloudflare.com/queues/examples/send-messages-from-dash/index.md\n---\n\nSending messages from the dashboard allows you to debug Queues or queue consumers without a producer Worker.\n\nTo send messages from the dashboard:\n\n1. In the Cloudflare dashboard, go to the **Queues** page.\n\n   [Go to **Queues**](https://dash.cloudflare.com/?to=/:account/workers/queues)\n\n2. Select the queue to send a message to.\n\n3. Select the **Messages** tab.\n\n4. Select **Send**.\n\n5. Choose your message **Content Type**: *Text* or *JSON*.\n\n6. Enter your message. Alternatively, drag a file over the textbox to upload a file as a message.\n\n7. Select **Send**.\n\nYour message will be sent to the queue.\n\nRefer to the [Get Started guide](https://developers.cloudflare.com/queues/get-started/) to learn how to send messages to a queue from a Worker.\n\n</page>\n\n<page>\n---\ntitle: Serverless ETL pipelines Â· Cloudflare Queues docs\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/examples/serverless-etl/\n  md: https://developers.cloudflare.com/queues/examples/serverless-etl/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Queues - Use Queues and Durable Objects Â· Cloudflare Queues docs\ndescription: Publish to a queue from within a Durable Object.\nlastUpdated: 2025-08-21T12:34:40.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/examples/use-queues-with-durable-objects/\n  md: https://developers.cloudflare.com/queues/examples/use-queues-with-durable-objects/index.md\n---\n\nThe following example shows you how to write a Worker script to publish to [Cloudflare Queues](https://developers.cloudflare.com/queues/) from within a [Durable Object](https://developers.cloudflare.com/durable-objects/).\n\nPrerequisites:\n\n* A [queue created](https://developers.cloudflare.com/queues/get-started/#3-create-a-queue) via the Cloudflare dashboard or the [wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/).\n* A [configured **producer** binding](https://developers.cloudflare.com/queues/configuration/configure-queues/#producer-worker-configuration) in the Cloudflare dashboard or Wrangler file.\n* A [Durable Object namespace binding](https://developers.cloudflare.com/workers/wrangler/configuration/#durable-objects).\n\nConfigure your Wrangler file as follows:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "The following Worker script:\n\n1. Creates a Durable Object stub, or retrieves an existing one based on a userId.\n2. Passes request data to the Durable Object.\n3. Publishes to a queue from within the Durable Object.\n\nThe `constructor()` in the Durable Object makes your `Environment` available (in scope) on `this.env` to the [`fetch()` handler](https://developers.cloudflare.com/durable-objects/best-practices/create-durable-object-stubs-and-send-requests/) in the Durable Object.",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Metrics Â· Cloudflare Queues docs\ndescription: Queues expose metrics which allow you to measure the queue backlog,\n  consumer concurrency, and message operations.\nlastUpdated: 2025-05-14T00:02:06.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/observability/metrics/\n  md: https://developers.cloudflare.com/queues/observability/metrics/index.md\n---\n\nQueues expose metrics which allow you to measure the queue backlog, consumer concurrency, and message operations.\n\nThe metrics displayed in the [Cloudflare dashboard](https://dash.cloudflare.com/) are queried from Cloudflareâ€™s [GraphQL Analytics API](https://developers.cloudflare.com/analytics/graphql-api/). You can access the metrics [programmatically](#query-via-the-graphql-api) via GraphQL or HTTP client.\n\n## Metrics\n\n### Backlog\n\nQueues export the below metrics within the `queuesBacklogAdaptiveGroups` dataset.\n\n| Metric | GraphQL Field Name | Description |\n| - | - | - |\n| Backlog bytes | `bytes` | Average size of the backlog, in bytes |\n| Backlog messages | `messages` | Average size of the backlog, in number of messages |\n\nThe `queuesBacklogAdaptiveGroups` dataset provides the following dimensions for filtering and grouping queries:\n\n* `queueID` - ID of the queue\n* `datetime` - Timestamp for when the message was sent\n* `date` - Timestamp for when the message was sent, truncated to the start of a day\n* `datetimeHour` - Timestamp for when the message was sent, truncated to the start of an hour\n* `datetimeMinute` - Timestamp for when the message was sent, truncated to the start of a minute\n\n### Consumer concurrency\n\nQueues export the below metrics within the `queueConsumerMetricsAdaptiveGroups` dataset.\n\n| Metric | GraphQL Field Name | Description |\n| - | - | - |\n| Avg. Consumer Concurrency | `concurrency` | Average number of concurrent consumers over the period |\n\nThe `queueConsumerMetricsAdaptiveGroups` dataset provides the following dimensions for filtering and grouping queries:\n\n* `queueID` - ID of the queue\n* `datetime` - Timestamp for the consumer metrics\n* `date` - Timestamp for the consumer metrics, truncated to the start of a day\n* `datetimeHour` - Timestamp for the consumer metrics, truncated to the start of an hour\n* `datetimeMinute` - Timestamp for the consumer metrics, truncated to the start of a minute\n\n### Message operations\n\nQueues export the below metrics within the `queueMessageOperationsAdaptiveGroups` dataset.\n\n| Metric | GraphQL Field Name | Description |\n| - | - | - |\n| Total billable operations | `billableOperations` | Sum of billable operations (writes, reads, and deletes) over the time period |\n| Total Bytes | `bytes` | Sum of bytes read, written, and deleted from the queue |\n| Lag | `lagTime` | Average lag time in milliseconds between when the message was written and the operation to consume the message. |\n| Retries | `retryCount` | Average number of retries per message |\n| Message Size | `messageSize` | Maximum message size over the specified period |\n\nThe `queueMessageOperationsAdaptiveGroups` dataset provides the following dimensions for filtering and grouping queries:\n\n* `queueID` - ID of the queue\n* `actionType` - The type of message operation. Can be `WriteMessage`, `ReadMessage` or `DeleteMessage`\n* `consumerType` - The queue consumer type. Can be `worker` or `http`. Only applicable for `ReadMessage` and `DeleteMessage` action types\n* `outcome` - The outcome of the mesage operation. Only applicable for `DeleteMessage` action types. Can be `success`, `dlq` or `fail`.\n* `datetime` - Timestamp for the message operation\n* `date` - Timestamp for the message operation, truncated to the start of a day\n* `datetimeHour` - Timestamp for the message operation, truncated to the start of an hour\n* `datetimeMinute` - Timestamp for the message operation, truncated to the start of a minute\n\n## Example GraphQL Queries\n\n### Get average queue backlog over time period",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAiucAhAhgYwNYBsD2BzACgCgYYASdNHEAOwBcAVFPALhgGc6IBLGvAQhLlQYcAEkAJm048+g0mQko6YOtwC2YAMp0UEOmwYaw88kpVrNAURpSYRzYICUMAN5CAbtzAB3SG6FSSmp6dgIAM24sFQg2Vxhg2kZmNgo0KiSmPBgAXxd3UkKYEWR0bHwAQSUABzUPMABxCGpqsMCimCwNbgMYAEYABiGB9qLI6Mg40Y6SsElU2clpovNVYwB9PDBgVNXLbV19ZcK9jaxt3eU161tjnOn845QPbIKOos12dmYwdmPSABGUBUf3epHu7whhSh9xyQA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBHWAUzaoBMsQAlAUQAKAGXz8KAdSrIAEtTqNOYRK0QBLALasAyojAAnRDwBMABmMBWALQBGY1YDMp5MeOZjATkwOA7AC0GECUVdS1+eG5sM0tbewcbZFMAFncvXwCAXyA)\n\n### Get average consumer concurrency by hour",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAiucBhA9gOwMYghMmoBCUAEitgBQBQMMAJAIYYZloAuAKvQOYBcMAzqwgBLNFwCE1OqDDgAkgBM+gkWMk1aC+qzCthAWzABlVvQis+7A2HV0tOvYYCiaJTCuHJAShgBvKQBuwmAA7pB+UjSMzCBs-OQAZsIANjoQfL4w0Swc3HwMTDmcXDAAvj7+NFUwMsjo-CCGEACyuiIY-ACCWgAOegFgAOIQZD3xkdUwyQbCFjAAjAAMy4sT1UmpkBlrk7Vgivl7ijvV9rrWAPpcYMD5Z47GpuYnVfeXyTd32ufOri+lLxQEAUkCIfAA2m9DKRsBcACJOIxIAC6OwqL3oARKlUm1WYmGwuHw-xeCmsaH4wnqEVxp2+DxhEBJuIB1VZZUopSAA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBHWAUzaoBMsQAlAUQAKAGXz8KAdSrIAEtTqNOYRK0QBLALasAyojAAnRDwBMABmMBWALQBGY1YDMp5MeOZjATkwOA7AC0GECUVdS1+eG5sM0tbewcbZFMAFncvXwCAXyA)\n\n### Get message operations by minute",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAiucBZMBnVBDA5mA8gB0gwBcBLAewDtUAhKJUykYsACgCgYYASDAYz7kQlYgBVsALhipiERlgCEnHqDDgAkgBMpMuZUXLumkmDIBbMAGViGCMSkARE0q5GT5sAFFK2mE5ZKAJQwAN7KAG6kYADukKHKXPyCwsSorABmpAA2LBBSITBJQiLiWFK8AsVi2DAAvsFhXE0wqshomDgERGRUqACCxvhk4WAA4hBC+GkJzTBZpGak9jAAjAAMG2szzZk5kPnbs61gWuXHWofNxiweAPo4wOXXpgtWNnaXTc93WWCPPN9Xt5NJ9ap9yBBNJA6FIANqAiwMJgsW4OTyWADCAF1Dg1PskRJ9UCAzPFZrMAEZQFioUGfTSvagUahk8lfdyvJHMMB08lg5r8ursWpAA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBHWAUzaoBMsQAlAUQAKAGXz8KAdSrIAEtTqNOYRK0QBLALasAyojAAnRDwBMABmMBWALQBGY1YDMphiCUr1W-vG7Yzl2-YONiAAvkA)\n\n</page>\n\n<page>\n---\ntitle: Audit Logs Â· Cloudflare Queues docs\ndescription: Audit logs provide a comprehensive summary of changes made within\n  your Cloudflare account, including those made to Queues. This functionality is\n  always enabled.\nlastUpdated: 2025-09-04T16:11:18.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/audit-logs/\n  md: https://developers.cloudflare.com/queues/platform/audit-logs/index.md\n---\n\n[Audit logs](https://developers.cloudflare.com/fundamentals/account/account-security/review-audit-logs/) provide a comprehensive summary of changes made within your Cloudflare account, including those made to Queues. This functionality is always enabled.\n\n## Viewing audit logs\n\nTo view audit logs for your Queue in the Cloudflare dashboard, go to the **Audit logs** page.\n\n[Go to **Audit logs**](https://dash.cloudflare.com/?to=/:account/audit-log)\n\nFor more information on how to access and use audit logs, refer to [Review audit logs](https://developers.cloudflare.com/fundamentals/account/account-security/review-audit-logs/).\n\n## Logged operations\n\nThe following configuration actions are logged:\n\n| Operation | Description |\n| - | - |\n| CreateQueue | Creation of a new queue. |\n| DeleteQueue | Deletion of an existing queue. |\n| UpdateQueue | Updating the configuration of a queue. |\n| AttachConsumer | Attaching a consumer, including HTTP pull consumers, to the Queue. |\n| RemoveConsumer | Removing a consumer, including HTTP pull consumers, from the Queue. |\n| UpdateConsumerSettings | Changing Queues consumer settings. |\n\n</page>\n\n<page>\n---\ntitle: Changelog Â· Cloudflare Queues docs\ndescription: Subscribe to RSS\nlastUpdated: 2025-02-13T19:35:19.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/changelog/\n  md: https://developers.cloudflare.com/queues/platform/changelog/index.md\n---\n\n[Subscribe to RSS](https://developers.cloudflare.com/queues/platform/changelog/index.xml)\n\n## 2025-04-17\n\n**Improved limits for pull consumers**\n\n[Queues Pull Consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/) can now pull and acknowledge up to 5,000 messages per second per queue. Previously, pull consumers were rate limited to 1200 requests / 5 minutes, aggregated across all queues.\n\nRefer to the [documentation on pull consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/) to learn how to setup a pull consumer, acknowledge / retry messages, and setup multiple consumers.\n\n## 2025-03-27\n\n**Pause delivery and purge queues**\n\nQueues now supports the ability to pause delivery and/or delete messages from a queue, allowing you to better manage queue backlogs.\n\nMessage delivery from a Queue to consumers can be paused / resumed. Queues continue to receive messages while paused.\n\nQueues can be purged to permanently delete all messages currently stored in a Queue. This operation is useful while testing a new application, if a queue producer was misconfigured and is sending bad messages.\n\nRefer to the [documentation on Pause & Purge](https://developers.cloudflare.com/queues/configuration/pause-purge/) to learn how to use both operations.\n\n## 2025-02-14\n\n**Customize message retention period**\n\nYou can now customize a queue's message retention period, from a minimum of 60 seconds to a maximum of 14 days. Previously, it was fixed to the default of 4 days.\n\nRefer to the [Queues confiuguration documentation](https://developers.cloudflare.com/queues/configuration/configure-queues/#queue-configuration) to learn more.\n\n## 2024-09-26\n\n**Queues is GA, with higher throughput & consumer concurrency**\n\nQueues is now generally available.\n\nThe per-queue message throughput has increased from 400 to 5,000 messages per second. This applies to new and existing queues.\n\nMaximum concurrent consumers has increased from 20 to 250. This applies to new and existing queues. Queues with no explicit limit will automatically scale to the new maximum. Review the [consumer concurrency documentation](https://developers.cloudflare.com/queues/configuration/consumer-concurrency) to learn more.\n\n## 2024-03-26\n\n**Delay messages published to a queue**\n\nMessages published to a queue and/or marked for retry from a queue consumer can now be explicitly delayed. Delaying messages allows you to defer tasks until later, and/or respond to backpressure when consuming from a queue.\n\nRefer to [Batching and Retries](https://developers.cloudflare.com/queues/configuration/batching-retries/) to learn how to delay messages written to a queue.\n\n## 2024-03-25\n\n**Support for pull-based consumers**\n\nQueues now supports [pull-based consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/). A pull-based consumer allows you to pull from a queue over HTTP from any environment and/or programming language outside of Cloudflare Workers. A pull-based consumer can be useful when your message consumption rate is limited by upstream infrastructure or long-running tasks.\n\nReview the [documentation on pull-based consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers/) to configure HTTP-based pull.\n\n## 2024-03-18\n\n**Default content type now set to JSON**\n\nThe default [content type](https://developers.cloudflare.com/queues/configuration/javascript-apis/#queuescontenttype) for messages published to a queue is now `json`, which improves compatibility with the upcoming pull-based queues.\n\nAny Workers created on or after the [compatibility date](https://developers.cloudflare.com/workers/configuration/compatibility-flags/#queues-send-messages-in-json-format) of `2024-03-18`, or that explicitly set the `queues_json_messages` compatibility flag, will use the new default behaviour. Existing Workers with a compatibility date prior will continue to use `v8` as the default content type for published messages.\n\n## 2024-02-24\n\n**Explicit retries no longer impact consumer concurrency/scaling.**\n\nCalling `retry()` or `retryAll()` on a message or message batch will no longer have an impact on how Queues scales [consumer concurrency](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/).\n\nPreviously, using [explicit retries](https://developers.cloudflare.com/queues/configuration/batching-retries/#explicit-acknowledgement-and-retries) via `retry()` or `retryAll()` would count as an error and could result in Queues scaling down the number of concurrent consumers.\n\n## 2023-10-07\n\n**More queues per account - up to 10,000**\n\nDevelopers building on Queues can now create up to 10,000 queues per account, enabling easier per-user, per-job and sharding use-cases.\n\nRefer to [Limits](https://developers.cloudflare.com/queues/platform/limits) to learn more about Queues' current limits.\n\n## 2023-10-05\n\n**Higher consumer concurrency limits**\n\n[Queue consumers](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/) can now scale to 20 concurrent invocations (per queue), up from 10. This allows you to scale out and process higher throughput queues more quickly.\n\nQueues with [no explicit limit specified](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/#limit-concurrency) will automatically scale to the new maximum.\n\nThis limit will continue to grow during the Queues beta.\n\n## 2023-03-28\n\n**Consumer concurrency (enabled)**\n\nQueue consumers will now [automatically scale up](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/) based on the number of messages being written to the queue. To control or limit concurrency, you can explicitly define a [`max_concurrency`](https://developers.cloudflare.com/queues/configuration/configure-queues/#consumer) for your consumer.\n\n## 2023-03-15\n\n**Consumer concurrency (upcoming)**\n\nQueue consumers will soon automatically scale up concurrently as a queues' backlog grows in order to keep overall message processing latency down. Concurrency will be enabled on all existing queues by 2023-03-28.\n\n**To opt-out, or to configure a fixed maximum concurrency**, set `max_concurrency = 1` in your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/) or via [the queues dashboard](https://dash.cloudflare.com/?to=/:account/queues).\n\n**To opt-in, you do not need to take any action**: your consumer will begin to scale out as needed to keep up with your message backlog. It will scale back down as the backlog shrinks, and/or if a consumer starts to generate a higher rate of errors. To learn more about how consumers scale, refer to the [consumer concurrency](https://developers.cloudflare.com/queues/configuration/consumer-concurrency/) documentation.\n\n## 2023-03-02\n\n**Explicit acknowledgement (new feature)**\n\nYou can now [acknowledge individual messages with a batch](https://developers.cloudflare.com/queues/configuration/batching-retries/#explicit-acknowledgement-and-retries) by calling `.ack()` on a message.\n\nThis allows you to mark a message as delivered as you process it within a batch, and avoids the entire batch from being redelivered if your consumer throws an error during batch processing. This can be particularly useful when you are calling external APIs, writing messages to a database, or otherwise performing non-idempotent actions on individual messages within a batch.\n\n## 2023-03-01\n\n**Higher per-queue throughput**\n\nThe per-queue throughput limit has now been [raised to 400 messages per second](https://developers.cloudflare.com/queues/platform/limits/).\n\n## 2022-12-13\n\n**sendBatch support**\n\nThe JavaScript API for Queue producers now includes a `sendBatch` method which supports sending up to 100 messages at a time.\n\n## 2022-12-12\n\n**Increased per-account limits**\n\nQueues now allows developers to create up to 100 queues per account, up from the initial beta limit of 10 per account. This limit will continue to increase over time.\n\n</page>\n\n<page>\n---\ntitle: Limits Â· Cloudflare Queues docs\ndescription: 1 1 KB is measured as 1000 bytes. Messages can include up to ~100\n  bytes of internal metadata that counts towards total message limits.\nlastUpdated: 2025-04-16T19:11:49.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/limits/\n  md: https://developers.cloudflare.com/queues/platform/limits/index.md\n---\n\n| Feature | Limit |\n| - | - |\n| Queues | 10,000 per account |\n| Message size | 128 KB 1 |\n| Message retries | 100 |\n| Maximum consumer batch size | 100 messages |\n| Maximum messages per `sendBatch` call | 100 (or 256KB in total) |\n| Maximum Batch wait time | 60 seconds |\n| Per-queue message throughput | 5,000 messages per second 2 |\n| Message retention period 3 | 4 days (default). [Configurable to 14 days](https://developers.cloudflare.com/queues/configuration/configure-queues/#queue-configuration) |\n| Per-queue backlog size 4 | 25GB |\n| Concurrent consumer invocations | 250 push-based only |\n| Consumer duration (wall clock time) | 15 minutes 5 |\n| [Consumer CPU time](https://developers.cloudflare.com/workers/platform/limits/#cpu-time) | 30 seconds (default). [Configurable to 5 minutes](https://developers.cloudflare.com/queues/platform/limits/#increasing-queue-consumer-worker-cpu-limits) |\n| `visibilityTimeout` (pull-based queues) | 12 hours |\n| `delaySeconds` (when sending or retrying) | 12 hours |\n| Requests to the Queues API (excluding pull consumer operations)6 | [1200 requests / 5 mins](https://developers.cloudflare.com/fundamentals/api/reference/limits/) |\n\n1 1 KB is measured as 1000 bytes. Messages can include up to \\~100 bytes of internal metadata that counts towards total message limits.\n\n2 Exceeding the maximum message throughput will cause the `send()` and `sendBatch()` methods to throw an exception with a `Too Many Requests` error until your producer falls below the limit.\n\n3 Messages in a queue that reach the maximum message retention are deleted from the queue. Queues does not delete messages in the same queue that have not reached this limit.\n\n4 Individual queues that reach this limit will receive a `Storage Limit Exceeded` error when calling `send()` or `sendBatch()` on the queue.\n\n5 Refer to [Workers limits](https://developers.cloudflare.com/workers/platform/limits/#cpu-time).\n\n6 [Pull Consumers](https://developers.cloudflare.com/queues/configuration/pull-consumers) allow you to consume messages from a queue over HTTP. Pulls, acknowledgements, and retries over HTTP are not subject to the API rate limit.\n\nNeed a higher limit?\n\nTo request an adjustment to a limit, complete the [Limit Increase Request Form](https://forms.gle/ukpeZVLWLnKeixDu7). If the limit can be increased, Cloudflare will contact you with next steps.\n\n### Increasing Queue Consumer Worker CPU Limits\n\n[Queue consumer Workers](https://developers.cloudflare.com/queues/reference/how-queues-works/#consumers) are Worker scripts, and share the same [per invocation CPU limits](https://developers.cloudflare.com/workers/platform/limits/#worker-limits) as any Workers do. Note that CPU time is active processing time: not time spent waiting on network requests, storage calls, or other general I/O.\n\nBy default, the maximum CPU time per consumer Worker invocation is set to 30 seconds, but can be increased by setting `limits.cpu_ms` in your Wrangler configuration:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "To learn more about CPU time and limits, [review the Workers documentation](https://developers.cloudflare.com/workers/platform/limits/#cpu-time).\n\n</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Pricing Â· Cloudflare Queues docs\ndescription: Cloudflare Queues charges for the total number of operations\n  against each of your queues during a given month.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/pricing/\n  md: https://developers.cloudflare.com/queues/platform/pricing/index.md\n---\n\nNote\n\nCloudflare Queues requires the [Workers Paid plan](https://developers.cloudflare.com/workers/platform/pricing/#workers) to use, but does not increase your monthly subscription cost.\n\nCloudflare Queues charges for the total number of operations against each of your queues during a given month.\n\n* An operation is counted for each 64 KB of data that is written, read, or deleted.\n* Messages larger than 64 KB are charged as if they were multiple messages: for example, a 65 KB message and a 127 KB message would both incur two operation charges when written, read, or deleted.\n* A KB is defined as 1,000 bytes, and each message includes approximately 100 bytes of internal metadata.\n* Operations are per message, not per batch. A batch of 10 messages (the default batch size), if processed, would incur 10x write, 10x read, and 10x delete operations: one for each message in the batch.\n* There are no data transfer (egress) or throughput (bandwidth) charges.\n\n| | Workers Paid |\n| - | - |\n| Standard operations | 1,000,000 operations/month included + $0.40/million operations |\n\nIn most cases, it takes 3 operations to deliver a message: 1 write, 1 read, and 1 delete. Therefore, you can use the following formula to estimate your monthly bill:",
      "language": "unknown"
    },
    {
      "code": "Additionally:\n\n* Each retry incurs a read operation. A batch of 10 messages that is retried would incur 10 operations for each retry.\n* Messages that reach the maximum retries and that are written to a [Dead Letter Queue](https://developers.cloudflare.com/queues/configuration/batching-retries/) incur a write operation for each 64 KB chunk. A message that was retried 3 times (the default), fails delivery on the fourth time and is written to a Dead Letter Queue would incur five (5) read operations.\n* Messages that are written to a queue, but that reach the maximum persistence duration (or \"expire\") before they are read, incur only a write and delete operation per 64 KB chunk.\n\n## Examples\n\nIf an application writes, reads and deletes (consumes) one million messages a day (in a 30 day month), and each message is less than 64 KB in size, the estimated bill for the month would be:\n\n| | Total Usage | Free Usage | Billed Usage | Price |\n| - | - | - | - | - |\n| Standard operations | 3 \\* 30 \\* 1,000,000 | 1,000,000 | 89,000,000 | $35.60 |\n| | (write, read, delete) | | | |\n| **TOTAL** | | | | **$35.60** |\n\nAn application that writes, reads and deletes (consumes) 100 million \\~127 KB messages (each message counts as two 64 KB chunks) per month would have an estimated bill resembling the following:\n\n| | Total Usage | Free Usage | Billed Usage | Price |\n| - | - | - | - | - |\n| Standard operations | 2 \\* 3 \\* 100 \\* 1,000,000 | 1,000,000 | 599,000,000 | $239.60 |\n| | (2x ops for > 64KB messages) | | | |\n| **TOTAL** | | | | **$239.60** |\n\n</page>\n\n<page>\n---\ntitle: Choose a data or storage product Â· Cloudflare Queues docs\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/platform/storage-options/\n  md: https://developers.cloudflare.com/queues/platform/storage-options/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Delivery guarantees Â· Cloudflare Queues docs\ndescription: Delivery guarantees define how strongly a messaging system enforces\n  the delivery of messages it processes.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/reference/delivery-guarantees/\n  md: https://developers.cloudflare.com/queues/reference/delivery-guarantees/index.md\n---\n\nDelivery guarantees define how strongly a messaging system enforces the delivery of messages it processes.\n\nAs you make stronger guarantees about message delivery, the system needs to perform more checks and acknowledgments to ensure that messages are delivered, or maintain state to ensure a message is only delivered the specified number of times. This increases the latency of the system and reduces the overall throughput of the system. Each message may require an additional internal acknowledgements, and an equivalent number of additional roundtrips, before it can be considered delivered.\n\n* **Queues provides *at least once* delivery by default** in order to optimize for reliability.\n* This means that messages are guaranteed to be delivered at least once, and in rare occasions, may be delivered more than once.\n* For the majority of applications, this is the right balance between not losing any messages and minimizing end-to-end latency, as exactly once delivery incurs additional overheads in any messaging system.\n\nIn cases where processing the same message more than once would introduce unintended behaviour, generating a unique ID when writing the message to the queue and using that as the primary key on database inserts and/or as an idempotency key to de-duplicate the message after processing. For example, using this idempotency key as the ID in an upstream email API or payment API will allow those services to reject the duplicate on your behalf, without you having to carry additional state in your application.\n\n</page>\n\n<page>\n---\ntitle: How Queues Works Â· Cloudflare Queues docs\ndescription: Cloudflare Queues is a flexible messaging queue that allows you to\n  queue messages for asynchronous processing. Message queues are great at\n  decoupling components of applications, like the checkout and order fulfillment\n  services for an e-commerce site. Decoupled services are easier to reason\n  about, deploy, and implement, allowing you to ship features that delight your\n  customers without worrying about synchronizing complex deployments. Queues\n  also allow you to batch and buffer calls to downstream services and APIs.\nlastUpdated: 2025-02-12T13:41:31.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/reference/how-queues-works/\n  md: https://developers.cloudflare.com/queues/reference/how-queues-works/index.md\n---\n\nCloudflare Queues is a flexible messaging queue that allows you to queue messages for asynchronous processing. Message queues are great at decoupling components of applications, like the checkout and order fulfillment services for an e-commerce site. Decoupled services are easier to reason about, deploy, and implement, allowing you to ship features that delight your customers without worrying about synchronizing complex deployments. Queues also allow you to batch and buffer calls to downstream services and APIs.\n\nThere are four major concepts to understand with Queues:\n\n1. [Queues](#what-is-a-queue)\n2. [Producers](#producers)\n3. [Consumers](#consumers)\n4. [Messages](#messages)\n\n## What is a queue\n\nA queue is a buffer or list that automatically scales as messages are written to it, and allows a consumer Worker to pull messages from that same queue.\n\nQueues are designed to be reliable, and messages written to a queue should never be lost once the write succeeds. Similarly, messages are not deleted from a queue until the [consumer](#consumers) has successfully consumed the message.\n\nQueues does not guarantee that messages will be delivered to a consumer in the same order in which they are published.\n\nDevelopers can create multiple queues. Creating multiple queues can be useful to:\n\n* Separate different use-cases and processing requirements: for example, a logging queue vs. a password reset queue.\n* Horizontally scale your overall throughput (messages per second) by using multiple queues to scale out.\n* Configure different batching strategies for each consumer connected to a queue.\n\nFor most applications, a single producer Worker per queue, with a single consumer Worker consuming messages from that queue allows you to logically separate the processing for each of your queues.\n\n## Producers\n\nA producer is the term for a client that is publishing or producing messages on to a queue. A producer is configured by [binding](https://developers.cloudflare.com/workers/runtime-apis/bindings/) a queue to a Worker and writing messages to the queue by calling that binding.\n\nFor example, if we bound a queue named `my-first-queue` to a binding of `MY_FIRST_QUEUE`, messages can be written to the queue by calling `send()` on the binding:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nYou can also use [`context.waitUntil()`](https://developers.cloudflare.com/workers/runtime-apis/context/#waituntil) to send the message without blocking the response.\n\nNote that because `waitUntil()` is non-blocking, any errors raised from the `send()` or `sendBatch()` methods on a queue will be implicitly ignored.\n\nA queue can have multiple producer Workers. For example, you may have multiple producer Workers writing events or logs to a shared queue based on incoming HTTP requests from users. There is no limit to the total number of producer Workers that can write to a single queue.\n\nAdditionally, multiple queues can be bound to a single Worker. That single Worker can decide which queue to write to (or write to multiple) based on any logic you define in your code.\n\n### Content types\n\nMessages published to a queue can be published in different formats, depending on what interoperability is needed with your consumer. The default content type is `json`, which means that any object that can be passed to `JSON.stringify()` will be accepted.\n\nTo explicitly set the content type or specify an alternative content type, pass the `contentType` option to the `send()` method of your queue:",
      "language": "unknown"
    },
    {
      "code": "To only accept simple strings when writing to a queue, set `{ contentType: \"text\" }` instead:",
      "language": "unknown"
    },
    {
      "code": "The [`QueuesContentType`](https://developers.cloudflare.com/queues/configuration/javascript-apis/#queuescontenttype) API documentation describes how each format is serialized to a queue.\n\n## Consumers\n\nQueues supports two types of consumer:\n\n1. A [consumer Worker](https://developers.cloudflare.com/queues/configuration/configure-queues/), which is push-based: the Worker is invoked when the queue has messages to deliver.\n2. A [HTTP pull consumer](https://developers.cloudflare.com/queues/configuration/pull-consumers/), which is pull-based: the consumer calls the queue endpoint over HTTP to receive and then acknowledge messages.\n\nA queue can only have one type of consumer configured.\n\n### Create a consumer Worker\n\nA consumer is the term for a client that is subscribing to or *consuming* messages from a queue. In its most basic form, a consumer is defined by creating a `queue` handler in a Worker:",
      "language": "unknown"
    },
    {
      "code": "You then connect that consumer to a queue with `wrangler queues consumer <queue-name> <worker-script-name>` or by defining a `[[queues.consumers]]` configuration in your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/) manually:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Importantly, each queue can only have one active consumer. This allows Cloudflare Queues to achieve at least once delivery and minimize the risk of duplicate messages beyond that.\n\nBest practice\n\nConfigure a single consumer per queue. This both logically separates your queues, and ensures that errors (failures) in processing messages from one queue do not impact your other queues.\n\nNotably, you can use the same consumer with multiple queues. The queue handler that defines your consumer Worker will be invoked by the queues it is connected to.\n\n* The `MessageBatch` that is passed to your `queue` handler includes a `queue` property with the name of the queue the batch was read from.\n* This can reduce the amount of code you need to write, and allow you to process messages based on the name of your queues.\n\nFor example, a consumer configured to consume messages from multiple queues would resemble the following:",
      "language": "unknown"
    },
    {
      "code": "### Remove a consumer\n\nTo remove a queue from your project, run `wrangler queues consumer remove <queue-name> <script-name>` and then remove the desired queue below the `[[queues.consumers]]` in Wrangler file.\n\n### Pull consumers\n\nA queue can have a HTTP-based consumer that pulls from the queue, instead of messages being pushed to a Worker.\n\nThis consumer can be any HTTP-speaking service that can communicate over the Internet. Review the [pull consumer guide](https://developers.cloudflare.com/queues/configuration/pull-consumers/) to learn how to configure a pull-based consumer for a queue.\n\n## Messages\n\nA message is the object you are producing to and consuming from a queue.\n\nAny JSON serializable object can be published to a queue. For most developers, this means either simple strings or JSON objects. You can explicitly [set the content type](#content-types) when sending a message.\n\nMessages themselves can be [batched when delivered to a consumer](https://developers.cloudflare.com/queues/configuration/batching-retries/). By default, messages within a batch are treated as all or nothing when determining retries. If the last message in a batch fails to be processed, the entire batch will be retried. You can also choose to [explicitly acknowledge](https://developers.cloudflare.com/queues/configuration/batching-retries/) messages as they are successfully processed, and/or mark individual messages to be retried.\n\n</page>\n\n<page>\n---\ntitle: Wrangler commands Â· Cloudflare Queues docs\ndescription: Queues Wrangler commands use REST APIs to interact with the control\n  plane. This page lists the Wrangler commands for Queues.\nlastUpdated: 2025-11-14T16:29:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/queues/reference/wrangler-commands/\n  md: https://developers.cloudflare.com/queues/reference/wrangler-commands/index.md\n---\n\nQueues Wrangler commands use REST APIs to interact with the control plane. This page lists the Wrangler commands for Queues.\n\n## `queues list`\n\nList queues\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `--page` number\n\n  Page number for pagination\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues create`\n\nCreate a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\n- `--delivery-delay-secs` number default: 0\n\n  How long a published message should be delayed for, in seconds. Must be between 0 and 42300\n\n- `--message-retention-period-secs` number default: 345600\n\n  How long to retain a message in the queue, in seconds. Must be between 60 and 1209600\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues update`\n\nUpdate a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\n- `--delivery-delay-secs` number\n\n  How long a published message should be delayed for, in seconds. Must be between 0 and 42300\n\n- `--message-retention-period-secs` number\n\n  How long to retain a message in the queue, in seconds. Must be between 60 and 1209600\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues delete`\n\nDelete a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues info`\n\nGet queue information\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer add`\n\nAdd a Queue Worker Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue to configure\n\n- `[SCRIPT-NAME]` string required\n\n  Name of the consumer script\n\n- `--batch-size` number\n\n  Maximum number of messages per batch\n\n- `--batch-timeout` number\n\n  Maximum number of seconds to wait to fill a batch with messages\n\n- `--message-retries` number\n\n  Maximum number of retries for each message\n\n- `--dead-letter-queue` string\n\n  Queue to send messages that failed to be consumed\n\n- `--max-concurrency` number\n\n  The maximum number of concurrent consumer Worker invocations. Must be a positive integer\n\n- `--retry-delay-secs` number\n\n  The number of seconds to wait before retrying a message\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer remove`\n\nRemove a Queue Worker Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue to configure\n\n- `[SCRIPT-NAME]` string required\n\n  Name of the consumer script\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer http add`\n\nAdd a Queue HTTP Pull Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue for the consumer\n\n- `--batch-size` number\n\n  Maximum number of messages per batch\n\n- `--message-retries` number\n\n  Maximum number of retries for each message\n\n- `--dead-letter-queue` string\n\n  Queue to send messages that failed to be consumed\n\n- `--visibility-timeout-secs` number\n\n  The number of seconds a message will wait for an acknowledgement before being returned to the queue.\n\n- `--retry-delay-secs` number\n\n  The number of seconds to wait before retrying a message\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer http remove`\n\nRemove a Queue HTTP Pull Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue for the consumer\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer worker add`\n\nAdd a Queue Worker Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue to configure\n\n- `[SCRIPT-NAME]` string required\n\n  Name of the consumer script\n\n- `--batch-size` number\n\n  Maximum number of messages per batch\n\n- `--batch-timeout` number\n\n  Maximum number of seconds to wait to fill a batch with messages\n\n- `--message-retries` number\n\n  Maximum number of retries for each message\n\n- `--dead-letter-queue` string\n\n  Queue to send messages that failed to be consumed\n\n- `--max-concurrency` number\n\n  The maximum number of concurrent consumer Worker invocations. Must be a positive integer\n\n- `--retry-delay-secs` number\n\n  The number of seconds to wait before retrying a message\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues consumer worker remove`\n\nRemove a Queue Worker Consumer\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE-NAME]` string required\n\n  Name of the queue to configure\n\n- `[SCRIPT-NAME]` string required\n\n  Name of the consumer script\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues pause-delivery`\n\nPause message delivery for a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues resume-delivery`\n\nResume message delivery for a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues purge`\n\nPurge messages from a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[NAME]` string required\n\n  The name of the queue\n\n- `--force` boolean\n\n  Skip the confirmation dialog and forcefully purge the Queue\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription create`\n\nCreate a new event subscription for a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue to create the subscription for\n\n- `--source` string required\n\n  The event source type\n\n- `--events` string required\n\n  Comma-separated list of event types to subscribe to\n\n- `--name` string\n\n  Name for the subscription (auto-generated if not provided)\n\n- `--enabled` boolean default: true\n\n  Whether the subscription should be active\n\n- `--model-name` string\n\n  Workers AI model name (required for workersAi.model source)\n\n- `--worker-name` string\n\n  Worker name (required for workersBuilds.worker source)\n\n- `--workflow-name` string\n\n  Workflow name (required for workflows.workflow source)\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription list`\n\nList event subscriptions for a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue to list subscriptions for\n\n- `--page` number default: 1\n\n  Page number for pagination\n\n- `--per-page` number default: 20\n\n  Number of subscriptions per page\n\n- `--json` boolean default: false\n\n  Output in JSON format\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription get`\n\nGet details about a specific event subscription\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue\n\n- `--id` string required\n\n  The ID of the subscription to retrieve\n\n- `--json` boolean default: false\n\n  Output in JSON format\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription delete`\n\nDelete an event subscription from a queue\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue\n\n- `--id` string required\n\n  The ID of the subscription to delete\n\n- `--force` boolean alias: --y default: false\n\n  Skip confirmation\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n## `queues subscription update`\n\nUpdate an existing event subscription\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[QUEUE]` string required\n\n  The name of the queue\n\n- `--id` string required\n\n  The ID of the subscription to update\n\n- `--name` string\n\n  New name for the subscription\n\n- `--events` string\n\n  Comma-separated list of event types to subscribe to\n\n- `--enabled` boolean\n\n  Whether the subscription should be active\n\n- `--json` boolean default: false\n\n  Output in JSON format\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Queues & Rate Limits Â· Cloudflare Queues docs\ndescription: Example of how to use Queues to handle rate limits of external APIs.\nlastUpdated: 2025-10-13T13:40:40.000Z\nchatbotDeprioritize: false\ntags: TypeScript\nsource_url:\n  html: https://developers.cloudflare.com/queues/tutorials/handle-rate-limits/\n  md: https://developers.cloudflare.com/queues/tutorials/handle-rate-limits/index.md\n---\n\nThis tutorial explains how to use Queues to handle rate limits of external APIs by building an application that sends email notifications using [Resend](https://www.resend.com/). However, you can use this pattern to handle rate limits of any external API.\n\nResend is a service that allows you to send emails from your application via an API. Resend has a default [rate limit](https://resend.com/docs/api-reference/introduction#rate-limit) of two requests per second. You will use Queues to handle the rate limit of Resend.\n\n## Prerequisites\n\n1. Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages).\n2. Install [`Node.js`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).\n\nNode.js version manager\n\nUse a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm) to avoid permission issues and change Node.js versions. [Wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/), discussed later in this guide, requires a Node version of `16.17.0` or later.\n\n1. Sign up for [Resend](https://resend.com/) and generate an API key by following the guide on the [Resend documentation](https://resend.com/docs/dashboard/api-keys/introduction).\n\n2. Additionally, you will need access to Cloudflare Queues.\n\nQueues are included in the monthly subscription cost of your Workers Paid plan, and charges based on operations against your queues. Refer to [Pricing](https://developers.cloudflare.com/queues/platform/pricing/) for more details.\n\nBefore you can use Queues, you must enable it via [the Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/workers/queues). You need a Workers Paid plan to enable Queues.\n\nTo enable Queues:\n\n1. In the Cloudflare dashboard, go to the **Queues** page.\n\n   [Go to **Queues**](https://dash.cloudflare.com/?to=/:account/workers/queues)\n\n2. Select **Enable Queues**.\n\n## 1. Create a new Workers application\n\nTo get started, create a Worker application using the [`create-cloudflare` CLI](https://github.com/cloudflare/workers-sdk/tree/main/packages/create-cloudflare). Open a terminal window and run the following command:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "For setup, select the following options:\n\n* For *What would you like to start with?*, choose `Hello World example`.\n* For *Which template would you like to use?*, choose `Worker only`.\n* For *Which language do you want to use?*, choose `TypeScript`.\n* For *Do you want to use git for version control?*, choose `Yes`.\n* For *Do you want to deploy your application?*, choose `No` (we will be making some changes before deploying).\n\nThen, go to your newly created directory:",
      "language": "unknown"
    },
    {
      "code": "## 2. Set up a Queue\n\nYou need to create a Queue and a binding to your Worker. Run the following command to create a Queue named `rate-limit-queue`:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### Add Queue bindings to your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/)\n\nIn your Wrangler file, add the following:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "It is important to include the `max_batch_size` of two to the consumer queue is important because the Resend API has a default rate limit of two requests per second. This batch size allows the queue to process the message in the batch size of two. If the batch size is less than two, the queue will wait for 10 seconds to collect the next message. If no more messages are available, the queue will process the message in the batch. For more information, refer to the [Batching, Retries and Delays documentation](https://developers.cloudflare.com/queues/configuration/batching-retries)\n\nYour final Wrangler file should look similar to the example below.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 3. Add bindings to environment\n\nAdd the bindings to the environment interface in `worker-configuration.d.ts`, so TypeScript correctly types the bindings. Type the queue as `Queue<any>`. Refer to the following step for instructions on how to change this type.",
      "language": "unknown"
    },
    {
      "code": "## 4. Send message to the queue\n\nThe application will send a message to the queue when the Worker receives a request. For simplicity, you will send the email address as a message to the queue. A new message will be sent to the queue with a delay of one second.",
      "language": "unknown"
    },
    {
      "code": "This will accept requests to any subpath and forwards the request's body. It expects that the request body to contain only an email. In production, you should check that the request was a `POST` request. You should also avoid sending such sensitive information (email) directly to the queue. Instead, you can send a message to the queue that contains a unique identifier for the user. Then, your consumer queue can use the unique identifier to look up the email address in a database and use that to send the email.\n\n## 5. Process the messages in the queue\n\nAfter the message is sent to the queue, it will be processed by the consumer Worker. The consumer Worker will process the message and send the email.\n\nSince you have not configured Resend yet, you will log the message to the console. After you configure Resend, you will use it to send the email.\n\nAdd the `queue()` handler as shown below:",
      "language": "unknown"
    },
    {
      "code": "The above `queue()` handler will log the email address to the console and send the email. It will also retry the message if sending the email fails. The `delaySeconds` is set to five seconds to avoid sending the email too quickly.\n\nTo test the application, run the following command:",
      "language": "unknown"
    },
    {
      "code": "Use the following cURL command to send a request to the application:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## 6. Set up Resend\n\nTo call the Resend API, you need to configure the Resend API key. Create a `.dev.vars` file in the root of your project and add the following:",
      "language": "unknown"
    },
    {
      "code": "Replace `your-resend-api-key` with your actual Resend API key.\n\nNext, update the `Env` interface in `worker-configuration.d.ts` to include the `RESEND_API_KEY` variable.",
      "language": "unknown"
    },
    {
      "code": "Lastly, install the [`resend` package](https://www.npmjs.com/package/resend) using the following command:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "You can now use the `RESEND_API_KEY` variable in your code.\n\n## 7. Send email with Resend\n\nIn your `src/index.ts` file, import the Resend package and update the `queue()` handler to send the email.",
      "language": "unknown"
    },
    {
      "code": "The `queue()` handler will now send the email using the Resend API. It also checks if sending the email failed and will retry the message.\n\nThe final script is included below:",
      "language": "unknown"
    },
    {
      "code": "To test the application, start the development server using the following command:",
      "language": "unknown"
    },
    {
      "code": "Use the following cURL command to send a request to the application:",
      "language": "unknown"
    },
    {
      "code": "On the Resend dashboard, you should see that the email was sent to the provided email address.\n\n## 8. Deploy your Worker\n\nTo deploy your Worker, run the following command:",
      "language": "unknown"
    },
    {
      "code": "Lastly, add the Resend API key using the following command:",
      "language": "unknown"
    },
    {
      "code": "Enter the value of your API key. Your API key will get added to your project. You can now use the `RESEND_API_KEY` variable in your code.\n\nYou have successfully created a Worker which can send emails using the Resend API respecting rate limits.\n\nTo test your Worker, you could use the following cURL request. Replace `<YOUR_WORKER_URL>` with the URL of your deployed Worker.",
      "language": "unknown"
    },
    {
      "code": "Refer to the [GitHub repository](https://github.com/harshil1712/queues-rate-limit) for the complete code for this tutorial. If you are using [Hono](https://hono.dev/), you can refer to the [Hono example](https://github.com/harshil1712/resend-rate-limit-demo).\n\n## Related resources\n\n* [How Queues works](https://developers.cloudflare.com/queues/reference/how-queues-works/)\n* [Queues Batching and Retries](https://developers.cloudflare.com/queues/configuration/batching-retries/)\n* [Resend](https://resend.com/docs/)\n\n</page>\n\n<page>\n---\ntitle: Cloudflare Queues - Queues & Browser Rendering Â· Cloudflare Queues docs\ndescription: Example of how to use Queues and Browser Rendering to power a web crawler.\nlastUpdated: 2025-11-06T19:11:47.000Z\nchatbotDeprioritize: false\ntags: TypeScript\nsource_url:\n  html: https://developers.cloudflare.com/queues/tutorials/web-crawler-with-browser-rendering/\n  md: https://developers.cloudflare.com/queues/tutorials/web-crawler-with-browser-rendering/index.md\n---\n\nThis tutorial explains how to build and deploy a web crawler with Queues, [Browser Rendering](https://developers.cloudflare.com/browser-rendering/), and [Puppeteer](https://developers.cloudflare.com/browser-rendering/puppeteer/).\n\nPuppeteer is a high-level library used to automate interactions with Chrome/Chromium browsers. On each submitted page, the crawler will find the number of links to `cloudflare.com` and take a screenshot of the site, saving results to [Workers KV](https://developers.cloudflare.com/kv/).\n\nYou can use Puppeteer to request all images on a page, save the colors used on a site, and more.\n\n## Prerequisites\n\n1. Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up/workers-and-pages).\n2. Install [`Node.js`](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).\n\nNode.js version manager\n\nUse a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm) to avoid permission issues and change Node.js versions. [Wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/), discussed later in this guide, requires a Node version of `16.17.0` or later.\n\n## 1. Create new Workers application\n\nTo get started, create a Worker application using the [`create-cloudflare` CLI](https://github.com/cloudflare/workers-sdk/tree/main/packages/create-cloudflare). Open a terminal window and run the following command:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "For setup, select the following options:\n\n* For *What would you like to start with?*, choose `Hello World example`.\n* For *Which template would you like to use?*, choose `Worker only`.\n* For *Which language do you want to use?*, choose `TypeScript`.\n* For *Do you want to use git for version control?*, choose `Yes`.\n* For *Do you want to deploy your application?*, choose `No` (we will be making some changes before deploying).\n\nThen, move into your newly created directory:",
      "language": "unknown"
    },
    {
      "code": "## 2. Create KV namespace\n\nWe need to create a KV store. This can be done through the Cloudflare dashboard or the Wrangler CLI. For this tutorial, we will use the Wrangler CLI.\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "- npm",
      "language": "unknown"
    },
    {
      "code": "- yarn",
      "language": "unknown"
    },
    {
      "code": "- pnpm",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### Add KV bindings to the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/)\n\nThen, in your Wrangler file, add the following with the values generated in the terminal:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 3. Set up Browser Rendering\n\nNow, you need to set up your Worker for Browser Rendering.\n\nIn your current directory, install Cloudflare's [fork of Puppeteer](https://developers.cloudflare.com/browser-rendering/puppeteer/) and also [robots-parser](https://www.npmjs.com/package/robots-parser):\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "- npm",
      "language": "unknown"
    },
    {
      "code": "- yarn",
      "language": "unknown"
    },
    {
      "code": "- pnpm",
      "language": "unknown"
    },
    {
      "code": "Then, add a Browser Rendering binding. Adding a Browser Rendering binding gives the Worker access to a headless Chromium instance you will control with Puppeteer.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 4. Set up a Queue\n\nNow, we need to set up the Queue.\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### Add Queue bindings to wrangler.toml\n\nThen, in your Wrangler file, add the following:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Adding the `max_batch_timeout` of 60 seconds to the consumer queue is important because Browser Rendering has a limit of two new browsers per minute per account. This timeout waits up to a minute before collecting queue messages into a batch. The Worker will then remain under this browser invocation limit.\n\nYour final Wrangler file should look similar to the one below.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## 5. Add bindings to environment\n\nAdd the bindings to the environment interface in `src/index.ts`, so TypeScript correctly types the bindings. Type the queue as `Queue<any>`. The following step will show you how to change this type.",
      "language": "unknown"
    },
    {
      "code": "## 6. Submit links to crawl\n\nAdd a `fetch()` handler to the Worker to submit links to crawl.",
      "language": "unknown"
    },
    {
      "code": "This will accept requests to any subpath and forwards the request's body to be crawled. It expects that the request body only contains a URL. In production, you should check that the request was a `POST` request and contains a well-formed URL in its body. This has been omitted for simplicity.\n\n## 7. Crawl with Puppeteer\n\nAdd a `queue()` handler to the Worker to process the links you send.",
      "language": "unknown"
    },
    {
      "code": "This is a skeleton for the crawler. It launches the Puppeteer browser and iterates through the Queue's received messages. It fetches the site's `robots.txt` and uses `robots-parser` to check that this site allows crawling. If crawling is not allowed, the message is `ack`'ed, removing it from the Queue. If crawling is allowed, you can continue to crawl the site.\n\nThe `puppeteer.launch()` is wrapped in a `try...catch` to allow the whole batch to be retried if the browser launch fails. The browser launch may fail due to going over the limit for number of browsers per account.",
      "language": "unknown"
    },
    {
      "code": "This helper function opens a new page in Puppeteer and navigates to the provided URL. `numCloudflareLinks` uses Puppeteer's `$$eval` (equivalent to `document.querySelectorAll`) to find the number of links to a `cloudflare.com` page. Checking if the link's `href` is to a `cloudflare.com` page is wrapped in a `try...catch` to handle cases where `href`s may not be URLs.\n\nThen, the function sets the browser viewport size and takes a screenshot of the full page. The screenshot is returned as a `Buffer` so it can be converted to an `ArrayBuffer` and written to KV.\n\nTo enable recursively crawling links, add a snippet after checking the number of Cloudflare links to send messages recursively from the queue consumer to the queue itself. Recursing too deep, as is possible with crawling, will cause a Durable Object `Subrequest depth limit exceeded.` error. If one occurs, it is caught, but the links are not retried.",
      "language": "unknown"
    },
    {
      "code": "Then, in the `queue` handler, call `crawlPage` on the URL.",
      "language": "unknown"
    },
    {
      "code": "This snippet saves the results from `crawlPage` into the appropriate KV namespaces. If an unexpected error occurred, the URL will be retried and resent to the queue again.\n\nSaving the timestamp of the crawl in KV helps you avoid crawling too frequently.\n\nAdd a snippet before checking `robots.txt` to check KV for a crawl within the last hour. This lists all KV keys beginning with the same URL (crawls of the same page), and check if any crawls have been done within the last hour. If any crawls have been done within the last hour, the message is `ack`'ed and not retried.",
      "language": "unknown"
    },
    {
      "code": "The final script is included below.",
      "language": "unknown"
    },
    {
      "code": "## 8. Deploy your Worker\n\nTo deploy your Worker, run the following command:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "You have successfully created a Worker which can submit URLs to a queue for crawling and save results to Workers KV.\n\nTo test your Worker, you could use the following cURL request to take a screenshot of this documentation page.",
      "language": "unknown"
    },
    {
      "code": "Refer to the [GitHub repository for the complete tutorial](https://github.com/cloudflare/queues-web-crawler), including a front end deployed with Pages to submit URLs and view crawler results.\n\n## Related resources\n\n* [How Queues works](https://developers.cloudflare.com/queues/reference/how-queues-works/)\n* [Queues Batching and Retries](https://developers.cloudflare.com/queues/configuration/batching-retries/)\n* [Browser Rendering](https://developers.cloudflare.com/browser-rendering/)\n* [Puppeteer Examples](https://github.com/puppeteer/puppeteer/tree/main/examples)\n\n</page>\n\n<page>\n---\ntitle: Aggregation intervals Â· Cloudflare Radar docs\ndescription: Aggregation intervals allow you to return data in a specified\n  interval (or frequency). If no interval is defined, data will be returned in\n  the default aggregation interval (or frequency). As a general principle, the\n  longer the date range, the bigger the aggregation interval.\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/concepts/aggregation-intervals/\n  md: https://developers.cloudflare.com/radar/concepts/aggregation-intervals/index.md\n---\n\nAggregation intervals allow you to return data in a specified interval (or frequency). If no interval is defined, data will be returned in the default aggregation interval (or frequency). As a general principle, the longer the date range, the bigger the aggregation interval.\n\nFor example, when requesting one day of data, the default aggregation interval is 15 minutes. When requesting more than one month of data, the default is one day.\n\n## Method\n\n| Aggregation Interval | Description |\n| - | - |\n| `15m` | 15 minutes frequency. |\n| `1h` | One hour frequency. |\n| `1d` | One day frequency. |\n| `1w` | One week frequency. |\n\n</page>\n\n<page>\n---\ntitle: Bot classes Â· Cloudflare Radar docs\ndescription: A bot class in Radar is a grouping of bot scores.\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/concepts/bot-classes/\n  md: https://developers.cloudflare.com/radar/concepts/bot-classes/index.md\n---\n\nA bot class in Radar is a grouping of [bot scores](https://developers.cloudflare.com/bots/concepts/bot-score).\n\nScores between 1 and 29 are classified as bot traffic. Scores equal or above 30 are classified as non-bot/human traffic.\n\n| Class | Description |\n| - | - |\n| **Likely automated** | Bot scores of 1 through 29. |\n| **Likely human** | Bot scores of 30 through 99. |\n\n</page>\n\n<page>\n---\ntitle: Confidence levels Â· Cloudflare Radar docs\ndescription: The result.meta.confidenceInfo.level in the response provides an\n  indication of how much confidence Cloudflare has in the data. Confidence\n  levels can be affected either by internal issues affecting data quality or by\n  not having a lot of data for a given location (like Antarctica) or Autonomous\n  System (AS).\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/concepts/confidence-levels/\n  md: https://developers.cloudflare.com/radar/concepts/confidence-levels/index.md\n---\n\nThe `result.meta.confidenceInfo.level` in the response provides an indication of how much confidence Cloudflare has in the data. Confidence levels can be affected either by internal issues affecting data quality or by not having a lot of data for a given location (like Antarctica) or Autonomous System (AS).\n\n| Level | Description |\n| - | - |\n| **1** | There is not enough data in this time range and/or for this location or Autonomous System. Data also exhibits an erratic pattern, possibly due to the reasons previously mentioned. |\n| **2** | There is not enough data in this timerange and/or in this location or Autonomous System. |\n| **3** | Data exhibits an erratic pattern but is not affected by known data issues (like pipeline issues). |\n| **4** | Unassigned. |\n| **5** | No known data quality issues. |\n\n</page>\n\n<page>\n---\ntitle: Normalization methods Â· Cloudflare Radar docs\ndescription: Cloudflare Radar does not normally return raw values. Instead,\n  values are returned as percentages or normalized using min-max.\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/concepts/normalization/\n  md: https://developers.cloudflare.com/radar/concepts/normalization/index.md\n---\n\nCloudflare Radar does not normally return raw values. Instead, values are returned as percentages or normalized using min-max.\n\nRefer to the `result.meta.normalization` property in the response to check which post-processing method was applied to the raw values, if any.\n\n## Method\n\n| Method | Description |\n| - | - |\n| `PERCENTAGE` | Values represent percentages. |\n| `PERCENTAGE_CHANGE` | Values represent a [percentage change](https://en.wikipedia.org/wiki/Relative_change_and_difference#Percentage_change) from a baseline period. |\n| `OVERLAPPED_PERCENTAGE` | Values represent percentages that exceed 100% due to overlap. |\n| `MIN_MAX` | Values have been normalized using [min-max](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_\\(min-max_normalization\\)). |\n| `MIN0_MAX` | Values have been normalized using min-max, but setting the minimum value to `0`. Equivalent to a proportion of the maximum value in the entire response, scaled between 0 and 1. |\n| `RAW_VALUES` | Values are raw and have not been changed. |\n\nIf you want to compare values across locations/time ranges/etc., in endpoints that normalize values using min-max, you must do so in the same request. This is done by asking for multiple series. All values will then be normalized using the same minimum and maximum value and can safely be compared against each other. Refer to [Make comparisons](https://developers.cloudflare.com/radar/get-started/making-comparisons/) for more information.\n\n</page>\n\n<page>\n---\ntitle: Configure alerts Â· Cloudflare Radar docs\ndescription: You can configure alerts to receive notifications when Radar\n  detects changes impacting countries, regions, or autonomous systems.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/get-started/configure-alerts/\n  md: https://developers.cloudflare.com/radar/get-started/configure-alerts/index.md\n---\n\nYou can configure alerts to receive notifications when Radar detects changes impacting countries, regions, or autonomous systems.\n\nRadar Alerts\n\n**Who is it for?**\n\nCustomers who want to receive a notification when traffic anomalies, outages, route hijacks, or route leaks are impacting one or more countries, regions, or autonomous systems (ASNs) of interest.\n\n**Other options / filters**\n\nFilters include:\n\n* Notification type (anomaly, outage, route hijack, route leak)\n* Location (country or region)\n* Autonomous systems (ASNs)\n\nYou have the option to send the notification via email, webhook, or PagerDuty.\n\n**Included with**\n\nAll Cloudflare plans.\n\n**What should you do if you receive one?**\n\nFurther action will depend on your role. Refer to the [Radar documentation](https://developers.cloudflare.com/radar/) for more information.\n\nRefer to [Cloudflare Notifications](https://developers.cloudflare.com/notifications/get-started/) for more information on how to set up an alert.\n\n</page>\n\n<page>\n---\ntitle: Share a Radar chart Â· Cloudflare Radar docs\ndescription: Radar allows you to download an image of a chart, as well as embed\n  interactive cards of most charts into your own web pages.\nlastUpdated: 2024-11-14T21:48:04.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/get-started/embed/\n  md: https://developers.cloudflare.com/radar/get-started/embed/index.md\n---\n\nRadar allows you to download an image of a chart, as well as embed interactive cards of most charts into your own web pages.\n\nCharts supporting this feature will have a share icon next to its description.\n\n## Download a chart in PNG format\n\n1. Select the Share icon next to the description of the chart you wish to share.\n2. Select `Download Image.`\n3. A .png file containing the requested chart will be downloaded.\n\n## Embed an interactive chart in your website\n\n1. Select the Share icon next to the description of the chart you wish to share.\n\n2. Select between Fixed Time and Real Time.\n\n   * Real Time uses a â€œsliding windowâ€ based on the selected date range, and will display data points looking back over that duration from the current date/time.\n   * Fixed Time will always display a chart with only the currently visible data points.\n\n3. Select Copy Code and paste the code into your web page.\n\n**Note**: Your current selections, such as date range, location, autonomous system (ASN), and visible series, will be reflected in the shared chart.\n\n</page>\n\n<page>\n---\ntitle: Radar API error codes Â· Cloudflare Radar docs\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/get-started/error-codes/\n  md: https://developers.cloudflare.com/radar/get-started/error-codes/index.md\n---\n\n| Error Code | HTTP Status Code | Description |\n| - | - | - |\n| `2000` | 500 | Internal Error |\n| `2001` | 400 | Input Validation Error |\n| `2002` | 422 | Query is above max cost |\n| `1015` | 429 | Too many requests |\n| `7003` | 404 | Not found |\n\n</page>\n\n<page>\n---\ntitle: Make your first Radar API request Â· Cloudflare Radar docs\ndescription: To make your first request to Cloudflare's Radar API, you must\n  obtain your API token first. Create a Custom Token, with Account > Radar in\n  the Permissions group, and select Read as the access level.\nlastUpdated: 2024-12-16T22:33:26.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/get-started/first-request/\n  md: https://developers.cloudflare.com/radar/get-started/first-request/index.md\n---\n\nTo make your first request to Cloudflare's Radar API, you must obtain your [API token](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/) first. Create a Custom Token, with *Account* > *Radar* in the **Permissions** group, and select *Read* as the access level.\n\nOnce you have the token, you are ready to make your first request to Radar's API at `https://api.cloudflare.com/client/v4/radar/`.\n\n## Example using cURL\n\nIn the following example, we will access the global percentage distribution of device types (like mobile and desktop traffic) for the last seven days. For more information, refer to [Get device types summary](https://developers.cloudflare.com/api/resources/radar/subresources/http/subresources/summary/methods/device_type/) endpoint:",
      "language": "unknown"
    },
    {
      "code": "A successful response will look similar to the following:",
      "language": "unknown"
    },
    {
      "code": "This response means that 41% of the requests are classified as coming from mobile devices, while 58% are desktop traffic.\n\nNote\n\nCloudflare Radar attempts to provide trends and insights into general Internet usage, using the traffic that goes through Cloudflare infrastructure. As such, Cloudflare Radar only provides data on traffic coming from end-users, unless otherwise specified (for example, origin fetches are excluded).\n\nThe previous example returns all traffic from bots and humans. However, you can access just the traffic classified as coming from humans (the default in [Cloudflare Radar](https://radar.cloudflare.com)) by adding `botClass=LIKELY_HUMAN`. You can also access traffic coming only from bots with `botClass=LIKELY_AUTOMATED` (refer to [bot classes](https://developers.cloudflare.com/radar/concepts/bot-classes) for more information). For example:",
      "language": "unknown"
    },
    {
      "code": "Running the above, can you find any differences between both in the distribution of mobile versus desktop traffic?\n\nThe `result.meta` property\n\nThe `result.meta` property in the response includes metadata about the current request. In the example above, `meta.dateRange` returns the date range specified in the query, while `meta.normalization` returns the type of normalization applied to the data (refer to [Normalization methods](https://developers.cloudflare.com/radar/concepts/normalization) for more information).\n\nWhen querying for time series, `result.meta` will also include the returned [aggregation interval](https://developers.cloudflare.com/radar/concepts/aggregation-intervals) in `meta.aggInterval`.\n\nWhen present, `meta.confidenceInfo.level` will also provide an indication of how much confidence Cloudflare has in the data. Confidence levels are affected either by internal issues affecting data quality or by Cloudflare not having sufficient data for a given location or Autonomous System (AS). In these cases, confidence level will be below `5` (refer to [Confidence levels](https://developers.cloudflare.com/radar/concepts/confidence-levels) for more information).\n\n## Use Python\n\n[Python](https://www.python.org/) has become one of the standard languages in data analysis. Here is a quick example on how to chart the same data using [Requests](https://pypi.org/project/requests/) and [Pandas](https://pandas.pydata.org/) libraries. Here, we are using `format=csv` in the parameters to make it easier for Pandas to import.",
      "language": "unknown"
    },
    {
      "code": "### Notebooks\n\nA [notebook](https://jupyter.org/) is a web-based interactive computing application, where text, code, and code outputs, like charts, can be combined into a single document. Refer to Radar's companion [colaboratory notebook](https://colab.research.google.com/github/cloudflare/radar-notebooks/blob/main/notebooks/example.ipynb) for more examples on how the API can be used in your own projects.\n\n## Next steps\n\nRefer to [Make comparisons](https://developers.cloudflare.com/radar/get-started/making-comparisons/) to learn how to compare data.\n\n</page>\n\n<page>\n---\ntitle: Make comparisons Â· Cloudflare Radar docs\ndescription: When comparing time series, across locations/time ranges/etc., in\n  endpoints that normalize values using min-max, you must do so in the same\n  request. This is done by asking for multiple series. All values will then be\n  normalized using the same minimum and maximum value and can safely be compared\n  against each other.\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/get-started/making-comparisons/\n  md: https://developers.cloudflare.com/radar/get-started/making-comparisons/index.md\n---\n\nWhen comparing time series, across locations/time ranges/etc., in endpoints that normalize values using [min-max](https://developers.cloudflare.com/radar/concepts/normalization), you must do so in the same request. This is done by asking for multiple series. All values will then be normalized using the same minimum and maximum value and can safely be compared against each other.\n\n[NetFlows](https://developers.cloudflare.com/radar/investigate/netflows) values are normalized using [min0-max](https://developers.cloudflare.com/radar/concepts/normalization), so we will use it as an example. Refer to [Get NetFlow time series](https://developers.cloudflare.com/api/resources/radar/subresources/netflows/methods/timeseries/) for more information.\n\n## Compare locations\n\nIn the following example, we will compare the traffic change across two different locations â€” United States and Portugal. The example will use [alpha-2 codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2#Officially_assigned_code_elements) for the last seven days:",
      "language": "unknown"
    },
    {
      "code": "In the example above we are asking for two timeseries. The first series has the following parameters:\n\n`name=us_data&dateRange=7d&location=US`\n\nThe second series has the following parameters:\n\n`name=pt_data&dateRange=7d&location=PT`\n\nAll of these parameters are arrays, and it is the position in the array that defines the series the filter belongs to. Refer to [NetFlow's endpoint](https://developers.cloudflare.com/api/resources/radar/subresources/netflows/methods/timeseries/) for more information on the available parameters.\n\nThe response (shortened below for brevity) uses the provided `name` property to wrap the timestamps and corresponding values. If we chart this data, it becomes obvious that Cloudflare received much less traffic from Portugal than from the United States.",
      "language": "unknown"
    },
    {
      "code": "Comparisons can be made in most endpoints, not just endpoints that use `min-max`.\n\n## Compare date ranges\n\nIn the next example, we will compare the United States across different date ranges using the shortcuts `7d` and `7dControl`. These mean the last seven days and the last seven days before those, respectively â€” or, in other words, this week versus the previous week.",
      "language": "unknown"
    },
    {
      "code": "The first series has these parameters:\n\n`name=this_week&dateRange=7d&location=US`\n\nThe second series has the following parameters:\n\n`name=previous_week&dateRange=7dControl&location=US`\n\nNow, in the `result` property, you should get something like this:",
      "language": "unknown"
    },
    {
      "code": "Examining this information, we can conclude that the maximum value was reached at `2022-10-27T14:00:00Z` (all Radar timestamps are in Coordinated Universal Time (UTC)). We can also check what the date range shortcuts `7d` and `7dControl` were resolved to at the time this was run.\n\n### Use specific timestamps\n\nYou can also request for specific timestamps. In the following example, we will ask for data relative to [Tonga](https://blog.cloudflare.com/tonga-internet-outage/) in October versus January 2022, when there was an outage.",
      "language": "unknown"
    },
    {
      "code": "The first series has these parameters (URL encoded):\n\n`name=tonga&dateStart=2022-10-15T02%3A00%3A00Z&dateEnd=2022-10-15T05%3A00%3A00Z%&location=TO`\n\nThe second series has these parameters:\n\n`name=tonga_outage&dateStart=2022-01-15T02%3A00%3A00Z&&dateEnd=2022-01-15T05%3A00%3A00Z&location=TO`\n\nIn the above example, we requested for an [aggregation interval](https://developers.cloudflare.com/radar/concepts/aggregation-intervals) of one hour (`aggInterval=1h`), so that the results could be shown in this page. `format` and `aggInterval` are not arrays, as specified in the [API reference](https://developers.cloudflare.com/api/resources/radar/subresources/netflows/methods/timeseries/), and apply globally to all series in the request.\n\nThe `result` property should return a response like this:",
      "language": "unknown"
    },
    {
      "code": "This shows how traffic dropped to almost zero during the outage. If we chart it and set the end date to January 18 to make it clearer, we get the following:\n\n![Tonga October vs January 2022](https://developers.cloudflare.com/_astro/tonga_outage.DWg4Our9_FxEHD.webp)\n\n## Next steps\n\nRefer to the Investigate section to drill down on the data Radar returns, such as [NetFlows](https://developers.cloudflare.com/radar/investigate/netflows).\n\n</page>\n\n<page>\n---\ntitle: Application layer attacks Â· Cloudflare Radar docs\ndescription: While in HTTP requests you can examine all kinds of web requests,\n  in application layer attacks you have access only to mitigated HTTP requests.\n  These requests can be mitigated by one of several Cloudflare products, like\n  WAF, Cloudflare DDoS Protection, Cloudflare bot solutions and others.\nlastUpdated: 2025-05-08T15:28:21.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/application-layer-attacks/\n  md: https://developers.cloudflare.com/radar/investigate/application-layer-attacks/index.md\n---\n\nWhile in [HTTP requests](https://developers.cloudflare.com/radar/investigate/http-requests) you can examine all kinds of web requests, in application layer attacks you have access only to mitigated HTTP requests. These requests can be mitigated by one of several Cloudflare products, like [WAF](https://developers.cloudflare.com/waf/), [Cloudflare DDoS Protection](https://developers.cloudflare.com/ddos-protection/), [Cloudflare bot solutions](https://developers.cloudflare.com/bots/) and others.\n\nMitigated traffic\n\nMitigated traffic is any HTTP request from an end-user that has a terminating action applied by the Cloudflare platform. These include actions like `BLOCK` or [challenges](https://developers.cloudflare.com/cloudflare-challenges/).\n\nSince we are examining attacks, we can inspect both sides of an attack â€” both the source location and the target location of the attack. For the source of the attack Cloudflare uses the location the attack is coming from associated with the IP (note that the human orchestrator of the attack may be in a different location than the computer the attack is originating from). For the target location of the attacks, Cloudflare uses the billing location associated with the zone under attack.\n\nThis ability to filter by both sides of the attack is only available in the `top locations` endpoints. Unless otherwise specified, other endpoints are filtered by source location, like the origin location of the attack.\n\nThe magnitude of the attack is defined by the total number of mitigated requests.\n\nLike in [HTTP requests](https://developers.cloudflare.com/radar/investigate/http-requests), these endpoints can be split into the ability to fetch a timeseries, a single value summarizing the entire date range, and a list of top locations.\n\n## List of endpoints\n\n### Timeseries\n\n#### Example: Hourly mitigation requests by product\n\nLet us examine the global distribution of mitigated requests by product.",
      "language": "unknown"
    },
    {
      "code": "From the abbreviated response below, we can conclude that distributed denial-of-service (DDoS) attacks make up the majority of the requests â€” which makes sense since DDoS attacks, by their very nature, will perform more requests. This is followed by WAF and then I reputation requests.",
      "language": "unknown"
    },
    {
      "code": "For more information refer to [Get layer 7 attacks by mitigation technique, over time](https://developers.cloudflare.com/api/resources/radar/subresources/attacks/subresources/layer7/subresources/timeseries_groups/).\n\n### Summary\n\n#### Example: Mitigation requests by product\n\nWe can also filter by source location and examine attacks coming from a specific place - in the following example, we examine attacks coming from Great Britain:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "This response means that 75% of all mitigated requests coming from Great Britain were mitigated by the [WAF](https://developers.cloudflare.com/waf/) product.\n\nFor more information refer to [Get layer 7 attacks summary](https://developers.cloudflare.com/api/resources/radar/subresources/attacks/subresources/layer7/subresources/summary/methods/get/).\n\n### Top\n\n#### Example: Top target locations\n\nIn the following example, we will examine the top locations being targeted in application layer attacks, in the last 24 hours:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "During the specified date range, mitigation requests to zones with a billing address located in Belgium represent 18%.\n\nFor more information refer to [Get layer 7 top target locations](https://developers.cloudflare.com/api/resources/radar/subresources/attacks/subresources/layer7/subresources/top/subresources/locations/methods/target/).\n\n#### Example: Top attacks\n\nWhich source-target location pairs constitute the biggest attacks in the last 24 hours?",
      "language": "unknown"
    },
    {
      "code": "A typical response will be similar to the following:",
      "language": "unknown"
    },
    {
      "code": "This means that 3.79% of all mitigated requests are from and to the US, 3.6% of all mitigated requests are from the US to Belgium, etc..\n\nFor more information refer to [Get layer 7 top attack pairs](https://developers.cloudflare.com/api/resources/radar/subresources/attacks/subresources/layer7/subresources/top/methods/attacks/).\n\n## Next steps\n\nRefer to [Network layer attacks](https://developers.cloudflare.com/radar/investigate/network-layer-attacks/) for more information on data on layer 3 of the Open Systems Interconnection (OSI) model.\n\n</page>\n\n<page>\n---\ntitle: BGP anomalies Â· Cloudflare Radar docs\ndescription: To access Cloudflare Radar BGP Anomaly Detection results, you will\n  first need to create an API token that includes a Account:Radar permission.\n  All the following examples should work with a free-tier Cloudflare account.\nlastUpdated: 2025-03-19T09:17:37.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/bgp-anomalies/\n  md: https://developers.cloudflare.com/radar/investigate/bgp-anomalies/index.md\n---\n\nTo access Cloudflare Radar BGP Anomaly Detection results, you will first need to create an API token that includes a `Account:Radar` permission. All the following examples should work with a free-tier Cloudflare account.\n\n## Search BGP hijack events\n\nIn the following example, we will query the [BGP hijack events API](https://developers.cloudflare.com/api/resources/radar/subresources/bgp/subresources/hijacks/subresources/events/methods/list/) for the most recent BGP origin hijacks originated by or affecting `AS64512` (example ASN).",
      "language": "unknown"
    },
    {
      "code": "The result shows the most recent 10 BGP hijack events that affects `AS64512`.",
      "language": "unknown"
    },
    {
      "code": "In the response we can learn about the following information about each event:\n\n* `hijack_msg_count`: the number of potential BGP hijack messages observed from all peers.\n\n* `peer_asns`: the AS numbers of the route collector peers who observed the hijack messages.\n\n* `prefixes`: the affected prefixes.\n\n* `hijacker_asn` and `victim_asns`: the potential hijacker ASN and victim ASNs.\n\n* `confidence_score`: a quantitative score describing how confident the system is for this event being a hijack:\n\n  * 1-3: low confidence.\n  * 4-7: medium confidence.\n  * 8-above: high confidence.\n\n* `tags`: the evidence collected for the events. Each `tag` is also associated with a score that affects the overall confidence score:\n\n  * a positive score indicates that the event is *more likely* to be a hijack.\n  * a negative score indicates that the event is *less likely* to be a hijack.\n\nUsers can further filter out low-confidence events by attaching a `minConfidence=8` parameter, which will return only events with a `confidence_score` of `8` or higher.",
      "language": "unknown"
    },
    {
      "code": "## Search BGP route leak events\n\nBGP route leak is another type of BGP anomalies that Cloudflare Radar detects. Currently, we focus on detecting specifically the `provider-customer-provider` type of route leak. You can learn more about our design and methodology in [our blog post](https://blog.cloudflare.com/route-leak-detection-with-cloudflare-radar/).\n\nIn the following example, we will query the [BGP route leak events API](https://developers.cloudflare.com/api/resources/radar/subresources/bgp/subresources/leaks/subresources/events/methods/list/) for the most recent BGP route leak events affecting `AS64512`.",
      "language": "unknown"
    },
    {
      "code": "The result shows the most recent 10 BGP route leak events that affects `AS64512`.",
      "language": "unknown"
    },
    {
      "code": "In the response we can learn about the following information about each event:\n\n* `leak_asn`: the AS who potentially caused the leak.\n* `leak_seg`: the AS path segment observed and believed to be a leak.\n* `min_ts` and `max_ts`: the earliest and latest timestamps of the leak announcements.\n* `leak_count`: the total number of BGP route leak announcements observed.\n* `peer_count`: the number of route collector peers observed the leak.\n* `prefix_count` and `origin_count`: the number of prefixes and origin ASes affected by the leak.\n\n## Send alerts for BGP hijacks\n\nIn this example, we will show you how you can build a Cloudflare Workers app that sends out alerts for BGP hijacks relevant to a given ASN using webhooks (works for Google Hangouts, Discord, Telegram, etc) or email.\n\nWe will use Cloudflare Workers as the platform and use its Cron Triggers to periodically check for new alerts.\n\nFor the app, we would like it to do the following things:\n\n* Fetch from Cloudflare API with a given API token.\n* Check against Cloudflare KV to know what events are new.\n* Construct messages for new hijacks and send out alerts via webhook triggers.\n\n### Worker app setup\n\nWe will start with setting up a Cloudflare Worker app.\n\nFirst, create a new Workers app in a local directory:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "For setup, select the following options:\n\n* For *What would you like to start with?*, choose `Hello World example`.\n* For *Which template would you like to use?*, choose `Worker only`.\n* For *Which language do you want to use?*, choose `JavaScript`.\n* For *Do you want to use git for version control?*, choose `Yes`.\n* For *Do you want to deploy your application?*, choose `No` (we will be making some changes before deploying).\n\nTo start developing your Worker, `cd` into your new project directory:",
      "language": "unknown"
    },
    {
      "code": "In your Wrangler file, change the default checking frequency (once per hour) to what you like. Here is an example of configuring the workers to run the script five minutes.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "In this example, we will also need to use Cloudflare KV to save the latest checked event IDs which allows us to know what events are new. Once you have created a KV, you can head back to the `wranglers.toml` file and add the following sections:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "### Fetch for newly detected BGP hijacks\n\nStart with the API fetching function.\n\nThe following `apiFetch(env, paramsStr)` handles taking in a request parameters string, construct proper headers and fetch from the Cloudflare API BGP hijacks endpoint.",
      "language": "unknown"
    },
    {
      "code": "The `env` parameter is passed in from the caller, and we do not need to worry about construct it. The `paramsStr` is a string variable that holds the query parameters in a query URL.\n\nNow in our main cron trigger function, we will need to construct the query parameters and call the API fetch function. The default cron trigger worker script is defined as the follows:",
      "language": "unknown"
    },
    {
      "code": "In our example, we use the `env` variables to get the runtime variables like the TOKEN and ASN of interest, and Cloudflare KV bindings. We do not use the `controller` and `ctx` variables in this example.\n\nFirst, we will need to learn about what are the new events. We define new events as the events the app has not yet processed. We use the Cloudflare KV bucket previously created and defined (`HIJACKS_KV`) to save and retrieve the most recent processed event ID.",
      "language": "unknown"
    },
    {
      "code": "The main loop that checks for the most recent events looks like this (some of the validation code is skipped):",
      "language": "unknown"
    },
    {
      "code": "Now that we have all the newly detected events saved in `new_events` variable, we can then send out alerts:",
      "language": "unknown"
    },
    {
      "code": "### Send alerts using webhook\n\nThe function `send_alert` handles constructing alert message and sending out alerts using webhook. Here we demonstrate an example plain-text message template using Google Hangouts webhook. Users can customize the message and the use of webhook based on their platform of choice and needs.",
      "language": "unknown"
    },
    {
      "code": "Note that the webhook is considered secret and should be set to the environment via `wrangler secret put WEBHOOK_URL` command.\n\nThe last step is to deploy the application with command `npx wrangler deploy` and the app should be up and running on your Cloudflare account, and will be triggered to execute every five minutes.\n\n### Send email alerts from Workers\n\nIf you have [Email Routing](https://developers.cloudflare.com/email-routing/) enabled for your domain, you can also send email alerts directly from Workers. Refer to [Send emails from Workers](https://developers.cloudflare.com/email-routing/email-workers/send-email-workers/) to learn more.\n\nFor this alert to work, you will need to configure the proper email bindings in the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/#email-bindings).\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Then, you can create an email-sending function to send alert emails to your configured destination address:",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\nRefer to our API documentation for [BGP route leaks](https://developers.cloudflare.com/api/resources/radar/subresources/bgp/subresources/leaks/subresources/events/methods/list/) and [BGP hijacks](https://developers.cloudflare.com/api/resources/radar/subresources/bgp/subresources/hijacks/subresources/events/methods/list/) for more information on these topics.\n\n</page>\n\n<page>\n---\ntitle: DNS Â· Cloudflare Radar docs\ndescription: Access aggregated and anonymized DNS queries to Cloudflare's\n  1.1.1.1 public resolver service.\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/dns/\n  md: https://developers.cloudflare.com/radar/investigate/dns/index.md\n---\n\nAccess aggregated and anonymized DNS queries to Cloudflare's [1.1.1.1](https://developers.cloudflare.com/1.1.1.1/) public resolver service.\n\n## List of endpoints\n\n### Top locations\n\n#### Example: Geographical distribution of `google.com` versus `yandex.ru`\n\nIn the next example, we will request the top originating locations for `google.com` DNS queries:",
      "language": "unknown"
    },
    {
      "code": "The response shows that most queries come from the United States and Brazil:",
      "language": "unknown"
    },
    {
      "code": "Making the same search request for `yandex.ru`, a Russian search engine:",
      "language": "unknown"
    },
    {
      "code": "Returns the following response:",
      "language": "unknown"
    },
    {
      "code": "As expected, most queries come from Russia.\n\nNote\n\nNote that these examples return the total number of DNS queries from a location to a hostname, *out* of the total DNS queries to a given hostname. In this sense, it is expected that locations with higher population numbers â€” like the United States â€” frequently appear in the top spots, even if the actual percentage is low.\n\nYou can also provide multiple hostnames. Refer to [Get DNS top locations](https://developers.cloudflare.com/api/resources/radar/subresources/dns/subresources/top/methods/locations/) for more information. This is useful when the application you want to explore uses several hostnames to serve its content (like a hostname for the main website, another hostname dedicated to its API, etc.).\n\n## Next steps\n\nRefer to [Domain ranking](https://developers.cloudflare.com/radar/investigate/domain-ranking-datasets/) for more information on rankings generated by Cloudflare based on DNS queries to [1.1.1.1 public resolver](https://developers.cloudflare.com/1.1.1.1/).\n\n</page>\n\n<page>\n---\ntitle: Domains ranking Â· Cloudflare Radar docs\ndescription: \"Cloudflare regularly generates a domain ranking based on DNS\n  queries to 1.1.1.1,  Cloudflare's public DNS resolver.  Refer to the blog post\n  for a deep dive. In short, Cloudflare generates two types of listings:\"\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/domain-ranking-datasets/\n  md: https://developers.cloudflare.com/radar/investigate/domain-ranking-datasets/index.md\n---\n\nCloudflare regularly generates a domain ranking based on DNS queries to [1.1.1.1](https://developers.cloudflare.com/1.1.1.1/), Cloudflare's public DNS resolver. Refer to the [blog post](https://blog.cloudflare.com/radar-domain-rankings/) for a deep dive. In short, Cloudflare generates two types of listings:\n\n* An ordered list of the top 100 most popular domains globally and per country. This includes the last 24 hours and is updated daily.\n* An unordered global most popular domains dataset, divided into buckets of the following number of domains: 200, 500, 1,000, 2,000, 5,000, 10,000, 20,000, 50,000, 100,000, 200,000, 500,000, 1,000,000. It includes the last seven days and is updated weekly.\n\n## List of endpoints\n\n### Top\n\n#### Example: Get the current ordered top domains in the Cloudflare ranking",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "For more information refer to [Get top domains](https://developers.cloudflare.com/api/resources/radar/subresources/ranking/methods/top/).\n\n#### Example: Download top `x` ranking bucket file\n\nAs mentioned in the [blog post](https://blog.cloudflare.com/radar-domain-rankings/), Cloudflare provides an ordered rank for the top 100 domains, but for the remainder it only provides ranking buckets â€” like top 200 thousand, top one million, etc.. These are available through Cloudflare's [datasets endpoints](https://developers.cloudflare.com/api/resources/radar/subresources/datasets/methods/list/).\n\nIn the following example we will request the last available domain ranking buckets:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "If you are interested in a specific top (like the top one million), go through the `meta.top` property. After finding the top you are looking for, get its `id` to fetch the dataset using the [`GET dataset download url`](https://developers.cloudflare.com/api/resources/radar/subresources/datasets/methods/download/) endpoint.\n\nThen you can request a download url:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "#### Example: Get the last top `x` ranking bucket\n\nThis endpoint allows you to directly request the latest top x bucket available (optionally at a given date) [Get dataset stream](https://developers.cloudflare.com/api/resources/radar/subresources/datasets/methods/get/) endpoint.\n\nThe dataset alias can be retrieved from the [Get datasets](https://developers.cloudflare.com/api/resources/radar/subresources/datasets/methods/list/) endpoint as the example above.\n\nThis stream endpoint is only available for datasets generated after 2023-01-08.",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\nRefer to [Investigate outages](https://developers.cloudflare.com/radar/investigate/outages/) to get data from outages occurring around the world.\n\n</page>\n\n<page>\n---\ntitle: HTTP requests Â· Cloudflare Radar docs\ndescription: While in NetFlows we can inspect bytes and packets reaching\n  Cloudflare's edge routers, in HTTP requests we are a layer above in the OSI\n  model. HTTP requests examines complete HTTP requests from end users that reach\n  websites served by Cloudflare's CDN.\nlastUpdated: 2025-04-08T15:24:25.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/http-requests/\n  md: https://developers.cloudflare.com/radar/investigate/http-requests/index.md\n---\n\nWhile in [NetFlows](https://developers.cloudflare.com/radar/investigate/netflows/) we can inspect bytes and packets reaching Cloudflare's edge routers, in HTTP requests we are a layer above in the [OSI model](https://en.wikipedia.org/wiki/OSI_model). HTTP requests examines complete HTTP requests from end users that reach websites served by Cloudflare's [CDN](https://www.cloudflare.com/en-gb/learning/cdn/what-is-a-cdn/).\n\nNote\n\nHTTP traffic includes both HTTP and HTTPS traffic coming from end users.\n\nMost of the charts in the [Adoption and Usage](https://radar.cloudflare.com/adoption-and-usage) section on Radar come from this data source.\n\nThese endpoints can be broadly split into:\n\n* `timeseries`: A time series of a group of metrics. For example, when looking at IP version, displays an IPv4 time series and an IPv6 time series.\n* `summary`: Displays a summary of a group of metrics over the specified time range. For example, IPv4 traffic percentage out of the total HTTP traffic during that time period.\n* `top`: A list of the top locations or [Autonomous Systems](https://www.cloudflare.com/en-gb/learning/network-layer/what-is-an-autonomous-system/) (ASes) ranked by adoption of a specific metric. For example, top locations by mobile device traffic (like which locations have a higher percentage of mobile traffic out of the total traffic for that location).\n\n## List of endpoints\n\n### Timeseries\n\n#### Example: hourly breakdown by device type\n\nIn this example, we will request traffic by device type globally, with and without [bot traffic](https://developers.cloudflare.com/radar/concepts/bot-classes/). Parameters for the `human` series are `name=human&botClass=LIKELY_HUMAN&dateRange=1d`. For the `bot` series, the parameters are `name=bot&botClass=LIKELY_AUTOMATED&dateRange=1d`:",
      "language": "unknown"
    },
    {
      "code": "Here is the abbreviated response:",
      "language": "unknown"
    },
    {
      "code": "Mobile devices tend to be considerably more present when examining human generated traffic versus bot generated traffic.\n\nNote\n\nNote that device classification comes from the [User-agent](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/User-Agent) header. Ultimately, this classification depends on the user agent(s) that bots use.\n\nFor more information refer to [Get device types time series](https://developers.cloudflare.com/api/resources/radar/subresources/http/subresources/timeseries_groups/methods/device_type/).\n\n### Summary\n\n#### Example: overall breakdown by device type and human/bot traffic\n\nWe can also look at the same information asking for a summary of the device type breakdown over the entire period, instead of a per hour breakdown like in the example before.",
      "language": "unknown"
    },
    {
      "code": "Here is the abbreviated response:",
      "language": "unknown"
    },
    {
      "code": "For more information refer to the [API reference](https://developers.cloudflare.com/api/resources/radar/subresources/http/subresources/summary/methods/device_type/) for this endpoint.\n\n#### Example: breakdown by IP version and human/bot traffic\n\nIn the following example, we will examine global breakdown of traffic by IP version, with and without bots:",
      "language": "unknown"
    },
    {
      "code": "This returns the following:",
      "language": "unknown"
    },
    {
      "code": "Bots tend to use more IPv4 addresses.\n\nIt is also interesting to know how your ISP fares in IPv6 adoption. If you know your ISPâ€™s autonomous system number (ASN), you can use the `asn` parameter to query for this information. Refer to the [API reference](https://developers.cloudflare.com/api/resources/radar/subresources/http/subresources/summary/methods/ip_version/) for other parameters.\n\nIf you do not know your ISPâ€™s ASN, you can use [Radar](https://radar.cloudflare.com/ip) to find what it is.\n\n### Top\n\n#### Example: top locations by IPv6 traffic\n\nIn the following example, we will find which locations had a higher adoption of [IPv6](https://en.wikipedia.org/wiki/IPv6) in the last 28 days.",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "According to the returned data, India is leading in IPv6 adoption.\n\nFor more information refer to the [API reference](https://developers.cloudflare.com/api/resources/radar/subresources/http/subresources/locations/subresources/ip_version/methods/get/) for this endpoint.\n\n## Next steps\n\nRefer to [Application layer attacks](https://developers.cloudflare.com/radar/investigate/application-layer-attacks/) to learn more about mitigfated HTTP requests.\n\n</page>\n\n<page>\n---\ntitle: NetFlows Â· Cloudflare Radar docs\ndescription: NetFlows shows network traffic data from end users collected from\n  Cloudflare's edge routers. NetFlows' data also feeds the Internet traffic\n  change chart.\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/netflows/\n  md: https://developers.cloudflare.com/radar/investigate/netflows/index.md\n---\n\n[NetFlows](https://en.wikipedia.org/wiki/NetFlow) shows network traffic data from end users collected from Cloudflare's edge routers. NetFlows' data also feeds the [Internet traffic change](https://radar.cloudflare.com/) chart.\n\nNetFlows includes all types of traffic from Cloudflare's routers, not just traffic to websites served by Cloudflare's [CDN](https://www.cloudflare.com/en-gb/learning/cdn/what-is-a-cdn/).\n\n## List of endpoints\n\n### Timeseries\n\n#### Example: filtering by product\n\nBesides comparing time series across locations or date ranges (discussed in [Make comparisons](https://developers.cloudflare.com/radar/get-started/making-comparisons/)), we can also examine `ALL` traffic versus only `HTTP` traffic using the `product` filter. For more information, refer to the [API reference](https://developers.cloudflare.com/api/resources/radar/subresources/netflows/methods/timeseries/) for this endpoint.\n\nNetFlow products\n\n`HTTP` traffic only includes web traffic to Cloudflare's zones, while `ALL` also includes traffic to all other services, like [Spectrum](https://developers.cloudflare.com/spectrum/), [Magic Transit](https://developers.cloudflare.com/magic-transit/), [1.1.1.1](https://developers.cloudflare.com/1.1.1.1/), and others.\n\nIn the following example, we will examine both `ALL` and `HTTP` traffic in two [autonomous systems](https://www.cloudflare.com/en-gb/learning/network-layer/what-is-an-autonomous-system/). First, we will examine [AS3243](https://radar.cloudflare.com/as3243), a Portuguese local Internet Service Provider (ISP). The parameters for all traffic are `name=AS3243_all&product=ALL&dateRange=1d&asn=3243`, and for just the HTTP traffic are `name=AS3243_http&product=HTTP&dateRange=1d&asn=3243`):",
      "language": "unknown"
    },
    {
      "code": "This is the abbreviated response:",
      "language": "unknown"
    },
    {
      "code": "`HTTP` traffic values are similar to `ALL` traffic values. This means that most traffic Cloudflare receives from this AS is traffic to websites served by Cloudflare's [CDN](https://www.cloudflare.com/en-gb/learning/cdn/what-is-a-cdn/) product.\n\nIn this other example, we will examine [AS174](https://radar.cloudflare.com/as174), another autonomous system that is not an ISP:",
      "language": "unknown"
    },
    {
      "code": "The abbreviated response is:",
      "language": "unknown"
    },
    {
      "code": "Here, there is less `HTTP` traffic compared to other types of traffic â€” which makes sense, since this is not an ISP serving end-users.\n\nNote that here we made two separate requests since we are only interested in whether `HTTP` comprises the majority of the traffic in each AS or not. If we wanted to actually [compare](https://developers.cloudflare.com/radar/get-started/making-comparisons/) the traffic values between them to, for example, examine who has more traffic, we would have to make a single request including all series. Here is how we could do that:",
      "language": "unknown"
    },
    {
      "code": "which would lead to a response like this:",
      "language": "unknown"
    },
    {
      "code": "This response shows how Cloudflare receives more traffic from AS174 than from AS3243.\n\n## Next steps\n\nRefer to [HTTP requests](https://developers.cloudflare.com/radar/investigate/http-requests/) for more information about requests from end users.\n\n</page>\n\n<page>\n---\ntitle: Network layer attacks Â· Cloudflare Radar docs\ndescription: \"Network layer attacks show DDoS attack trends at the network\n  layer. These attacks can be split by the network protocol they use: ICMP, TCP,\n  UDP and others.\"\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/network-layer-attacks/\n  md: https://developers.cloudflare.com/radar/investigate/network-layer-attacks/index.md\n---\n\nNetwork layer attacks show [DDoS](https://www.cloudflare.com/en-gb/learning/ddos/layer-3-ddos-attacks/) attack trends at the network layer. These attacks can be split by the network protocol they use: [ICMP](https://www.cloudflare.com/en-gb/learning/ddos/glossary/internet-control-message-protocol-icmp/), [TCP](https://www.cloudflare.com/learning/ddos/glossary/tcp-ip/), [UDP](https://www.cloudflare.com/en-gb/learning/ddos/glossary/user-datagram-protocol-udp/) and others.\n\nNote\n\nUnlike what happens in [Application Layer Attacks](https://developers.cloudflare.com/radar/investigate/application-layer-attacks/), in network layer attacks location attribution does not use the location associated with the client's IP address. Instead, it uses the location of the data center itself. This is due to [IP spoofing](https://www.cloudflare.com/en-gb/learning/ddos/glossary/ip-spoofing/).\n\nWhen filtering by location or autonomous system (AS), we are filtering by the source location/AS of the attack â€” which can be very different to the location of the human orchestrator of the attack. Refer to [botnets](https://www.cloudflare.com/learning/ddos/what-is-a-ddos-botnet/) for more information.\n\n## List of endpoints\n\n### Timeseries\n\n#### Example: hourly percentage breakdown by attack method\n\nIn the following example, we will examine the worldwide versus Singapore distribution of mitigated attacks by network protocol:",
      "language": "unknown"
    },
    {
      "code": "If we inspect the abbreviated response below, we can conclude that globally, at those timestamps, `UDP` and `TCP` attacks were mostly evenly split.",
      "language": "unknown"
    },
    {
      "code": "We can also conclude that the distribution of network layer attacks coming from Singapore â€” or, more accurately, reaching Cloudflare's data center located in Singapore â€” differs quite a bit from the worldwide distribution. At those times, the distribution of network layer attacks clearly favors [TCP](https://www.cloudflare.com/learning/ddos/glossary/tcp-ip/).\n\nFor more information refer to the [API reference](https://developers.cloudflare.com/api/resources/radar/subresources/attacks/subresources/layer3/methods/timeseries/) for this endpoint.\n\n### Summary\n\n#### Example: Russia - overall percentage breakdown by network protocol\n\nWe can also filter by source location and examine attacks coming from Russia:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "The response shows that the attacks coming from Russia to other locations tended to use the [UDP](https://www.cloudflare.com/en-gb/learning/ddos/glossary/user-datagram-protocol-udp/) network protocol at those timestamps.\n\nFor more information refer to the [API reference](https://developers.cloudflare.com/api/resources/radar/subresources/attacks/subresources/layer3/methods/timeseries/) for this endpoint.\n\n## Next steps\n\nRefer to [DNS](https://developers.cloudflare.com/radar/investigate/dns/) to learn more about the aggregated and anonymized DNS queries to Cloudflare's [1.1.1.1](https://developers.cloudflare.com/1.1.1.1/) public resolver service.\n\n</page>\n\n<page>\n---\ntitle: Outages Â· Cloudflare Radar docs\ndescription: Cloudflare Radar Outage Center (CROC) provides data on outages\n  occurring around the world.\nlastUpdated: 2025-02-04T11:06:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/outages/\n  md: https://developers.cloudflare.com/radar/investigate/outages/index.md\n---\n\n[Cloudflare Radar Outage Center (CROC)](https://radar.cloudflare.com/outage-center) provides data on outages occurring around the world.\n\nRefer the [blog post](https://blog.cloudflare.com/announcing-cloudflare-radar-outage-center/) for more information but, in short, Radar provides the following information:\n\n* **Location**: Where was the outage?\n* **ASN**: What autonomous system experienced a disruption in connectivity?\n* **Type**: How broad was the outage? Did connectivity fail nationwide, or at a sub-national level? Did just a single network provider have an outage?\n* **Scope**: If it was a sub-national/regional outage, what state or city was impacted? If it was a network-level outage, which one was it?\n* **Cause**: Insight into the likely cause of the outage, based on publicly available information. Historically, some outages have been government directed shutdowns, while others are caused by severe weather or natural disasters, or by infrastructure issues such as cable cuts, power outages, or filtering/blocking.\n* **Start time**: When did the outage start?\n* **End time**: When did the outage end?\n\n## List of endpoints\n\n### Outages\n\n#### Example: Get outages in the last 7 days",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "Refer to the [API reference](https://developers.cloudflare.com/api/resources/radar/subresources/annotations/subresources/outages/methods/get/) for more information regarding this endpoint.\n\nHaving data on a given outage allows you to examine its impact through both [NetFlows](https://developers.cloudflare.com/radar/investigate/netflows/) (like in the [Tonga outage](https://developers.cloudflare.com/radar/get-started/making-comparisons/#use-specific-timestamps) and [others](https://blog.cloudflare.com/q3-2022-internet-disruption-summary/)) and [HTTP](https://developers.cloudflare.com/radar/investigate/http-requests/) data (for example, did the outage affect more mobile than desktop traffic?).\n\n</page>\n\n<page>\n---\ntitle: URL Scanner Â· Cloudflare Radar docs\ndescription: To better understand Internet usage around the world, use\n  Cloudflare's URL Scanner. With Cloudflare's URL Scanner, you have the ability\n  to investigate the details of a domain, IP, URL, or ASN. Cloudflare's URL\n  Scanner is available in the Security Center of the Cloudflare dashboard,\n  Cloudflare Radar, and the Cloudflare API.\nlastUpdated: 2025-09-04T11:41:09.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/investigate/url-scanner/\n  md: https://developers.cloudflare.com/radar/investigate/url-scanner/index.md\n---\n\nTo better understand Internet usage around the world, use Cloudflare's URL Scanner. With Cloudflare's URL Scanner, you have the ability to investigate the details of a domain, IP, URL, or ASN. Cloudflare's URL Scanner is available in the Security Center of the Cloudflare dashboard, [Cloudflare Radar](https://radar.cloudflare.com/scan), and the Cloudflare [API](https://developers.cloudflare.com/api/resources/url_scanner/).\n\n## Use the API\n\nTo make your first URL scan using the API, you must obtain a URL Scanner specific [API token](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/). Create a Custom Token with *Account* > *URL Scanner* in the **Permissions** group, and select *Edit* as the access level.\n\nOnce you have the token, and you know your `account_id`, you are ready to make your first request to the API at `https://api.cloudflare.com/client/v4/accounts/{account_id}/urlscanner/`.\n\n### Submit URL to scan\n\nTo submit a URL to scan, the only required information is the URL to be scanned in the `POST` request body:",
      "language": "unknown"
    },
    {
      "code": "By default, the report will have a `Public` visibility level, which means it will appear in the [recent scans](https://radar.cloudflare.com/scan#recent-scans) list and in search results. It will also include a single screenshot with desktop resolution.\n\nA successful response will have a status code of `200` and be similar to the following:",
      "language": "unknown"
    },
    {
      "code": "You can submit up to 100 URLs at the same time via the [API](https://developers.cloudflare.com/api/resources/url_scanner/subresources/scans/methods/bulk_create/).\n\nThe `uuid` property in the response above identifies the scan and will be required when fetching the scan report.\n\n#### Submit a custom URL Scan\n\nHere's an example request body with some custom configuration options:",
      "language": "unknown"
    },
    {
      "code": "Above, the visibility level is set as `Unlisted`, which means that the scan report won't be included in the [recent scans](https://radar.cloudflare.com/scan#recent-scans) list nor in search results. In effect, only users with knowledge of the scan ID will be able to access it.\n\nThere will also be three screenshots taken of the webpage, one per target device type. The [`User-Agent`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/User-Agent) will be set as \"XXX-my-user-agent\". Note that you can set any custom HTTP header, including [Authorization](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Authorization).\n\nHeader\n\nSuccessful scans are subject to a retention policy of 12 months. Failed scans older than 30 days will be deleted.\n\n### Get scan report\n\nOnce the URL Scan submission is made, the current progress can be checked by calling `https://api.cloudflare.com/client/v4/accounts/{account_id}/urlscanner/v2/result/{scan_id}`. The `scan_id` will be the `uuid` value returned in the previous response.\n\nWhile the scan is in progress, the HTTP status code will be `404`; once it is finished, it will be `200`. Cloudflare recommends that you poll every 10-30 seconds.\n\nThe response will include, among others, the following top properties:\n\n* `task` - Information on the scan submission.\n* `page` - Information pertaining to the primary response, for example IP address, ASN, server, and page redirect history.\n* `data.requests` - Request chains involved in the page load.\n* `data.cookies` - Cookies set by the page.\n* `data.globals` - Non-standard JavaScript global variables.\n* `data.console` - Console logs.\n* `data.performance` - Timings as given by the [`PerformanceNavigationTiming`](https://developer.mozilla.org/en-US/docs/Web/API/PerformanceNavigationTiming) interface.\n* `meta` - Meta processors output including detected technologies, domain and URL categories, rank, geolocation information, and others.\n* `lists.ips` - IPs contacted.\n* `lists.asns` - AS Numbers contacted.\n* `lists.domains` - Hostnames contacted, including `dns` record information.\n* `lists.hashes` - Hashes of response bodies, of the main page HTML structure, screenshots, and favicons.\n* `lists.certificates` - TLS certificates of HTTP responses.\n* `verdicts` - Verdicts on malicious content.\n\nSome examples of more specific properties include:\n\n* `task.uuid` - ID of the scan.\n* `task.url` - Submitted URL of the scan. May differ from final URL (`page.url`) if there are HTTP redirects.\n* `task.success` - Whether scan was successful or not. Scans can fail for various reasons, including DNS errors.\n* `task.status` - Current scan status, for example, `Queued`, `InProgress`, or `Finished`.\n* `meta.processors.domainCategories` - Cloudflare categories of the main hostname contacted.\n* `meta.processors.phishing` - What kind of phishing, if any, was detected.\n* `meta.processors.radarRank` - [Cloudflare Radar Rank](http://blog.cloudflare.com/radar-domain-rankings/) of the main hostname contacted.\n* `meta.processors.wappa` - The kind of technologies detected as being in use by the website, with the help of [Wappalyzer](https://github.com/Lissy93/wapalyzer).\n* `page.url` - URL of the primary request, after all HTTP redirects.\n* `page.country` - Country name from geolocation data associated with the main IP address contacted.\n* `page.history` - Main page history, including any HTTP redirects.\n* `page.screenshot` - Various hashes of the main screenshot. Can be used to search for sites with similar screenshots.\n* `page.domStructHash` - HTML structure hash. Use it to search for sites with similar structure.\n* `page.favicon.hash` - MD5 hash of the favicon.\n* `verdicts.overall.malicious` - Whether the website was considered malicious *at the time of the scan*. Please check the remaining properties for each subsystem(s) for specific threats detected.\n\nThe [Get URL Scan](https://developers.cloudflare.com/api/resources/url_scanner/subresources/scans/methods/get/) API endpoint documentation contains the full response schema.\n\nTo fetch the scan's [screenshots](https://developers.cloudflare.com/api/resources/url_scanner/subresources/scans/methods/screenshot/) or full [network log](https://developers.cloudflare.com/api/resources/url_scanner/subresources/scans/methods/har/) refer to the corresponding endpoints' documentation.\n\n### Search scans\n\nUse a subset of ElasticSearch Query syntax to filter scans. Search results will include `Public` scans and your own `Unlisted` scans.\n\nTo search for scans to the hostname `google.com`, use the query parameter `q=page.domain:\"google.com\"`:",
      "language": "unknown"
    },
    {
      "code": "If, instead, you wanted to search for scans that made at least one request to the hostname `cdnjs.cloudflare.com`, for example sites that use a JavaScript library hosted at `cdnjs.cloudflare.com`, use the query parameter `hostname=cdnjs.cloudflare.com`:",
      "language": "unknown"
    },
    {
      "code": "Some other example queries:\n\n* `task.url:\"https://google.com\" OR task.url:\"https://www.google.com\"`: Search for scans whose submitted URL was either `google.com` or `www.google.com`. URLs must be enclosed in quotes.\n* `page.url:\"https://google.com\" AND NOT task.url:\"https://google.com\"`: Search for scans to `google.com` whose submitted URL was not `google.com` (that is, sites that redirected to google.com).\n* `page.domain:microsoft AND verdicts.malicious:true AND NOT page.domain:microsoft.com`: Malicious scans whose hostname starts with `microsoft`. Would match domains like `microsoft.phish.com`.\n* `apikey:me AND date:[2024-01 TO 2024-10]`: Your scans from January 2024 to October 2024.\n* `page.domain:(blogspot OR www.blogspot)`: Searches for scans whose main domain starts with `blogspot` or with `www.blogspot`.\n* `date:>now-7d AND path:okta-sign-in.min.js`: Scans from the last seven days with any request path that ends with `okta-sign-in.min.js`.\n* `page.asn:AS24940 AND hash:-557369673`: Websites hosted in AS24940 where a resource with the given hash was retrieved.\n* `hash:8f662c2ce9472ba8d03bfeb8cdae112dbc0426f99da01c5d70c7eb4afd5893ca`: Using the hash at `page.domStructHash` search for other scans with the same HTML structure hash.\n\nGo to [Search URL scans](https://developers.cloudflare.com/api/resources/url_scanner/subresources/scans/methods/list/) in the API documentation for the full list of available options.\n\n### Security Center\n\nAlternatively, you can search in the Security Center:\n\n1. In the Cloudflare dashboard, go to the **Investigate** page.\n\n   [Go to **Investigate**](https://dash.cloudflare.com/?to=/:account/security-center/investigate)\n\n2. Enter your query and select **Search**.\n\nYou can scan a URL by location. Scanning a URL by location allows you to analyze how a website may present different content depending on your location. This helps to expose and examine region-specific malicious activities.\n\nNote\n\nOnly Enterprise customers can scan a URL by location.\n\nTo scan a URL based on your geographic location:\n\n1. Enter your URL.\n2. Go to **Location** > Select which country to scan the URL from.\n3. Select **Scan now**.\n\nYou can also use the [API](https://developers.cloudflare.com/api/resources/url_scanner/subresources/scans/methods/create/#\\(params\\)%20default%20%3E%20\\(param\\)%20account_id%20%3E%20) to scan a URL from a specific location.\n\nIn Security Center, you can retrieve pre-filtered information by:\n\n* Similar screenshot\n* Identical favicon\n* Similar favicon\n* Similar HTML structure\n* Identical ASN\n* Identical IP\n* Identical domain\n* Identical final URL (after all redirections)\n\n</page>\n\n<page>\n---\ntitle: Quarterly DDoS threat reports Â· Cloudflare Radar docs\ndescription: Quarterly DDoS threat reports provide a comprehensive overview of\n  DDoS attack insights and trends over a three-month period.\nlastUpdated: 2025-06-24T23:18:20.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/radar/reference/quarterly-ddos-reports/\n  md: https://developers.cloudflare.com/radar/reference/quarterly-ddos-reports/index.md\n---\n\nQuarterly DDoS threat reports provide a comprehensive overview of DDoS attack insights and trends over a three-month period.\n\nThanks to our vast network, Cloudflare provides insights on the evolving threat landscape, including variations in attack sizes, techniques, top source countries, top targeted countries and targeted industries. Each report presents a global outlook, dives into significant attacks and campaigns, and explores shifts in DDoS tactics, offering a blend of data analysis and insights to better understand the cyber threat environment.\n\nFind the latest quarterly DDoS threat reports in the [**Reports**](https://radar.cloudflare.com/reports) section of Cloudflare Radar.\n\n***\n\n## Methodologies\n\n### How we count the number of DDoS attacks\n\nCloudflare's main DDoS system, the [DDoS Managed Ruleset](https://developers.cloudflare.com/ddos-protection/managed-rulesets/), generates real-time fingerprints for DDoS attacks that it automatically detects and mitigates. While there may be multiple fingerprints generated for a single DDoS attack, or attack campaign, we count unique fingerprints that resulted in mitigation to get an understanding of the number of DDoS attacks for a given period of time. While in some cases, we can see an 'explosion' of fingerprints due to randomized DDoS attacks, for the most part, this figure gives us a reliable indicator to track over time.\n\nCurrently, the number of DDoS attacks does not take into consideration the [Advanced TCP Protection system](https://developers.cloudflare.com/ddos-protection/advanced-ddos-systems/overview/advanced-tcp-protection/) or the [Advanced DNS Protection system](https://developers.cloudflare.com/ddos-protection/advanced-ddos-systems/overview/advanced-dns-protection/). We also don't take into account any mitigations by customer-created rules or configuration.\n\n### How we calculate ransom DDoS attack insights\n\nCloudflareâ€™s systems constantly analyze traffic and automatically apply mitigation when DDoS attacks are detected. Each attacked customer is prompted with an automated survey to help Cloudflare better understand the nature of the attack and the success of the mitigation. For over two years, Cloudflare has been surveying attacked customers. One of the questions in the survey asks the respondents if they received a threat or a ransom note. Over the past few years, on average, Cloudflare has been collecting around 200 responses per quarter. The responses of this survey are used to calculate the percentage of ransom DDoS attacks.\n\n### How we calculate geographical and industry insights\n\n#### Source country\n\nAt the application layer, Cloudflare uses the attacking IP addresses to understand the origin country of the attacks. That is because at that layer, IP addresses cannot be spoofed [1](#user-content-fn-1) (or modified). However, at the network layer, source IP addresses can be spoofed. So, instead of relying on IP addresses to understand the source, Cloudflare uses the location of our data centers where the attack packets were ingested. It is possible to obtain geographical accuracy due to Cloudflare's large global coverage in over 300 locations around the world.\n\n#### Target country\n\nFor both application-layer and network-layer DDoS attacks, attacks and traffic are grouped by customersâ€™ billing country. This allows Cloudflare to understand which countries are subject to more attacks.\n\n#### Target industry\n\nFor both application-layer and network-layer DDoS attacks, attacks and traffic are grouped by customersâ€™ industry according to the internal customer relations management system. This allows Cloudflare to understand which industries are subject to more attacks.\n\n#### Total volume versus percentage\n\nFor both source and target insights, Cloudflare looks at the total volume of attack traffic compared to all traffic as one data point. Additionally, Cloudflare also takes into account the percentage of attack traffic towards or from a specific country, to a specific country or to a specific industry. This gives us an \"attack activity rate\" for a given country/industry which is normalized by their total traffic levels. This helps us remove biases of a country or industry that normally receives a lot of traffic and therefore, a lot of attack traffic as well.\n\n#### Ranking\n\nFor source, target, and industry insights, Cloudflare may calculate a \"Rank\" for each dimension. The calculation takes into consideration HTTP DDoS attacks, network-layer DDoS attacks, and the total volume and the percentage of DDoS attack traffic out of the total traffic for each attack type. The Ranking system lets Cloudflare provide a simple single score that means how much a certain industry is being attacked, for example. In the graphs, the Rank values are displayed in an inverted way (a longer bar in the chart means a higher rank and more attacks).\n\n### How we calculate attack characteristics\n\nTo calculate the attack size, duration, attack vectors, and emerging threats, Cloudflare buckets attacks and then provides the share of each bucket out of the total amount for each dimension.\n\nHowever, in the **Network layer attack distribution** graph of the [**Security & Attacks**](https://radar.cloudflare.com/security-and-attacks) Radar page these trends are calculated by number of bytes instead. Since attacks may vary greatly in number of bytes from one another, this could lead to trends differing between the quarterly reports and the graph displayed in Cloudflare Radar.\n\n***\n\n## Final remarks\n\n### Countries as source or target of attacks\n\nWhen Cloudflare describes \"top countries\" as the source or target of attacks, it does not necessarily mean that a certain country was attacked as a country, but rather that organizations that use that country as their billing country were targeted by attacks.\n\nSimilarly, \"attacks originating from a country\" does not mean that a given country launched the attacks, but rather that the attack was launched from IP addresses mapped to that country. Threat actors operate global botnets with nodes all over the world, and often also use VPNs (virtual private networks) and proxies to obfuscate their true location. This means that the source country could indicate the presence of exit nodes or botnet nodes within that country.\n\n### Excluded items due to insufficient data\n\nThe insights and trends presented in quarterly reports exclude certain countries and industries when there is not enough data to provide statistically meaningful insights.\n\n### Map chart coloring\n\nIn the map charts, the countries and regions are colored using a diverging scale, ranging from white (minimum value) to red (maximum value). These vary according to the selected world continent group.\n\n## Footnotes\n\n1. IP spoofing is the creation of Internet Protocol (IP) packets which have a modified source address to hide the identity of the sender, impersonate another computer system, or both. [â†©](#user-content-fnref-1)\n\n</page>\n\n<page>\n---\ntitle: R2 SQL - Pricing Â· R2 SQL docs\ndescription: R2 SQL is in open beta and available to any developer with an R2 subscription.\nlastUpdated: 2025-09-25T04:13:57.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2-sql/platform/pricing/\n  md: https://developers.cloudflare.com/r2-sql/platform/pricing/index.md\n---\n\nR2 SQL is in open beta and available to any developer with an [R2 subscription](https://developers.cloudflare.com/r2/pricing/).\n\nWe are not currently billing for R2 SQL during open beta. However, you will be billed for standard [R2 storage and operations](https://developers.cloudflare.com/r2/pricing/) for data accessed by queries.\n\nWe plan to bill based on the volume of data queried by R2 SQL. We'll provide at least 30 days notice before we make any changes or start charging for R2 SQL usage.\n\n</page>\n\n<page>\n---\ntitle: Limitations and best practices Â· R2 SQL docs\ndescription: R2 SQL is designed for querying partitioned Apache Iceberg tables\n  in your R2 data catalog. This document outlines the supported features,\n  limitations, and best practices of R2 SQL.\nlastUpdated: 2025-12-12T16:58:55.000Z\nchatbotDeprioritize: false\ntags: SQL\nsource_url:\n  html: https://developers.cloudflare.com/r2-sql/reference/limitations-best-practices/\n  md: https://developers.cloudflare.com/r2-sql/reference/limitations-best-practices/index.md\n---\n\nNote\n\nR2 SQL is in open beta. Limitations and best practices will change over time.\n\nR2 SQL is designed for querying **partitioned** Apache Iceberg tables in your R2 data catalog. This document outlines the supported features, limitations, and best practices of R2 SQL.\n\n## Quick Reference\n\n| Feature | Supported | Notes |\n| - | - | - |\n| Basic SELECT | Yes | Columns, \\* |\n| Aggregation functions | Yes | COUNT(\\*), SUM, AVG, MIN, MAX |\n| Single table FROM | Yes | Note, aliasing not supported |\n| WHERE clause | Yes | Filters, comparisons, equality, etc |\n| JOINs | No | No table joins |\n| Array filtering | No | No array type support |\n| JSON filtering | No | No nested object queries |\n| Simple LIMIT | Yes | 1-10,000 range, no pagination support |\n| ORDER BY | Yes | Partition key or with GROUP BY columns |\n| GROUP BY | Yes | Supported |\n| HAVING | Yes | Supported |\n\n## Supported SQL Clauses\n\nR2 SQL supports: `DESCRIBE`, `SHOW`, `SELECT`, `FROM`, `WHERE`, `GROUP BY`, `HAVING`, `ORDER BY`, and `LIMIT`. New features will be released in the future, keep an eye on this page for the latest.\n\n***\n\n## SELECT Clause\n\n### Supported Features\n\n* **Individual columns**: `SELECT column1, column2`\n* **All columns**: `SELECT *`\n\n### Limitations\n\n* **No JSON field querying**: Cannot query individual fields from JSON objects\n* **Limited aggregation functions**: See Aggregation Functions section below for details\n* **No synthetic data**: Cannot create synthetic columns like `SELECT 1 AS what, \"hello\" AS greeting`\n* **No field aliasing**: `SELECT field AS another_name` (applies to both regular columns and aggregations)\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## Aggregation Functions\n\n### Supported Features\n\n* **COUNT(\\*)**: Count total rows **note**: only `*` is supported\n* **SUM(column)**: Sum numeric values\n* **AVG(column)**: Calculate average of numeric values\n* **MIN(column)**: Find minimum value\n* **MAX(column)**: Find maximum value\n* **With GROUP BY**: All aggregations work with `GROUP BY`\n\n### Limitations\n\n* **No aliases**: `AS` keyword not supported (`SELECT COUNT(*) AS total` fails)\n* **COUNT(\\*) only**: `COUNT(column_name)` or `COUNT(DISTINCT column)` is not supported\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## GROUP BY Clause\n\n### Supported Features\n\n* **Single column grouping**: `GROUP BY column`\n* **Multiple column grouping**: `GROUP BY column1, column2`\n* **With WHERE**: Filter before grouping\n* **With LIMIT**: Limit grouped results\n\n### Limitations\n\n* **No expressions**: Cannot use expressions in GROUP BY (e.g., `GROUP BY YEAR(date)`)\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## HAVING Clause\n\n### Supported Features\n\n* **With COUNT(\\*)**: Filter groups by count\n* **Comparison operators**: `>`, `>=`, `=`, `<`, `<=`, `!=`, `BETWEEN`, `AND`, `IS NOT NULL`\n* **With GROUP BY**: Must be used with GROUP BY\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## FROM Clause\n\n### Supported Features\n\n* **Single table queries**: `SELECT * FROM table_name`\n\n### Limitations\n\n* **No multiple tables**: Cannot specify multiple tables in FROM clause\n* **No subqueries**: `SELECT ... FROM (SELECT ...)` is not supported\n* **No JOINs**: No INNER, LEFT, RIGHT, or FULL JOINs\n* **No SQL functions**: Cannot use functions like `read_parquet()`\n* **No synthetic tables**: Cannot create tables from values\n* **No schema evolution**: Schema cannot be altered (no ALTER TABLE, migrations)\n* **Immutable datasets**: No UPDATE or DELETE operations allowed\n* **Fully defined schema**: Dynamic or union-type fields are not supported\n* **No table aliasing**: `SELECT * FROM table_name AS alias`\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## WHERE Clause\n\n### Supported Features\n\n* **Simple type filtering**: Supports `string`, `boolean`, `number` types, and timestamps expressed as RFC3339\n* **Boolean logic**: Supports `AND`, `OR`, `NOT` operators\n* **Comparison operators**: `>`, `>=`, `=`, `<`, `<=`, `!=`\n* **Grouped conditions**: `WHERE col_a=\"hello\" AND (col_b>5 OR col_c != 3)`\n* **Pattern matching:** `WHERE col_a LIKE â€˜hello w%â€™` (prefix matching only)\n* **NULL Handling :** `WHERE col_a IS NOT NULL` (`IS`/`IS NOT`)\n\n### Limitations\n\n* **No column-to-column comparisons**: Cannot use `WHERE col_a = col_b`\n* **No array filtering**: Cannot filter on array types (array\\[number], array\\[string], array\\[boolean])\n* **No JSON/object filtering**: Cannot filter on fields inside nested objects or JSON\n* **No SQL functions**: No function calls in WHERE clause\n* **No arithmetic operators**: Cannot use `+`, `-`, `*`, `/` in conditions\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## ORDER BY Clause\n\n### Supported Features\n\n* **ASC**: Ascending order\n* **DESC**: Descending order (Default, on full partition key)\n* **With partition key**: Order by partition key columns\n* **With GROUP BY**: Can order by all aggregation columns\n\n### Limitations\n\n* **Non-partition keys not supported**: `ORDER BY` on columns other than the partition key is not supported (except with aggregations)\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## LIMIT Clause\n\n### Supported Features\n\n* **Simple limits**: `LIMIT number`\n* **Range**: Minimum 1, maximum 10,000\n\n### Limitations\n\n* **No pagination**: `LIMIT offset, count` syntax not supported\n* **No SQL functions**: Cannot use functions to determine limit\n* **No arithmetic**: Cannot use expressions like `LIMIT 10 * 50`\n\n### Examples",
      "language": "unknown"
    },
    {
      "code": "***\n\n## Unsupported SQL Clauses\n\nThe following SQL clauses are **not supported**:\n\n* `UNION`/`INTERSECT`/`EXCEPT`\n* `WITH` (Common Table Expressions)\n* `WINDOW` functions\n* `INSERT`/`UPDATE`/`DELETE`\n* `CREATE`/`ALTER`/`DROP`\n\n***\n\n## Best Practices\n\n1. Always include time filters in your WHERE clause to ensure efficient queries.\n2. Use specific column selection instead of `SELECT *` when possible for better performance.\n3. Flatten your data to avoid nested JSON objects if you need to filter on those fields.\n4. Use `COUNT(*)` exclusively - avoid `COUNT(column_name)` or `COUNT(DISTINCT column)`.\n5. Enable compaction in R2 Data Catalog to reduce the number of data files needed to be scanned.\n\n***\n\n</page>\n\n<page>\n---\ntitle: Wrangler commands Â· R2 SQL docs\ndescription: Execute SQL query against R2 Data Catalog\nlastUpdated: 2025-11-17T17:45:17.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2-sql/reference/wrangler-commands/\n  md: https://developers.cloudflare.com/r2-sql/reference/wrangler-commands/index.md\n---\n\nNote\n\nR2 SQL is currently in open beta. Report R2 SQL bugs in [GitHub](https://github.com/cloudflare/workers-sdk/issues/new/choose). R2 SQL expects there to be a [`WRANGLER_R2_SQL_AUTH_TOKEN`](https://developers.cloudflare.com/r2-sql/query-data/#authentication) environment variable to be set.\n\n### `r2 sql query`\n\nExecute SQL query against R2 Data Catalog\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "- `[WAREHOUSE]` string required\n\n  R2 Data Catalog warehouse name\n\n- `[QUERY]` string required\n\n  The SQL query to execute\n\nGlobal flags\n\n* `--v` boolean alias: --version\n\n  Show version number\n\n* `--cwd` string\n\n  Run as if Wrangler was started in the specified directory instead of the current working directory\n\n* `--config` string alias: --c\n\n  Path to Wrangler configuration file\n\n* `--env` string alias: --e\n\n  Environment to use for operations, and for selecting .env and .dev.vars files\n\n* `--env-file` string\n\n  Path to an .env file to load - can be specified multiple times - values from earlier files are overridden by values in later files\n\n* `--experimental-provision` boolean aliases: --x-provision default: true\n\n  Experimental: Enable automatic resource provisioning\n\n* `--experimental-auto-create` boolean alias: --x-auto-create default: true\n\n  Automatically provision draft bindings with new resources\n\n</page>\n\n<page>\n---\ntitle: Build an end to end data pipeline Â· R2 SQL docs\ndescription: This tutorial demonstrates how to build a complete data pipeline\n  using Cloudflare Pipelines, R2 Data Catalog, and R2 SQL.\nlastUpdated: 2025-11-17T14:08:01.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2-sql/tutorials/end-to-end-pipeline/\n  md: https://developers.cloudflare.com/r2-sql/tutorials/end-to-end-pipeline/index.md\n---\n\nIn this tutorial, you will learn how to build a complete data pipeline using Cloudflare Pipelines, R2 Data Catalog, and R2 SQL. This also includes a sample Python script that creates and sends financial transaction data to your Pipeline that can be queried by R2 SQL or any Apache Iceberg-compatible query engine.\n\nThis tutorial demonstrates how to:\n\n* Set up R2 Data Catalog to store our transaction events in an Apache Iceberg table\n* Set up a Cloudflare Pipeline\n* Create transaction data with fraud patterns to send to your Pipeline\n* Query your data using R2 SQL for fraud analysis\n\n## Prerequisites\n\n1. Sign up for a [Cloudflare account](https://dash.cloudflare.com/sign-up).\n2. Install [Node.js](https://nodejs.org/en/).\n3. Install [Python 3.8+](https://python.org) for the data generation script.\n\nNode.js version manager\n\nUse a Node version manager like [Volta](https://volta.sh/) or [nvm](https://github.com/nvm-sh/nvm) to avoid permission issues and change Node.js versions.\n\nWrangler requires a Node version of 16.17.0 or later.\n\n## 1. Set up authentication\n\nYou will need API tokens to interact with Cloudflare services.\n\n1. In the Cloudflare dashboard, go to the **API tokens** page.\n\n   [Go to **Account API tokens**](https://dash.cloudflare.com/?to=/:account/api-tokens)\n\n2. Select **Create Token**.\n\n3. Select **Get started** next to Create Custom Token.\n\n4. Enter a name for your API token.\n\n5. Under **Permissions**, choose:\n\n   * **Workers Pipelines** with Read, Send, and Edit permissions\n   * **Workers R2 Data Catalog** with Read and Edit permissions\n   * **Workers R2 SQL** with Read permissions\n   * **Workers R2 Storage** with Read and Edit permissions\n\n6. Optionally, add a TTL to this token.\n\n7. Select **Continue to summary**.\n\n8. Click **Create Token**\n\n9. Note the **Token value**.\n\nExport your new token as an environment variable:",
      "language": "unknown"
    },
    {
      "code": "If this is your first time using Wrangler, make sure to log in.",
      "language": "unknown"
    },
    {
      "code": "## 2. Create an R2 bucket and enable R2 Data Catalog\n\n* Wrangler CLI\n\n  Create an R2 bucket:",
      "language": "unknown"
    },
    {
      "code": "* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select **Create bucket**.\n\n  3. Enter the bucket name: `fraud-pipeline`\n\n  4. Select **Create bucket**.\n\nEnable the catalog on your R2 bucket:\n\n* Wrangler CLI",
      "language": "unknown"
    },
    {
      "code": "When you run this command, take note of the \"Warehouse\" and \"Catalog URI\". You will need these later.\n\n* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket: `fraud-pipeline`.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, and select **Enable**.\n\n  4. Once enabled, note the **Catalog URI** and **Warehouse name**.\n\nNote\n\nCopy the `warehouse` (ACCOUNTID\\_BUCKETNAME) and paste it in the `export` below. We will use it later in the tutorial.",
      "language": "unknown"
    },
    {
      "code": "### (Optional) Enable compaction on your R2 Data Catalog\n\nR2 Data Catalog can automatically compact tables for you. In production event streaming use cases, it is common to end up with many small files, so it is recommended to enable compaction. Since the tutorial only demonstrates a sample use case, this step is optional.\n\n* Wrangler CLI",
      "language": "unknown"
    },
    {
      "code": "* Dashboard\n\n  1. In the Cloudflare dashboard, go to the **R2 object storage** page.\n\n     [Go to **Overview**](https://dash.cloudflare.com/?to=/:account/r2/overview)\n\n  2. Select the bucket: `fraud-pipeline`.\n\n  3. Switch to the **Settings** tab, scroll down to **R2 Data Catalog**, click on edit icon, and select **Enable**.\n\n  4. You can choose a target file size or leave the default. Click save.\n\n## 3. Set up the pipeline infrastructure\n\n### 3.1. Create the Pipeline stream\n\n* Wrangler CLI\n\n  First, create a schema file called `raw_transactions_schema.json` with the following `json` schema:",
      "language": "unknown"
    },
    {
      "code": "Create a stream to receive incoming fraud detection events:",
      "language": "unknown"
    },
    {
      "code": "Note\n\n  Note the **HTTP Ingest Endpoint URL** from the output. This is the endpoint you will use to send data to your pipeline.",
      "language": "unknown"
    },
    {
      "code": "The output should look like this:",
      "language": "unknown"
    },
    {
      "code": "### 3.2. Create the data sink\n\n  Create a sink that writes data to your R2 bucket as Apache Iceberg tables:",
      "language": "unknown"
    },
    {
      "code": "Note\n\n  This creates a `sink` configuration that will write to the Iceberg table `fraud_detection.transactions` in your R2 Data Catalog every 30 seconds. Pipelines automatically appends an `__ingest_ts` column that is used to partition the table by `DAY`.\n\n  ### 3.3. Create the pipeline\n\n  Connect your stream to your sink with SQL:",
      "language": "unknown"
    },
    {
      "code": "* Dashboard\n\n  1. In the Cloudflare dashboard, go to **Pipelines** > **Pipelines**.\n\n     [Go to **Pipelines**](https://dash.cloudflare.com/?to=/:account/pipelines/overview)\n\n  2. Select **Create Pipeline**.\n\n  3. **Connect to a Stream**:\n\n     * Pipeline name: `raw_events`\n     * Enable HTTP endpoint for sending data: Enabled\n     * HTTP authentication: Disabled (default)\n     * Select **Next**\n\n  4. **Define Input Schema**:\n\n     * Select **JSON editor**\n\n     * Copy in the schema:",
      "language": "unknown"
    },
    {
      "code": "* Select **Next**\n\n  5. **Define Sink**:\n\n     * Select your R2 bucket: `fraud-pipeline`\n     * Storage type: **R2 Data Catalog**\n     * Namespace: `fraud_detection`\n     * Table name: `transactions`\n     * **Advanced Settings**: Change **Maximum Time Interval** to `30 seconds`\n     * Select **Next**\n\n  6. **Credentials**:\n\n     * Disable **Automatically create an Account API token for your sink**\n     * Enter **Catalog Token** from step 1\n     * Select **Next**\n\n  7. **Pipeline Definition**:\n\n     * Leave the default SQL query:",
      "language": "unknown"
    },
    {
      "code": "* Select **Create Pipeline**\n\n  8. After pipeline creation, note the **Stream ID** for the next step.\n\n## 4. Generate sample fraud detection data\n\nCreate a Python script to generate realistic transaction data with fraud patterns:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Metrics",
      "id": "metrics"
    },
    {
      "level": "h3",
      "text": "Backlog",
      "id": "backlog"
    },
    {
      "level": "h3",
      "text": "Consumer concurrency",
      "id": "consumer-concurrency"
    },
    {
      "level": "h3",
      "text": "Message operations",
      "id": "message-operations"
    },
    {
      "level": "h2",
      "text": "Example GraphQL Queries",
      "id": "example-graphql-queries"
    },
    {
      "level": "h3",
      "text": "Get average queue backlog over time period",
      "id": "get-average-queue-backlog-over-time-period"
    },
    {
      "level": "h3",
      "text": "Get average consumer concurrency by hour",
      "id": "get-average-consumer-concurrency-by-hour"
    },
    {
      "level": "h3",
      "text": "Get message operations by minute",
      "id": "get-message-operations-by-minute"
    },
    {
      "level": "h2",
      "text": "Viewing audit logs",
      "id": "viewing-audit-logs"
    },
    {
      "level": "h2",
      "text": "Logged operations",
      "id": "logged-operations"
    },
    {
      "level": "h2",
      "text": "2025-04-17",
      "id": "2025-04-17"
    },
    {
      "level": "h2",
      "text": "2025-03-27",
      "id": "2025-03-27"
    },
    {
      "level": "h2",
      "text": "2025-02-14",
      "id": "2025-02-14"
    },
    {
      "level": "h2",
      "text": "2024-09-26",
      "id": "2024-09-26"
    },
    {
      "level": "h2",
      "text": "2024-03-26",
      "id": "2024-03-26"
    },
    {
      "level": "h2",
      "text": "2024-03-25",
      "id": "2024-03-25"
    },
    {
      "level": "h2",
      "text": "2024-03-18",
      "id": "2024-03-18"
    },
    {
      "level": "h2",
      "text": "2024-02-24",
      "id": "2024-02-24"
    },
    {
      "level": "h2",
      "text": "2023-10-07",
      "id": "2023-10-07"
    },
    {
      "level": "h2",
      "text": "2023-10-05",
      "id": "2023-10-05"
    },
    {
      "level": "h2",
      "text": "2023-03-28",
      "id": "2023-03-28"
    },
    {
      "level": "h2",
      "text": "2023-03-15",
      "id": "2023-03-15"
    },
    {
      "level": "h2",
      "text": "2023-03-02",
      "id": "2023-03-02"
    },
    {
      "level": "h2",
      "text": "2023-03-01",
      "id": "2023-03-01"
    },
    {
      "level": "h2",
      "text": "2022-12-13",
      "id": "2022-12-13"
    },
    {
      "level": "h2",
      "text": "2022-12-12",
      "id": "2022-12-12"
    },
    {
      "level": "h3",
      "text": "Increasing Queue Consumer Worker CPU Limits",
      "id": "increasing-queue-consumer-worker-cpu-limits"
    },
    {
      "level": "h2",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "What is a queue",
      "id": "what-is-a-queue"
    },
    {
      "level": "h2",
      "text": "Producers",
      "id": "producers"
    },
    {
      "level": "h3",
      "text": "Content types",
      "id": "content-types"
    },
    {
      "level": "h2",
      "text": "Consumers",
      "id": "consumers"
    },
    {
      "level": "h3",
      "text": "Create a consumer Worker",
      "id": "create-a-consumer-worker"
    },
    {
      "level": "h3",
      "text": "Remove a consumer",
      "id": "remove-a-consumer"
    },
    {
      "level": "h3",
      "text": "Pull consumers",
      "id": "pull-consumers"
    },
    {
      "level": "h2",
      "text": "Messages",
      "id": "messages"
    },
    {
      "level": "h2",
      "text": "`queues list`",
      "id": "`queues-list`"
    },
    {
      "level": "h2",
      "text": "`queues create`",
      "id": "`queues-create`"
    },
    {
      "level": "h2",
      "text": "`queues update`",
      "id": "`queues-update`"
    },
    {
      "level": "h2",
      "text": "`queues delete`",
      "id": "`queues-delete`"
    },
    {
      "level": "h2",
      "text": "`queues info`",
      "id": "`queues-info`"
    },
    {
      "level": "h2",
      "text": "`queues consumer add`",
      "id": "`queues-consumer-add`"
    },
    {
      "level": "h2",
      "text": "`queues consumer remove`",
      "id": "`queues-consumer-remove`"
    },
    {
      "level": "h2",
      "text": "`queues consumer http add`",
      "id": "`queues-consumer-http-add`"
    },
    {
      "level": "h2",
      "text": "`queues consumer http remove`",
      "id": "`queues-consumer-http-remove`"
    },
    {
      "level": "h2",
      "text": "`queues consumer worker add`",
      "id": "`queues-consumer-worker-add`"
    },
    {
      "level": "h2",
      "text": "`queues consumer worker remove`",
      "id": "`queues-consumer-worker-remove`"
    },
    {
      "level": "h2",
      "text": "`queues pause-delivery`",
      "id": "`queues-pause-delivery`"
    },
    {
      "level": "h2",
      "text": "`queues resume-delivery`",
      "id": "`queues-resume-delivery`"
    },
    {
      "level": "h2",
      "text": "`queues purge`",
      "id": "`queues-purge`"
    },
    {
      "level": "h2",
      "text": "`queues subscription create`",
      "id": "`queues-subscription-create`"
    },
    {
      "level": "h2",
      "text": "`queues subscription list`",
      "id": "`queues-subscription-list`"
    },
    {
      "level": "h2",
      "text": "`queues subscription get`",
      "id": "`queues-subscription-get`"
    },
    {
      "level": "h2",
      "text": "`queues subscription delete`",
      "id": "`queues-subscription-delete`"
    },
    {
      "level": "h2",
      "text": "`queues subscription update`",
      "id": "`queues-subscription-update`"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Create a new Workers application",
      "id": "1.-create-a-new-workers-application"
    },
    {
      "level": "h2",
      "text": "2. Set up a Queue",
      "id": "2.-set-up-a-queue"
    },
    {
      "level": "h3",
      "text": "Add Queue bindings to your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/)",
      "id": "add-queue-bindings-to-your-[wrangler-configuration-file](https://developers.cloudflare.com/workers/wrangler/configuration/)"
    },
    {
      "level": "h2",
      "text": "3. Add bindings to environment",
      "id": "3.-add-bindings-to-environment"
    },
    {
      "level": "h2",
      "text": "4. Send message to the queue",
      "id": "4.-send-message-to-the-queue"
    },
    {
      "level": "h2",
      "text": "5. Process the messages in the queue",
      "id": "5.-process-the-messages-in-the-queue"
    },
    {
      "level": "h2",
      "text": "6. Set up Resend",
      "id": "6.-set-up-resend"
    },
    {
      "level": "h2",
      "text": "7. Send email with Resend",
      "id": "7.-send-email-with-resend"
    },
    {
      "level": "h2",
      "text": "8. Deploy your Worker",
      "id": "8.-deploy-your-worker"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Create new Workers application",
      "id": "1.-create-new-workers-application"
    },
    {
      "level": "h2",
      "text": "2. Create KV namespace",
      "id": "2.-create-kv-namespace"
    },
    {
      "level": "h3",
      "text": "Add KV bindings to the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/)",
      "id": "add-kv-bindings-to-the-[wrangler-configuration-file](https://developers.cloudflare.com/workers/wrangler/configuration/)"
    },
    {
      "level": "h2",
      "text": "3. Set up Browser Rendering",
      "id": "3.-set-up-browser-rendering"
    },
    {
      "level": "h2",
      "text": "4. Set up a Queue",
      "id": "4.-set-up-a-queue"
    },
    {
      "level": "h3",
      "text": "Add Queue bindings to wrangler.toml",
      "id": "add-queue-bindings-to-wrangler.toml"
    },
    {
      "level": "h2",
      "text": "5. Add bindings to environment",
      "id": "5.-add-bindings-to-environment"
    },
    {
      "level": "h2",
      "text": "6. Submit links to crawl",
      "id": "6.-submit-links-to-crawl"
    },
    {
      "level": "h2",
      "text": "7. Crawl with Puppeteer",
      "id": "7.-crawl-with-puppeteer"
    },
    {
      "level": "h2",
      "text": "8. Deploy your Worker",
      "id": "8.-deploy-your-worker"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Method",
      "id": "method"
    },
    {
      "level": "h2",
      "text": "Method",
      "id": "method"
    },
    {
      "level": "h2",
      "text": "Download a chart in PNG format",
      "id": "download-a-chart-in-png-format"
    },
    {
      "level": "h2",
      "text": "Embed an interactive chart in your website",
      "id": "embed-an-interactive-chart-in-your-website"
    },
    {
      "level": "h2",
      "text": "Example using cURL",
      "id": "example-using-curl"
    },
    {
      "level": "h2",
      "text": "Use Python",
      "id": "use-python"
    },
    {
      "level": "h3",
      "text": "Notebooks",
      "id": "notebooks"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "Compare locations",
      "id": "compare-locations"
    },
    {
      "level": "h2",
      "text": "Compare date ranges",
      "id": "compare-date-ranges"
    },
    {
      "level": "h3",
      "text": "Use specific timestamps",
      "id": "use-specific-timestamps"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "List of endpoints",
      "id": "list-of-endpoints"
    },
    {
      "level": "h3",
      "text": "Timeseries",
      "id": "timeseries"
    },
    {
      "level": "h3",
      "text": "Summary",
      "id": "summary"
    },
    {
      "level": "h3",
      "text": "Top",
      "id": "top"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "Search BGP hijack events",
      "id": "search-bgp-hijack-events"
    },
    {
      "level": "h2",
      "text": "Search BGP route leak events",
      "id": "search-bgp-route-leak-events"
    },
    {
      "level": "h2",
      "text": "Send alerts for BGP hijacks",
      "id": "send-alerts-for-bgp-hijacks"
    },
    {
      "level": "h3",
      "text": "Worker app setup",
      "id": "worker-app-setup"
    },
    {
      "level": "h3",
      "text": "Fetch for newly detected BGP hijacks",
      "id": "fetch-for-newly-detected-bgp-hijacks"
    },
    {
      "level": "h3",
      "text": "Send alerts using webhook",
      "id": "send-alerts-using-webhook"
    },
    {
      "level": "h3",
      "text": "Send email alerts from Workers",
      "id": "send-email-alerts-from-workers"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "List of endpoints",
      "id": "list-of-endpoints"
    },
    {
      "level": "h3",
      "text": "Top locations",
      "id": "top-locations"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "List of endpoints",
      "id": "list-of-endpoints"
    },
    {
      "level": "h3",
      "text": "Top",
      "id": "top"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "List of endpoints",
      "id": "list-of-endpoints"
    },
    {
      "level": "h3",
      "text": "Timeseries",
      "id": "timeseries"
    },
    {
      "level": "h3",
      "text": "Summary",
      "id": "summary"
    },
    {
      "level": "h3",
      "text": "Top",
      "id": "top"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "List of endpoints",
      "id": "list-of-endpoints"
    },
    {
      "level": "h3",
      "text": "Timeseries",
      "id": "timeseries"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "List of endpoints",
      "id": "list-of-endpoints"
    },
    {
      "level": "h3",
      "text": "Timeseries",
      "id": "timeseries"
    },
    {
      "level": "h3",
      "text": "Summary",
      "id": "summary"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "List of endpoints",
      "id": "list-of-endpoints"
    },
    {
      "level": "h3",
      "text": "Outages",
      "id": "outages"
    },
    {
      "level": "h2",
      "text": "Use the API",
      "id": "use-the-api"
    },
    {
      "level": "h3",
      "text": "Submit URL to scan",
      "id": "submit-url-to-scan"
    },
    {
      "level": "h3",
      "text": "Get scan report",
      "id": "get-scan-report"
    },
    {
      "level": "h3",
      "text": "Search scans",
      "id": "search-scans"
    },
    {
      "level": "h3",
      "text": "Security Center",
      "id": "security-center"
    },
    {
      "level": "h2",
      "text": "Methodologies",
      "id": "methodologies"
    },
    {
      "level": "h3",
      "text": "How we count the number of DDoS attacks",
      "id": "how-we-count-the-number-of-ddos-attacks"
    },
    {
      "level": "h3",
      "text": "How we calculate ransom DDoS attack insights",
      "id": "how-we-calculate-ransom-ddos-attack-insights"
    },
    {
      "level": "h3",
      "text": "How we calculate geographical and industry insights",
      "id": "how-we-calculate-geographical-and-industry-insights"
    },
    {
      "level": "h3",
      "text": "How we calculate attack characteristics",
      "id": "how-we-calculate-attack-characteristics"
    },
    {
      "level": "h2",
      "text": "Final remarks",
      "id": "final-remarks"
    },
    {
      "level": "h3",
      "text": "Countries as source or target of attacks",
      "id": "countries-as-source-or-target-of-attacks"
    },
    {
      "level": "h3",
      "text": "Excluded items due to insufficient data",
      "id": "excluded-items-due-to-insufficient-data"
    },
    {
      "level": "h3",
      "text": "Map chart coloring",
      "id": "map-chart-coloring"
    },
    {
      "level": "h2",
      "text": "Footnotes",
      "id": "footnotes"
    },
    {
      "level": "h2",
      "text": "Quick Reference",
      "id": "quick-reference"
    },
    {
      "level": "h2",
      "text": "Supported SQL Clauses",
      "id": "supported-sql-clauses"
    },
    {
      "level": "h2",
      "text": "SELECT Clause",
      "id": "select-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "Aggregation Functions",
      "id": "aggregation-functions"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "GROUP BY Clause",
      "id": "group-by-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "HAVING Clause",
      "id": "having-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "FROM Clause",
      "id": "from-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "WHERE Clause",
      "id": "where-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "ORDER BY Clause",
      "id": "order-by-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "LIMIT Clause",
      "id": "limit-clause"
    },
    {
      "level": "h3",
      "text": "Supported Features",
      "id": "supported-features"
    },
    {
      "level": "h3",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "Unsupported SQL Clauses",
      "id": "unsupported-sql-clauses"
    },
    {
      "level": "h2",
      "text": "Best Practices",
      "id": "best-practices"
    },
    {
      "level": "h3",
      "text": "`r2 sql query`",
      "id": "`r2-sql-query`"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Set up authentication",
      "id": "1.-set-up-authentication"
    },
    {
      "level": "h2",
      "text": "2. Create an R2 bucket and enable R2 Data Catalog",
      "id": "2.-create-an-r2-bucket-and-enable-r2-data-catalog"
    },
    {
      "level": "h3",
      "text": "(Optional) Enable compaction on your R2 Data Catalog",
      "id": "(optional)-enable-compaction-on-your-r2-data-catalog"
    },
    {
      "level": "h2",
      "text": "3. Set up the pipeline infrastructure",
      "id": "3.-set-up-the-pipeline-infrastructure"
    },
    {
      "level": "h3",
      "text": "3.1. Create the Pipeline stream",
      "id": "3.1.-create-the-pipeline-stream"
    },
    {
      "level": "h2",
      "text": "4. Generate sample fraud detection data",
      "id": "4.-generate-sample-fraud-detection-data"
    }
  ],
  "url": "llms-txt#make-sure-to-replace-the-placeholder-with-your-shared-secret",
  "links": []
}