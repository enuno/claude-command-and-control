{
  "title": "llama-guard-3-8b",
  "content": "Text Generation • Meta\n\n@cf/meta/llama-guard-3-8b\n\nLlama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\n| Model Info | |\n| - | - |\n| Context Window[](https://developers.cloudflare.com/workers-ai/glossary/) | 131,072 tokens |\n| LoRA | Yes |\n| Unit Pricing | $0.48 per M input tokens, $0.03 per M output tokens |\n\nTry out this model with Workers AI LLM Playground. It does not require any setup or authentication and an instant way to preview and test a model directly in the browser.\n\n[Launch the LLM Playground](https://playground.ai.cloudflare.com/?model=@cf/meta/llama-guard-3-8b)\n\n\\* indicates a required field\n\n* `messages` array required\n\nAn array of message objects representing the conversation history.\n\nThe role of the message sender must alternate between 'user' and 'assistant'.\n\n* `content` string required\n\nThe content of the message as a string.\n\n* `max_tokens` integer default 256\n\nThe maximum number of tokens to generate in the response.\n\n* `temperature` number default 0.6 min 0 max 5\n\nControls the randomness of the output; higher values produce more random results.\n\n* `response_format` object\n\nDictate the output format of the generated response.\n\nSet to json\\_object to process and output generated text as JSON.\n\nThe generated text response from the model.\n\nThe json response parsed from the generated text response from the model.\n\nWhether the conversation is safe or not.\n\nA list of what hazard categories predicted for the conversation, if the conversation is deemed unsafe.\n\nHazard category classname, from S1 to S14.\n\nUsage statistics for the inference request\n\n* `prompt_tokens` number 0\n\nTotal number of tokens in input\n\n* `completion_tokens` number 0\n\nTotal number of tokens in output\n\n* `total_tokens` number 0\n\nTotal number of input and output tokens\n\nThe following schemas are based on JSON Schema\n\n<page>\n---\ntitle: llamaguard-7b-awq · Cloudflare Workers AI docs\ndescription: >\n  Llama Guard is a model for classifying the safety of LLM prompts and\n  responses, using a taxonomy of safety risks.\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/workers-ai/models/llamaguard-7b-awq/\n  md: https://developers.cloudflare.com/workers-ai/models/llamaguard-7b-awq/index.md\n---",
  "code_samples": [
    {
      "code": "export interface Env {\n  AI: Ai;\n}\n\n\nexport default {\n  async fetch(request, env): Promise<Response> {\n    const messages = [\n      {\n        role: 'user',\n        content: 'I wanna bully someone online',\n      },\n      {\n        role: 'assistant',\n        content: 'That sounds interesting, how can I help?',\n      },\n    ];\n    const response = await env.AI.run(\"@cf/meta/llama-guard-3-8b\", { messages });\n\n\n    return Response.json(response);\n  },\n} satisfies ExportedHandler<Env>;",
      "language": "ts"
    },
    {
      "code": "import os\nimport requests\n\n\nACCOUNT_ID = \"your-account-id\"\nAUTH_TOKEN = os.environ.get(\"CLOUDFLARE_AUTH_TOKEN\")\n\n\nresponse = requests.post(\n  f\"https://api.cloudflare.com/client/v4/accounts/{ACCOUNT_ID}/ai/run/@cf/meta/llama-guard-3-8b\",\n    headers={\"Authorization\": f\"Bearer {AUTH_TOKEN}\"},\n    json={\n      \"messages\": [\n        {\"role\": \"user\", \"content\": \"I want to bully somebody online\"},\n        {\"role\": \"assistant\", \"content\": \"Interesting. Let me know how I can be of assistance?\"},\n      ]\n    }\n)\nresult = response.json()\nprint(result)",
      "language": "py"
    },
    {
      "code": "curl https://api.cloudflare.com/client/v4/accounts/$CLOUDFLARE_ACCOUNT_ID/ai/run/@cf/meta/llama-guard-3-8b \\\n  -X POST \\\n  -H \"Authorization: Bearer $CLOUDFLARE_AUTH_TOKEN\" \\\n  -d '{ \"messages\": [{ \"role\": \"user\", \"content\": \"I want to bully someone online\" }, {\"role\": \"assistant\", \"content\": \"Interesting. How can I assist you?\"}]}'",
      "language": "sh"
    },
    {
      "code": "{\n      \"type\": \"object\",\n      \"properties\": {\n          \"messages\": {\n              \"type\": \"array\",\n              \"description\": \"An array of message objects representing the conversation history.\",\n              \"items\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                      \"role\": {\n                          \"enum\": [\n                              \"user\",\n                              \"assistant\"\n                          ],\n                          \"description\": \"The role of the message sender must alternate between 'user' and 'assistant'.\"\n                      },\n                      \"content\": {\n                          \"type\": \"string\",\n                          \"description\": \"The content of the message as a string.\"\n                      }\n                  },\n                  \"required\": [\n                      \"role\",\n                      \"content\"\n                  ]\n              }\n          },\n          \"max_tokens\": {\n              \"type\": \"integer\",\n              \"default\": 256,\n              \"description\": \"The maximum number of tokens to generate in the response.\"\n          },\n          \"temperature\": {\n              \"type\": \"number\",\n              \"default\": 0.6,\n              \"minimum\": 0,\n              \"maximum\": 5,\n              \"description\": \"Controls the randomness of the output; higher values produce more random results.\"\n          },\n          \"response_format\": {\n              \"type\": \"object\",\n              \"description\": \"Dictate the output format of the generated response.\",\n              \"properties\": {\n                  \"type\": {\n                      \"type\": \"string\",\n                      \"description\": \"Set to json_object to process and output generated text as JSON.\"\n                  }\n              }\n          }\n      },\n      \"required\": [\n          \"messages\"\n      ]\n  }",
      "language": "json"
    },
    {
      "code": "{\n      \"type\": \"object\",\n      \"contentType\": \"application/json\",\n      \"properties\": {\n          \"response\": {\n              \"oneOf\": [\n                  {\n                      \"type\": \"string\",\n                      \"description\": \"The generated text response from the model.\"\n                  },\n                  {\n                      \"type\": \"object\",\n                      \"description\": \"The json response parsed from the generated text response from the model.\",\n                      \"properties\": {\n                          \"safe\": {\n                              \"type\": \"boolean\",\n                              \"description\": \"Whether the conversation is safe or not.\"\n                          },\n                          \"categories\": {\n                              \"type\": \"array\",\n                              \"description\": \"A list of what hazard categories predicted for the conversation, if the conversation is deemed unsafe.\",\n                              \"items\": {\n                                  \"type\": \"string\",\n                                  \"description\": \"Hazard category classname, from S1 to S14.\"\n                              }\n                          }\n                      }\n                  }\n              ]\n          },\n          \"usage\": {\n              \"type\": \"object\",\n              \"description\": \"Usage statistics for the inference request\",\n              \"properties\": {\n                  \"prompt_tokens\": {\n                      \"type\": \"number\",\n                      \"description\": \"Total number of tokens in input\",\n                      \"default\": 0\n                  },\n                  \"completion_tokens\": {\n                      \"type\": \"number\",\n                      \"description\": \"Total number of tokens in output\",\n                      \"default\": 0\n                  },\n                  \"total_tokens\": {\n                      \"type\": \"number\",\n                      \"description\": \"Total number of input and output tokens\",\n                      \"default\": 0\n                  }\n              }\n          }\n      }\n  }",
      "language": "json"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Playground",
      "id": "playground"
    },
    {
      "level": "h2",
      "text": "Usage",
      "id": "usage"
    },
    {
      "level": "h2",
      "text": "Parameters",
      "id": "parameters"
    },
    {
      "level": "h3",
      "text": "Input",
      "id": "input"
    },
    {
      "level": "h3",
      "text": "Output",
      "id": "output"
    },
    {
      "level": "h2",
      "text": "API Schemas",
      "id": "api-schemas"
    }
  ],
  "url": "llms-txt#llama-guard-3-8b",
  "links": []
}