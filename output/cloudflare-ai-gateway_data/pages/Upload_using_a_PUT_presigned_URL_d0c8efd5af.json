{
  "title": "Upload using a PUT presigned URL",
  "content": "curl -X PUT \"https://my-bucket.<ACCOUNT_ID>.r2.cloudflarestorage.com/image.png?X-Amz-Algorithm=...\" \\\n  --data-binary @image.png\nplaintext\nhttps://my-bucket.123456789abcdef0123456789abcdef.r2.cloudflarestorage.com/photos/cat.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=CFEXAMPLEKEY12345%2F20251201%2Fauto%2Fs3%2Faws4_request&X-Amz-Date=20251201T180512Z&X-Amz-Expires=3600&X-Amz-Signature=8c3ac40fa6c83d64b4516e0c9e5fa94c998bb79131be9ddadf90cefc5ec31033&X-Amz-SignedHeaders=host&x-amz-checksum-mode=ENABLED&x-id=GetObject\ntxt\nPUT / HTTP/1.1\nHost: bucket.account.r2.cloudflarestorage.com\n<CreateBucketConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n   <LocationConstraint>auto</LocationConstraint>\n</CreateBucketConfiguration>\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"r2_buckets\": [\n      {\n        \"binding\": \"MY_BUCKET\",\n        \"bucket_name\": \"<YOUR_BUCKET_NAME>\"\n      }\n    ]\n  }\n  toml\n  [[r2_buckets]]\n  binding = 'MY_BUCKET' # <~ valid JavaScript variable name\n  bucket_name = '<YOUR_BUCKET_NAME>'\n  js\nexport default {\n  async fetch(request, env) {\n    const url = new URL(request.url);\n    const key = url.pathname.slice(1);\n\nswitch (request.method) {\n      case \"PUT\":\n        await env.MY_BUCKET.put(key, request.body);\n        return new Response(`Put ${key} successfully!`);\n\ndefault:\n        return new Response(`${request.method} is not allowed.`, {\n          status: 405,\n          headers: {\n            Allow: \"PUT\",\n          },\n        });\n    }\n  },\n};\njs\nconst options = {\n  limit: 500,\n  include: [\"customMetadata\"],\n};\n\nconst listed = await env.MY_BUCKET.list(options);\n\nlet truncated = listed.truncated;\nlet cursor = truncated ? listed.cursor : undefined;\n\n// ❌ - if your limit can't fit into a single response or your\n// bucket has less objects than the limit, it will get stuck here.\nwhile (listed.objects.length < options.limit) {\n  // ...\n}\n\n// ✅ - use the truncated property to check if there are more\n// objects to be returned\nwhile (truncated) {\n  const next = await env.MY_BUCKET.list({\n    ...options,\n    cursor: cursor,\n  });\n  listed.objects.push(...next.objects);\n\ntruncated = next.truncated;\n  cursor = next.cursor;\n}\njs\ninterface Env {\n  MY_BUCKET: R2Bucket;\n}\n\nexport default {\n  async fetch(\n    request,\n    env,\n    ctx\n  ): Promise<Response> {\n    const bucket = env.MY_BUCKET;\n\nconst url = new URL(request.url);\n    const key = url.pathname.slice(1);\n    const action = url.searchParams.get(\"action\");\n\nif (action === null) {\n      return new Response(\"Missing action type\", { status: 400 });\n    }\n\n// Route the request based on the HTTP method and action type\n    switch (request.method) {\n      case \"POST\":\n        switch (action) {\n          case \"mpu-create\": {\n            const multipartUpload = await bucket.createMultipartUpload(key);\n            return new Response(\n              JSON.stringify({\n                key: multipartUpload.key,\n                uploadId: multipartUpload.uploadId,\n              })\n            );\n          }\n          case \"mpu-complete\": {\n            const uploadId = url.searchParams.get(\"uploadId\");\n            if (uploadId === null) {\n              return new Response(\"Missing uploadId\", { status: 400 });\n            }\n\nconst multipartUpload = env.MY_BUCKET.resumeMultipartUpload(\n              key,\n              uploadId\n            );\n\ninterface completeBody {\n              parts: R2UploadedPart[];\n            }\n            const completeBody: completeBody = await request.json();\n            if (completeBody === null) {\n              return new Response(\"Missing or incomplete body\", {\n                status: 400,\n              });\n            }\n\n// Error handling in case the multipart upload does not exist anymore\n            try {\n              const object = await multipartUpload.complete(completeBody.parts);\n              return new Response(null, {\n                headers: {\n                  etag: object.httpEtag,\n                },\n              });\n            } catch (error: any) {\n              return new Response(error.message, { status: 400 });\n            }\n          }\n          default:\n            return new Response(`Unknown action ${action} for POST`, {\n              status: 400,\n            });\n        }\n      case \"PUT\":\n        switch (action) {\n          case \"mpu-uploadpart\": {\n            const uploadId = url.searchParams.get(\"uploadId\");\n            const partNumberString = url.searchParams.get(\"partNumber\");\n            if (partNumberString === null || uploadId === null) {\n              return new Response(\"Missing partNumber or uploadId\", {\n                status: 400,\n              });\n            }\n            if (request.body === null) {\n              return new Response(\"Missing request body\", { status: 400 });\n            }\n\nconst partNumber = parseInt(partNumberString);\n            const multipartUpload = env.MY_BUCKET.resumeMultipartUpload(\n              key,\n              uploadId\n            );\n            try {\n              const uploadedPart: R2UploadedPart =\n                await multipartUpload.uploadPart(partNumber, request.body);\n              return new Response(JSON.stringify(uploadedPart));\n            } catch (error: any) {\n              return new Response(error.message, { status: 400 });\n            }\n          }\n          default:\n            return new Response(`Unknown action ${action} for PUT`, {\n              status: 400,\n            });\n        }\n      case \"GET\":\n        if (action !== \"get\") {\n          return new Response(`Unknown action ${action} for GET`, {\n            status: 400,\n          });\n        }\n        const object = await env.MY_BUCKET.get(key);\n        if (object === null) {\n          return new Response(\"Object Not Found\", { status: 404 });\n        }\n        const headers = new Headers();\n        object.writeHttpMetadata(headers);\n        headers.set(\"etag\", object.httpEtag);\n        return new Response(object.body, { headers });\n      case \"DELETE\":\n        switch (action) {\n          case \"mpu-abort\": {\n            const uploadId = url.searchParams.get(\"uploadId\");\n            if (uploadId === null) {\n              return new Response(\"Missing uploadId\", { status: 400 });\n            }\n            const multipartUpload = env.MY_BUCKET.resumeMultipartUpload(\n              key,\n              uploadId\n            );\n\ntry {\n              multipartUpload.abort();\n            } catch (error: any) {\n              return new Response(error.message, { status: 400 });\n            }\n            return new Response(null, { status: 204 });\n          }\n          case \"delete\": {\n            await env.MY_BUCKET.delete(key);\n            return new Response(null, { status: 204 });\n          }\n          default:\n            return new Response(`Unknown action ${action} for DELETE`, {\n              status: 400,\n            });\n        }\n      default:\n        return new Response(\"Method Not Allowed\", {\n          status: 405,\n          headers: { Allow: \"PUT, POST, GET, DELETE\" },\n        });\n    }\n  },\n} satisfies ExportedHandler<Env>;\npython\nimport math\nimport os\nimport requests\nfrom requests.adapters import HTTPAdapter, Retry\nimport sys\nimport concurrent.futures",
  "code_samples": [
    {
      "code": "You can also use presigned URLs directly in web browsers, mobile apps, or any HTTP client. The same presigned URL can be reused multiple times until it expires.\n\n## Presigned URL example\n\nThe following is an example of a presigned URL that was created using R2 API credentials and following the AWS Signature Version 4 signing process:",
      "language": "unknown"
    },
    {
      "code": "In this example, this presigned url performs a `GetObject` on the object `photos/cat.png` within bucket `my-bucket` in the account with id `123456789abcdef0123456789abcdef`. The key signature parameters that compose this presigned URL are:\n\n* `X-Amz-Algorithm`: Identifies the algorithm used to sign the URL.\n* `X-Amz-Credential`: Contains information about the credentials used to calculate the signature.\n* `X-Amz-Date`: The date and time (in ISO 8601 format) when the signature was created.\n* `X-Amz-Expires`: The duration in seconds that the presigned URL remains valid, starting from `X-Amz-Date`.\n* `X-Amz-Signature`: The signature proving the URL was signed using the secret key.\n* `X-Amz-SignedHeaders`: Lists the HTTP headers that were included in the signature calculation.\n\nNote\n\nThe signature parameters (e.g. `X-Amz-Algorithm`, `X-Amz-Credential`, `X-Amz-Date`, `X-Amz-Expires`, `X-Amz-Signature`) cannot be tampered with. Attempting to modify the resource, operation, or expiry will result in a `403/SignatureDoesNotMatch` error.\n\n## Supported operations\n\nR2 supports presigned URLs for the following HTTP methods:\n\n* `GET`: Fetch an object from a bucket\n* `HEAD`: Fetch an object's metadata from a bucket\n* `PUT`: Upload an object to a bucket\n* `DELETE`: Delete an object from a bucket\n\n`POST` (multipart form uploads via HTML forms) is not currently supported.\n\n## Security considerations\n\nTreat presigned URLs as bearer tokens. Anyone with the URL can perform the specified operation until it expires. Share presigned URLs only with intended recipients and consider using short expiration times for sensitive operations.\n\n## Custom domains\n\nPresigned URLs work with the S3 API domain (`<ACCOUNT_ID>.r2.cloudflarestorage.com`) and cannot be used with custom domains.\n\nIf you need authentication with R2 buckets accessed via custom domains (public buckets), use the [WAF HMAC validation feature](https://developers.cloudflare.com/ruleset-engine/rules-language/functions/#hmac-validation) (requires Pro plan or above).\n\n## Related resources\n\n[R2 API tokens ](https://developers.cloudflare.com/r2/api/tokens/)Create credentials for generating presigned URLs.\n\n[Public buckets ](https://developers.cloudflare.com/r2/buckets/public-buckets/)Alternative approach for public read access without authentication.\n\n[R2 bindings in Workers ](https://developers.cloudflare.com/r2/api/workers/workers-api-usage/)Alternative for server-side R2 access with built-in authentication.\n\n[Storing user generated content ](https://developers.cloudflare.com/reference-architecture/diagrams/storage/storing-user-generated-content/)Architecture guide for handling user uploads with R2.\n\n</page>\n\n<page>\n---\ntitle: Extensions · Cloudflare R2 docs\ndescription: R2 implements some extensions on top of the basic S3 API. This page\n  outlines these additional, available features. Some of the functionality\n  described in this page requires setting a custom header. For examples on how\n  to do so, refer to Configure custom headers.\nlastUpdated: 2025-04-08T15:24:25.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/api/s3/extensions/\n  md: https://developers.cloudflare.com/r2/api/s3/extensions/index.md\n---\n\nR2 implements some extensions on top of the basic S3 API. This page outlines these additional, available features. Some of the functionality described in this page requires setting a custom header. For examples on how to do so, refer to [Configure custom headers](https://developers.cloudflare.com/r2/examples/aws/custom-header).\n\n## Extended metadata using Unicode\n\nThe [Workers R2 API](https://developers.cloudflare.com/r2/api/workers/workers-api-reference/) supports Unicode in keys and values natively without requiring any additional encoding or decoding for the `customMetadata` field. These fields map to the `x-amz-meta-`-prefixed headers used within the R2 S3-compatible API endpoint.\n\nHTTP header names and values may only contain ASCII characters, which is a small subset of the Unicode character library. To easily accommodate users, R2 adheres to [RFC 2047](https://datatracker.ietf.org/doc/html/rfc2047) and automatically decodes all `x-amz-meta-*` header values before storage. On retrieval, any metadata values with unicode are RFC 2047-encoded before rendering the response. The length limit for metadata values is applied to the decoded Unicode value.\n\nMetadata variance\n\nBe mindful when using both Workers and S3 API endpoints to access the same data. If the R2 metadata keys contain Unicode, they are stripped when accessed through the S3 API and the `x-amz-missing-meta` header is set to the number of keys that were omitted.\n\nThese headers map to the `httpMetadata` field in the [R2 bindings](https://developers.cloudflare.com/workers/runtime-apis/bindings/):\n\n| HTTP Header | Property Name |\n| - | - |\n| `Content-Encoding` | `httpMetadata.contentEncoding` |\n| `Content-Type` | `httpMetadata.contentType` |\n| `Content-Language` | `httpMetadata.contentLanguage` |\n| `Content-Disposition` | `httpMetadata.contentDisposition` |\n| `Cache-Control` | `httpMetadata.cacheControl` |\n| `Expires` | `httpMetadata.expires` |\n| | |\n\nIf using Unicode in object key names, refer to [Unicode Interoperability](https://developers.cloudflare.com/r2/reference/unicode-interoperability/).\n\n## Auto-creating buckets on upload\n\nIf you are creating buckets on demand, you might initiate an upload with the assumption that a target bucket exists. In this situation, if you received a `NoSuchBucket` error, you would probably issue a `CreateBucket` operation. However, following this approach can cause issues: if the body has already been partially consumed, the upload will need to be aborted. A common solution to this issue, followed by other object storage providers, is to use the [HTTP `100`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/100) response to detect whether the body should be sent, or if the bucket must be created before retrying the upload. However, Cloudflare does not support the HTTP `100` response. Even if the HTTP `100` response was supported, you would still have additional latency due to the round trips involved.\n\nTo support sending an upload with a streaming body to a bucket that may not exist yet, upload operations such as `PutObject` or `CreateMultipartUpload` allow you to specify a header that will ensure the `NoSuchBucket` error is not returned. If the bucket does not exist at the time of upload, it is implicitly instantiated with the following `CreateBucket` request:",
      "language": "unknown"
    },
    {
      "code": "This is only useful if you are creating buckets on demand because you do not know the name of the bucket or the preferred access location ahead of time. For example, you have one bucket per one of your customers and the bucket is created on first upload to the bucket and not during account registration. In these cases, the [`ListBuckets` extension](#listbuckets), which supports accounts with more than 1,000 buckets, may also be useful.\n\n## PutObject and CreateMultipartUpload\n\n### cf-create-bucket-if-missing\n\nAdd a `cf-create-bucket-if-missing` header with the value `true` to implicitly create the bucket if it does not exist yet. Refer to [Auto-creating buckets on upload](#auto-creating-buckets-on-upload) for a more detailed explanation of when to add this header.\n\n## PutObject\n\n### Conditional operations in `PutObject`\n\n`PutObject` supports [conditional uploads](https://developer.mozilla.org/en-US/docs/Web/HTTP/Conditional_requests) via the [`If-Match`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/If-Match), [`If-None-Match`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/If-None-Match), [`If-Modified-Since`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/If-Modified-Since), and [`If-Unmodified-Since`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/If-Unmodified-Since) headers. These headers will cause the `PutObject` operation to be rejected with `412 PreconditionFailed` error codes when the preceding state of the object that is being written to does not match the specified conditions.\n\n## CopyObject\n\n### MERGE metadata directive\n\nThe `x-amz-metadata-directive` allows a `MERGE` value, in addition to the standard `COPY` and `REPLACE` options. When used, `MERGE` is a combination of `COPY` and `REPLACE`, which will `COPY` any metadata keys from the source object and `REPLACE` those that are specified in the request with the new value. You cannot use `MERGE` to remove existing metadata keys from the source — use `REPLACE` instead.\n\n## `ListBuckets`\n\n`ListBuckets` supports all the same search parameters as `ListObjectsV2` in R2 because some customers may have more than 1,000 buckets. Because tooling, like existing S3 libraries, may not expose a way to set these search parameters, these values may also be sent in via headers. Values in headers take precedence over the search parameters.\n\n| Search parameter | HTTP Header | Meaning |\n| - | - | - |\n| `prefix` | `cf-prefix` | Show buckets with this prefix only. |\n| `start-after` | `cf-start-after` | Show buckets whose name appears lexicographically in the account. |\n| `continuation-token` | `cf-continuation-token` | Resume listing from a previously returned continuation token. |\n| `max-keys` | `cf-max-keys` | Return this maximum number of buckets. Default and max is `1000`. |\n| | | |\n\nThe XML response contains a `NextContinuationToken` and `IsTruncated` elements as appropriate. Since these may not be accessible from existing S3 APIs, these are also available in response headers:\n\n| XML Response Element | HTTP Response Header | Meaning |\n| - | - | - |\n| `IsTruncated` | `cf-is-truncated` | This is set to `true` if the list of buckets returned is not all the buckets on the account. |\n| `NextContinuationToken` | `cf-next-continuation-token` | This is set to continuation token to pass on a subsequent `ListBuckets` to resume the listing. |\n| `StartAfter` | | This is the start-after value that was passed in on the request. |\n| `KeyCount` | | The number of buckets returned. |\n| `ContinuationToken` | | The continuation token that was supplied in the request. |\n| `MaxKeys` | | The max keys that were specified in the request. |\n| | | |\n\n### Conditional operations in `CopyObject` for the destination object\n\nNote\n\nThis feature is currently in beta. If you have feedback, reach out to us on the [Cloudflare Developer Discord](https://discord.cloudflare.com) in the #r2-storage channel or open a thread on the [Community Forum](https://community.cloudflare.com/c/developers/storage/81).\n\n`CopyObject` already supports conditions that relate to the source object through the `x-amz-copy-source-if-...` headers as part of our compliance with the S3 API. In addition to this, R2 supports an R2 specific set of headers that allow the `CopyObject` operation to be conditional on the target object:\n\n* `cf-copy-destination-if-match`\n* `cf-copy-destination-if-none-match`\n* `cf-copy-destination-if-modified-since`\n* `cf-copy-destination-if-unmodified-since`\n\nThese headers work akin to the similarly named conditional headers supported on `PutObject`. When the preceding state of the destination object to does not match the specified conditions the `CopyObject` operation will be rejected with a `412 PreconditionFailed` error code.\n\n#### Non-atomicity relative to `x-amz-copy-source-if`\n\nThe `x-amz-copy-source-if-...` headers are guaranteed to be checked when the source object for the copy operation is selected, and the `cf-copy-destination-if-...` headers are guaranteed to be checked when the object is committed to the bucket state. However, the time at which the source object is selected for copying, and the point in time when the destination object is committed to the bucket state are not necessarily the same. This means that the `cf-copy-destination-if-...` headers are not atomic in relation to the `x-amz-copy-source-if...` headers.\n\n</page>\n\n<page>\n---\ntitle: Workers API reference · Cloudflare R2 docs\ndescription: The in-Worker R2 API is accessed by binding an R2 bucket to a\n  Worker. The Worker you write can expose external access to buckets via a route\n  or manipulate R2 objects internally.\nlastUpdated: 2025-01-29T12:28:42.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/api/workers/workers-api-reference/\n  md: https://developers.cloudflare.com/r2/api/workers/workers-api-reference/index.md\n---\n\nThe in-Worker R2 API is accessed by binding an R2 bucket to a [Worker](https://developers.cloudflare.com/workers). The Worker you write can expose external access to buckets via a route or manipulate R2 objects internally.\n\nThe R2 API includes some extensions and semantic differences from the S3 API. If you need S3 compatibility, consider using the [S3-compatible API](https://developers.cloudflare.com/r2/api/s3/).\n\n## Concepts\n\nR2 organizes the data you store, called objects, into containers, called buckets. Buckets are the fundamental unit of performance, scaling, and access within R2.\n\n## Create a binding\n\nBindings\n\nA binding is how your Worker interacts with external resources such as [KV Namespaces](https://developers.cloudflare.com/kv/concepts/kv-namespaces/), [Durable Objects](https://developers.cloudflare.com/durable-objects/), or [R2 Buckets](https://developers.cloudflare.com/r2/buckets/). A binding is a runtime variable that the Workers runtime provides to your code. You can declare a variable name in your Wrangler file that will be bound to these resources at runtime, and interact with them through this variable. Every binding's variable name and behavior is determined by you when deploying the Worker. Refer to [Environment Variables](https://developers.cloudflare.com/workers/configuration/environment-variables/) for more information.\n\nA binding is defined in the Wrangler file of your Worker project's directory.\n\nTo bind your R2 bucket to your Worker, add the following to your Wrangler file. Update the `binding` property to a valid JavaScript variable identifier and `bucket_name` to the name of your R2 bucket:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Within your Worker, your bucket binding is now available under the `MY_BUCKET` variable and you can begin interacting with it using the [bucket methods](#bucket-method-definitions) described below.\n\n## Bucket method definitions\n\nThe following methods are available on the bucket binding object injected into your code.\n\nFor example, to issue a `PUT` object request using the binding above:",
      "language": "unknown"
    },
    {
      "code": "* `head` (key: string): Promise\\<R2Object | null>\n\n  * Retrieves the `R2Object` for the given key containing only object metadata, if the key exists, and `null` if the key does not exist.\n\n* `get` (key: string, options?: R2GetOptions): Promise\\<R2ObjectBody | R2Object | null>\n\n  * Retrieves the `R2ObjectBody` for the given key containing object metadata and the object body as a `ReadableStream`, if the key exists, and `null` if the key does not exist.\n  * In the event that a precondition specified in `options` fails, `get()` returns an `R2Object` with `body` undefined.\n\n* `put` (key: string, value: ReadableStream | ArrayBuffer | ArrayBufferView | string | null | Blob, options?: R2PutOptions): Promise\\<R2Object | null>\n\n  * Stores the given `value` and metadata under the associated `key`. Once the write succeeds, returns an `R2Object` containing metadata about the stored Object.\n  * In the event that a precondition specified in `options` fails, `put()` returns `null`, and the object will not be stored.\n  * R2 writes are strongly consistent. Once the Promise resolves, all subsequent read operations will see this key value pair globally.\n\n* `delete` (key: string | string\\[]): Promise\\<void>\n\n  * Deletes the given `values` and metadata under the associated `keys`. Once the delete succeeds, returns `void`.\n  * R2 deletes are strongly consistent. Once the Promise resolves, all subsequent read operations will no longer see the provided key value pairs globally.\n  * Up to 1000 keys may be deleted per call.\n\n* `list` (options?: R2ListOptions): Promise\\<R2Objects>\n\n  * Returns an `R2Objects` containing a list of `R2Object` contained within the bucket.\n  * The returned list of objects is ordered lexicographically.\n  * Returns up to 1000 entries, but may return less in order to minimize memory pressure within the Worker.\n  * To explicitly set the number of objects to list, provide an [R2ListOptions](https://developers.cloudflare.com/r2/api/workers/workers-api-reference/#r2listoptions) object with the `limit` property set.\n\n- `createMultipartUpload` (key: string, options?: R2MultipartOptions): Promise\\<R2MultipartUpload>\n\n  * Creates a multipart upload.\n  * Returns Promise which resolves to an `R2MultipartUpload` object representing the newly created multipart upload. Once the multipart upload has been created, the multipart upload can be immediately interacted with globally, either through the Workers API, or through the S3 API.\n\n* `resumeMultipartUpload` (key: string, uploadId: string): R2MultipartUpload\n\n  * Returns an object representing a multipart upload with the given key and uploadId.\n  * The resumeMultipartUpload operation does not perform any checks to ensure the validity of the uploadId, nor does it verify the existence of a corresponding active multipart upload. This is done to minimize latency before being able to call subsequent operations on the `R2MultipartUpload` object.\n\n## `R2Object` definition\n\n`R2Object` is created when you `PUT` an object into an R2 bucket. `R2Object` represents the metadata of an object based on the information provided by the uploader. Every object that you `PUT` into an R2 bucket will have an `R2Object` created.\n\n* `key` string\n\n  * The object's key.\n\n* `version` string\n\n  * Random unique string associated with a specific upload of a key.\n\n* `size` number\n\n  * Size of the object in bytes.\n\n* `etag` string\n\nNote\n\nCloudflare recommends using the `httpEtag` field when returning an etag in a response header. This ensures the etag is quoted and conforms to [RFC 9110](https://www.rfc-editor.org/rfc/rfc9110#section-8.8.3).\n\n* The etag associated with the object upload.\n\n* `httpEtag` string\n\n  * The object's etag, in quotes so as to be returned as a header.\n\n* `uploaded` Date\n\n  * A Date object representing the time the object was uploaded.\n\n* `httpMetadata` R2HTTPMetadata\n\n  * Various HTTP headers associated with the object. Refer to [HTTP Metadata](#http-metadata).\n\n* `customMetadata` Record\\<string, string>\n\n  * A map of custom, user-defined metadata associated with the object.\n\n* `range` R2Range\n\n  * A `R2Range` object containing the returned range of the object.\n\n* `checksums` R2Checksums\n\n  * A `R2Checksums` object containing the stored checksums of the object. Refer to [checksums](#checksums).\n\n* `writeHttpMetadata` (headers: Headers): void\n\n  * Retrieves the `httpMetadata` from the `R2Object` and applies their corresponding HTTP headers to the `Headers` input object. Refer to [HTTP Metadata](#http-metadata).\n\n* `storageClass` 'Standard' | 'InfrequentAccess'\n\n  * The storage class associated with the object. Refer to [Storage Classes](#storage-class).\n\n* `ssecKeyMd5` string\n\n  * Hex-encoded MD5 hash of the [SSE-C](https://developers.cloudflare.com/r2/examples/ssec) key used for encryption (if one was provided). Hash can be used to identify which key is needed to decrypt object.\n\n## `R2ObjectBody` definition\n\n`R2ObjectBody` represents an object's metadata combined with its body. It is returned when you `GET` an object from an R2 bucket. The full list of keys for `R2ObjectBody` includes the list below and all keys inherited from [`R2Object`](#r2object-definition).\n\n* `body` ReadableStream\n\n  * The object's value.\n\n* `bodyUsed` boolean\n\n  * Whether the object's value has been consumed or not.\n\n* `arrayBuffer` (): Promise\\<ArrayBuffer>\n\n  * Returns a Promise that resolves to an `ArrayBuffer` containing the object's value.\n\n* `text` (): Promise\\<string>\n\n  * Returns a Promise that resolves to an string containing the object's value.\n\n* `json` \\<T>() : Promise\\<T>\n\n  * Returns a Promise that resolves to the given object containing the object's value.\n\n* `blob` (): Promise\\<Blob>\n\n  * Returns a Promise that resolves to a binary Blob containing the object's value.\n\n## `R2MultipartUpload` definition\n\nAn `R2MultipartUpload` object is created when you call `createMultipartUpload` or `resumeMultipartUpload`. `R2MultipartUpload` is a representation of an ongoing multipart upload.\n\nUncompleted multipart uploads will be automatically aborted after 7 days.\n\nNote\n\nAn `R2MultipartUpload` object does not guarantee that there is an active underlying multipart upload corresponding to that object.\n\nA multipart upload can be completed or aborted at any time, either through the S3 API, or by a parallel invocation of your Worker. Therefore it is important to add the necessary error handling code around each operation on a `R2MultipartUpload` object in case the underlying multipart upload no longer exists.\n\n* `key` string\n\n  * The `key` for the multipart upload.\n\n* `uploadId` string\n\n  * The `uploadId` for the multipart upload.\n\n* `uploadPart` (partNumber: number, value: ReadableStream | ArrayBuffer | ArrayBufferView | string | Blob, options?: R2MultipartOptions): Promise\\<R2UploadedPart>\n\n  * Uploads a single part with the specified part number to this multipart upload. Each part must be uniform in size with an exception for the final part which can be smaller.\n  * Returns an `R2UploadedPart` object containing the `etag` and `partNumber`. These `R2UploadedPart` objects are required when completing the multipart upload.\n\n* `abort` (): Promise\\<void>\n\n  * Aborts the multipart upload. Returns a Promise that resolves when the upload has been successfully aborted.\n\n* `complete` (uploadedParts: R2UploadedPart\\[]): Promise\\<R2Object>\n\n  * Completes the multipart upload with the given parts.\n  * Returns a Promise that resolves when the complete operation has finished. Once this happens, the object is immediately accessible globally by any subsequent read operation.\n\n## Method-specific types\n\n### R2GetOptions\n\n* `onlyIf` R2Conditional | Headers\n\n  * Specifies that the object should only be returned given satisfaction of certain conditions in the `R2Conditional` or in the conditional Headers. Refer to [Conditional operations](#conditional-operations).\n\n* `range` R2Range\n\n  * Specifies that only a specific length (from an optional offset) or suffix of bytes from the object should be returned. Refer to [Ranged reads](#ranged-reads).\n\n* `ssecKey` ArrayBuffer | string\n\n  * Specifies a key to be used for [SSE-C](https://developers.cloudflare.com/r2/examples/ssec). Key must be 32 bytes in length, in the form of a hex-encoded string or an ArrayBuffer.\n\n#### Ranged reads\n\n`R2GetOptions` accepts a `range` parameter, which can be used to restrict the data returned in `body`.\n\nThere are 3 variations of arguments that can be used in a range:\n\n* An offset with an optional length.\n\n* An optional offset with a length.\n\n* A suffix.\n\n* `offset` number\n\n  * The byte to begin returning data from, inclusive.\n\n* `length` number\n\n  * The number of bytes to return. If more bytes are requested than exist in the object, fewer bytes than this number may be returned.\n\n* `suffix` number\n\n  * The number of bytes to return from the end of the file, starting from the last byte. If more bytes are requested than exist in the object, fewer bytes than this number may be returned.\n\n### R2PutOptions\n\n* `onlyIf` R2Conditional | Headers\n\n  * Specifies that the object should only be stored given satisfaction of certain conditions in the `R2Conditional`. Refer to [Conditional operations](#conditional-operations).\n\n* `httpMetadata` R2HTTPMetadata | Headers optional\n\n  * Various HTTP headers associated with the object. Refer to [HTTP Metadata](#http-metadata).\n\n* `customMetadata` Record\\<string, string> optional\n\n  * A map of custom, user-defined metadata that will be stored with the object.\n\nNote\n\nOnly a single hashing algorithm can be specified at once.\n\n* `md5` ArrayBuffer | string optional\n\n  * A md5 hash to use to check the received object's integrity.\n\n* `sha1` ArrayBuffer | string optional\n\n  * A SHA-1 hash to use to check the received object's integrity.\n\n* `sha256` ArrayBuffer | string optional\n\n  * A SHA-256 hash to use to check the received object's integrity.\n\n* `sha384` ArrayBuffer | string optional\n\n  * A SHA-384 hash to use to check the received object's integrity.\n\n* `sha512` ArrayBuffer | string optional\n\n  * A SHA-512 hash to use to check the received object's integrity.\n\n* `storageClass` 'Standard' | 'InfrequentAccess'\n\n  * Sets the storage class of the object if provided. Otherwise, the object will be stored in the default storage class associated with the bucket. Refer to [Storage Classes](#storage-class).\n\n* `ssecKey` ArrayBuffer | string\n\n  * Specifies a key to be used for [SSE-C](https://developers.cloudflare.com/r2/examples/ssec). Key must be 32 bytes in length, in the form of a hex-encoded string or an ArrayBuffer.\n\n### R2MultipartOptions\n\n* `httpMetadata` R2HTTPMetadata | Headers optional\n\n  * Various HTTP headers associated with the object. Refer to [HTTP Metadata](#http-metadata).\n\n* `customMetadata` Record\\<string, string> optional\n\n  * A map of custom, user-defined metadata that will be stored with the object.\n\n* `storageClass` string\n\n  * Sets the storage class of the object if provided. Otherwise, the object will be stored in the default storage class associated with the bucket. Refer to [Storage Classes](#storage-class).\n\n* `ssecKey` ArrayBuffer | string\n\n  * Specifies a key to be used for [SSE-C](https://developers.cloudflare.com/r2/examples/ssec). Key must be 32 bytes in length, in the form of a hex-encoded string or an ArrayBuffer.\n\n### R2ListOptions\n\n* `limit` number optional\n\n  * The number of results to return. Defaults to `1000`, with a maximum of `1000`.\n\n  * If `include` is set, you may receive fewer than `limit` results in your response to accommodate metadata.\n\n* `prefix` string optional\n\n  * The prefix to match keys against. Keys will only be returned if they start with given prefix.\n\n* `cursor` string optional\n\n  * An opaque token that indicates where to continue listing objects from. A cursor can be retrieved from a previous list operation.\n\n* `delimiter` string optional\n\n  * The character to use when grouping keys.\n\n* `include` Array\\<string> optional\n\n  * Can include `httpMetadata` and/or `customMetadata`. If included, items returned by the list will include the specified metadata.\n\n  * Note that there is a limit on the total amount of data that a single `list` operation can return. If you request data, you may receive fewer than `limit` results in your response to accommodate metadata.\n\n  * The [compatibility date](https://developers.cloudflare.com/workers/configuration/compatibility-dates/) must be set to `2022-08-04` or later in your Wrangler file. If not, then the `r2_list_honor_include` compatibility flag must be set. Otherwise it is treated as `include: ['httpMetadata', 'customMetadata']` regardless of what the `include` option provided actually is.\n\n  This means applications must be careful to avoid comparing the amount of returned objects against your `limit`. Instead, use the `truncated` property to determine if the `list` request has more data to be returned.",
      "language": "unknown"
    },
    {
      "code": "### R2Objects\n\nAn object containing an `R2Object` array, returned by `BUCKET_BINDING.list()`.\n\n* `objects` Array\\<R2Object>\n\n  * An array of objects matching the `list` request.\n\n* `truncated` boolean\n\n  * If true, indicates there are more results to be retrieved for the current `list` request.\n\n* `cursor` string optional\n\n  * A token that can be passed to future `list` calls to resume listing from that point. Only present if truncated is true.\n\n* `delimitedPrefixes` Array\\<string>\n\n  * If a delimiter has been specified, contains all prefixes between the specified prefix and the next occurrence of the delimiter.\n\n  * For example, if no prefix is provided and the delimiter is '/', `foo/bar/baz` would return `foo` as a delimited prefix. If `foo/` was passed as a prefix with the same structure and delimiter, `foo/bar` would be returned as a delimited prefix.\n\n### Conditional operations\n\nYou can pass an `R2Conditional` object to `R2GetOptions` and `R2PutOptions`. If the condition check for `get()` fails, the body will not be returned. This will make `get()` have lower latency.\n\nIf the condition check for `put()` fails, `null` will be returned instead of the `R2Object`.\n\n* `etagMatches` string optional\n\n  * Performs the operation if the object's etag matches the given string.\n\n* `etagDoesNotMatch` string optional\n\n  * Performs the operation if the object's etag does not match the given string.\n\n* `uploadedBefore` Date optional\n\n  * Performs the operation if the object was uploaded before the given date.\n\n* `uploadedAfter` Date optional\n\n  * Performs the operation if the object was uploaded after the given date.\n\nAlternatively, you can pass a `Headers` object containing conditional headers to `R2GetOptions` and `R2PutOptions`. For information on these conditional headers, refer to [the MDN docs on conditional requests](https://developer.mozilla.org/en-US/docs/Web/HTTP/Conditional_requests#conditional_headers). All conditional headers aside from `If-Range` are supported.\n\nFor more specific information about conditional requests, refer to [RFC 7232](https://datatracker.ietf.org/doc/html/rfc7232).\n\n### HTTP Metadata\n\nGenerally, these fields match the HTTP metadata passed when the object was created. They can be overridden when issuing `GET` requests, in which case, the given values will be echoed back in the response.\n\n* `contentType` string optional\n\n* `contentLanguage` string optional\n\n* `contentDisposition` string optional\n\n* `contentEncoding` string optional\n\n* `cacheControl` string optional\n\n* `cacheExpiry` Date optional\n\n### Checksums\n\nIf a checksum was provided when using the `put()` binding, it will be available on the returned object under the `checksums` property. The MD5 checksum will be included by default for non-multipart objects.\n\n* `md5` ArrayBuffer optional\n\n  * The MD5 checksum of the object.\n\n* `sha1` ArrayBuffer optional\n\n  * The SHA-1 checksum of the object.\n\n* `sha256` ArrayBuffer optional\n\n  * The SHA-256 checksum of the object.\n\n* `sha384` ArrayBuffer optional\n\n  * The SHA-384 checksum of the object.\n\n* `sha512` ArrayBuffer optional\n\n  * The SHA-512 checksum of the object.\n\n### `R2UploadedPart`\n\nAn `R2UploadedPart` object represents a part that has been uploaded. `R2UploadedPart` objects are returned from `uploadPart` operations and must be passed to `completeMultipartUpload` operations.\n\n* `partNumber` number\n\n  * The number of the part.\n\n* `etag` string\n\n  * The `etag` of the part.\n\n### Storage Class\n\nThe storage class where an `R2Object` is stored. The available storage classes are `Standard` and `InfrequentAccess`. Refer to [Storage classes](https://developers.cloudflare.com/r2/buckets/storage-classes/) for more information.\n\n</page>\n\n<page>\n---\ntitle: Use the R2 multipart API from Workers · Cloudflare R2 docs\ndescription: >-\n  By following this guide, you will create a Worker through which your\n  applications can perform multipart uploads.\n\n  This example worker could serve as a basis for your own use case where you can\n  add authentication to the worker, or even add extra validation logic when\n  uploading each part.\n\n  This guide also contains an example Python application that uploads files to\n  this worker.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/r2/api/workers/workers-multipart-usage/\n  md: https://developers.cloudflare.com/r2/api/workers/workers-multipart-usage/index.md\n---\n\nBy following this guide, you will create a Worker through which your applications can perform multipart uploads. This example worker could serve as a basis for your own use case where you can add authentication to the worker, or even add extra validation logic when uploading each part. This guide also contains an example Python application that uploads files to this worker.\n\nThis guide assumes you have set up the [R2 binding](https://developers.cloudflare.com/workers/runtime-apis/bindings/) for your Worker. Refer to [Use R2 from Workers](https://developers.cloudflare.com/r2/api/workers/workers-api-usage) for instructions on setting up an R2 binding.\n\n## An example Worker using the multipart API\n\nThe following example Worker exposes an HTTP API which enables applications to use the multipart API through the Worker.\n\nIn this example, each request is routed based on the HTTP method and the action request parameter. As your Worker becomes more complicated, consider utilizing a serverless web framework such as [Hono](https://honojs.dev/) to handle the routing for you.\n\nThe following example Worker includes any new information about the state of the multipart upload in the response to each request. For the request which creates the multipart upload, the `uploadId` is returned. For requests uploading a part, the part number and `etag` are returned. In turn, the client keeps track of this state, and includes the uploadId in subsequent requests, and the `etag` and part number of each part when completing a multipart upload.\n\nAdd the following code to your project's `index.js` file and replace `MY_BUCKET` with your bucket's name:",
      "language": "unknown"
    },
    {
      "code": "After you have updated your Worker with the above code, run `npx wrangler deploy`.\n\nYou can now use this Worker to perform multipart uploads. You can either send requests from your existing application to this Worker to perform uploads or use a script to upload files through this Worker.\n\nThe next section is optional and shows an example of a Python script which uploads a chosen file on your machine to your Worker.\n\n## Perform a multipart upload with your Worker (optional)\n\nThis example application uploads a local file to the Worker in multiple parts. It uses Python's built-in `ThreadPoolExecutor` to parallelize the uploading of parts to the Worker, which increases upload speeds. HTTP requests to the Worker are made with the [requests](https://pypi.org/project/requests/) library.\n\nUtilizing the multipart API in this way also allows you to use your Worker to upload files larger than the [Workers request body size limit](https://developers.cloudflare.com/workers/platform/limits#request-limits). The uploading of individual parts is still subject to this limit.\n\nSave the following code in a file named `mpuscript.py` on your local machine. Change the `worker_endpoint variable` to where your worker is deployed. Pass the file you want to upload as an argument when running this script: `python3 mpuscript.py myfile`. This will upload the file `myfile` from your machine to your bucket through the Worker.",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Presigned URL example",
      "id": "presigned-url-example"
    },
    {
      "level": "h2",
      "text": "Supported operations",
      "id": "supported-operations"
    },
    {
      "level": "h2",
      "text": "Security considerations",
      "id": "security-considerations"
    },
    {
      "level": "h2",
      "text": "Custom domains",
      "id": "custom-domains"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Extended metadata using Unicode",
      "id": "extended-metadata-using-unicode"
    },
    {
      "level": "h2",
      "text": "Auto-creating buckets on upload",
      "id": "auto-creating-buckets-on-upload"
    },
    {
      "level": "h2",
      "text": "PutObject and CreateMultipartUpload",
      "id": "putobject-and-createmultipartupload"
    },
    {
      "level": "h3",
      "text": "cf-create-bucket-if-missing",
      "id": "cf-create-bucket-if-missing"
    },
    {
      "level": "h2",
      "text": "PutObject",
      "id": "putobject"
    },
    {
      "level": "h3",
      "text": "Conditional operations in `PutObject`",
      "id": "conditional-operations-in-`putobject`"
    },
    {
      "level": "h2",
      "text": "CopyObject",
      "id": "copyobject"
    },
    {
      "level": "h3",
      "text": "MERGE metadata directive",
      "id": "merge-metadata-directive"
    },
    {
      "level": "h2",
      "text": "`ListBuckets`",
      "id": "`listbuckets`"
    },
    {
      "level": "h3",
      "text": "Conditional operations in `CopyObject` for the destination object",
      "id": "conditional-operations-in-`copyobject`-for-the-destination-object"
    },
    {
      "level": "h2",
      "text": "Concepts",
      "id": "concepts"
    },
    {
      "level": "h2",
      "text": "Create a binding",
      "id": "create-a-binding"
    },
    {
      "level": "h2",
      "text": "Bucket method definitions",
      "id": "bucket-method-definitions"
    },
    {
      "level": "h2",
      "text": "`R2Object` definition",
      "id": "`r2object`-definition"
    },
    {
      "level": "h2",
      "text": "`R2ObjectBody` definition",
      "id": "`r2objectbody`-definition"
    },
    {
      "level": "h2",
      "text": "`R2MultipartUpload` definition",
      "id": "`r2multipartupload`-definition"
    },
    {
      "level": "h2",
      "text": "Method-specific types",
      "id": "method-specific-types"
    },
    {
      "level": "h3",
      "text": "R2GetOptions",
      "id": "r2getoptions"
    },
    {
      "level": "h3",
      "text": "R2PutOptions",
      "id": "r2putoptions"
    },
    {
      "level": "h3",
      "text": "R2MultipartOptions",
      "id": "r2multipartoptions"
    },
    {
      "level": "h3",
      "text": "R2ListOptions",
      "id": "r2listoptions"
    },
    {
      "level": "h3",
      "text": "R2Objects",
      "id": "r2objects"
    },
    {
      "level": "h3",
      "text": "Conditional operations",
      "id": "conditional-operations"
    },
    {
      "level": "h3",
      "text": "HTTP Metadata",
      "id": "http-metadata"
    },
    {
      "level": "h3",
      "text": "Checksums",
      "id": "checksums"
    },
    {
      "level": "h3",
      "text": "`R2UploadedPart`",
      "id": "`r2uploadedpart`"
    },
    {
      "level": "h3",
      "text": "Storage Class",
      "id": "storage-class"
    },
    {
      "level": "h2",
      "text": "An example Worker using the multipart API",
      "id": "an-example-worker-using-the-multipart-api"
    },
    {
      "level": "h2",
      "text": "Perform a multipart upload with your Worker (optional)",
      "id": "perform-a-multipart-upload-with-your-worker-(optional)"
    }
  ],
  "url": "llms-txt#upload-using-a-put-presigned-url",
  "links": []
}