{
  "title": "The gcloud CLI will replace any existing authorized networks with the list you provide here.",
  "content": "gcloud sql instances patch YOUR_INSTANCE_NAME --authorized-networks=\"0.0.0.0/0\"\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\ntxt\n    postgres://0191c898-...:4d7d8b45-...@eu-central-1.db.thenile.dev:5432/my_database\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/\nbash\nnpx create-db@latest\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\nbash\n     npx wrangler hyperdrive update <HYPERDRIVE_ID> --origin-connection-limit=10\n     bash\n     npx wrangler hyperdrive get <HYPERDRIVE_ID>\n     sql\nCREATE ROLE hyperdrive_user LOGIN PASSWORD 'sufficientlyRandomPassword';\n\n-- Here, you are granting it the postgres role. In practice, you want to create a role with lesser privileges.\nGRANT postgres to hyperdrive_user;\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\ntxt\npostgres://tsdbadmin:YOUR_PASSWORD_HERE@pn79dztyy0.xzhhbfensb.tsdb.cloud.timescale.com:31358/tsdb\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\ntxt\npostgres://USERNAME:PASSWORD@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\nsh\n     npx wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=\"postgres://user:password@HOSTNAME_OR_IP_ADDRESS:PORT/database_name\"\n     jsonc\n       {\n         \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n         \"name\": \"hyperdrive-example\",\n         \"main\": \"src/index.ts\",\n         \"compatibility_date\": \"2024-08-21\",\n         \"compatibility_flags\": [\n           \"nodejs_compat\"\n         ],\n         \"hyperdrive\": [\n           {\n             \"binding\": \"HYPERDRIVE\",\n             \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n           }\n         ]\n       }\n       toml\n       name = \"hyperdrive-example\"\n       main = \"src/index.ts\"\n       compatibility_date = \"2024-08-21\"\n       compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n       [[hyperdrive]]\n       binding = \"HYPERDRIVE\"\n       id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n       jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"hyperdrive-example\",\n    \"main\": \"src/index.ts\",\n    \"compatibility_date\": \"2024-08-21\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n      }\n    ]\n  }\n  toml\n  name = \"hyperdrive-example\"\n  main = \"src/index.ts\"\n  compatibility_date = \"2024-08-21\"\n  compatibility_flags = [\"nodejs_compat\"]\n\n# Pasted from the output of `wrangler hyperdrive create <NAME_OF_HYPERDRIVE_CONFIG> --connection-string=[...]` above.\n  [[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<ID OF THE CREATED HYPERDRIVE CONFIGURATION>\"\n  sh\n  npm i pg@>8.16.3\n  sh\n  yarn add pg@>8.16.3\n  sh\n  pnpm add pg@>8.16.3\n  sh\n  npm i -D @types/pg\n  sh\n  yarn add -D @types/pg\n  sh\n  pnpm add -D @types/pg\n  jsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"compatibility_flags\": [\n      \"nodejs_compat\"\n    ],\n    \"compatibility_date\": \"2024-09-23\",\n    \"hyperdrive\": [\n      {\n        \"binding\": \"HYPERDRIVE\",\n        \"id\": \"<your-hyperdrive-id-here>\"\n      }\n    ]\n  }\n  toml\n  # required for database drivers to function\n  compatibility_flags = [\"nodejs_compat\"]\n  compatibility_date = \"2024-09-23\"\n\n[[hyperdrive]]\n  binding = \"HYPERDRIVE\"\n  id = \"<your-hyperdrive-id-here>\"\n  ts\n// filepath: src/index.ts\nimport { Client } from \"pg\";\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext,\n  ): Promise<Response> {\n    // Create a new client instance for each request.\n    const client = new Client({\n      connectionString: env.HYPERDRIVE.connectionString,\n    });\n\ntry {\n      // Connect to the database\n      await client.connect();\n      console.log(\"Connected to PostgreSQL database\");\n\n// Perform a simple query\n      const result = await client.query(\"SELECT * FROM pg_tables\");\n\nreturn Response.json({\n        success: true,\n        result: result.rows,\n      });\n    } catch (error: any) {\n      console.error(\"Database error:\", error.message);\n\nnew Response(\"Internal error occurred\", { status: 500 });\n    }\n  },\n};\njson\n{\n  \"origin_steering\": {\n    \"policy\": \"least_outstanding_requests\"\n  }\n}\njson\n  // PUT /zones/:zone_id/load_balancers\n  {\n    \"description\": \"Load Balancer for www.example.com\",\n    \"name\": \"www.example.com\",\n    \"ttl\": 30,\n    \"proxied\": true,\n    \"fallback_pool\": \"ff02c959d17f7bb2b1184a202e3c0af7\",\n    \"default_pools\": [\n      \"17b5962d775c646f3f9725cbc7a53df4\",\n      \"ff02c959d17f7bb2b1184a202e3c0af7\"\n    ],\n    \"region_pools\": {\n      \"WNAM\": [\n        \"17b5962d775c646f3f9725cbc7a53df4\",\n        \"ff02c959d17f7bb2b1184a202e3c0af7\"\n      ],\n      \"ENAM\": [\n        \"17b5962d775c646f3f9725cbc7a53df4\",\n        \"ff02c959d17f7bb2b1184a202e3c0af7\"\n      ],\n      \"EEU\": [\n        \"ff02c959d17f7bb2b1184a202e3c0af7\",\n        \"17b5962d775c646f3f9725cbc7a53df4\"\n      ]\n    }\n  }\n  json\n{\n  \"steering_policy\": \"least_outstanding_requests\"\n}\njson\n{\n  \"Id\": \"<POLICY_ID>\",\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Stmt1506627150918\",\n      \"Action\": [\"s3:PutObject\"],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"arn:aws:s3:::burritobot/logs/*\",\n      \"Principal\": {\n        \"AWS\": [\"arn:aws:iam::391854517948:user/cloudflare-logpush\"]\n      }\n    }\n  ]\n}\nbash\n\"datadog://<DATADOG_ENDPOINT_URL>?header_DD-API-KEY=<DATADOG_API_KEY>&ddsource=cloudflare&service=<SERVICE>&host=<HOST>&ddtags=<TAGS>\"\nbash\ncurl \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/logpush/jobs\" \\\n  --request POST \\\n  --header \"Authorization: Bearer $CLOUDFLARE_API_TOKEN\" \\\n  --json '{\n    \"name\": \"<DOMAIN_NAME>\",\n    \"destination_conf\": \"datadog://<DATADOG_ENDPOINT_URL>?header_DD-API-KEY=<DATADOG_API_KEY>&ddsource=cloudflare&service=<SERVICE>&host=<HOST>&ddtags=<TAGS>\",\n    \"output_options\": {\n        \"field_names\": [\n            \"ClientIP\",\n            \"ClientRequestHost\",\n            \"ClientRequestMethod\",\n            \"ClientRequestURI\",\n            \"EdgeEndTimestamp\",\n            \"EdgeResponseBytes\",\n            \"EdgeResponseStatus\",\n            \"EdgeStartTimestamp\",\n            \"RayID\"\n        ],\n        \"timestamp_format\": \"rfc3339\"\n    },\n    \"dataset\": \"http_requests\"\n  }'\njson\n{\n  \"errors\": [],\n  \"messages\": [],\n  \"result\": {\n    \"id\": <JOB_ID>,\n    \"dataset\": \"http_requests\",\n    \"enabled\": false,\n    \"name\": \"<DOMAIN_NAME>\",\n    \"output_options\": {\n      \"field_names\": [\"ClientIP\", \"ClientRequestHost\", \"ClientRequestMethod\", \"ClientRequestURI\", \"EdgeEndTimestamp\", \"EdgeResponseBytes\", \"EdgeResponseStatus\" ,\"EdgeStartTimestamp\", \"RayID\"],\n      \"timestamp_format\": \"rfc3339\"\n    },\n    \"destination_conf\": \"datadog://<DATADOG_ENDPOINT_URL>?header_DD-API-KEY=<DATADOG_API_KEY>\",\n    \"last_complete\": null,\n    \"last_error\": null,\n    \"error_message\": null\n  },\n  \"success\": true\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/logpush/jobs/$JOB_ID\" \\\n  --request PUT \\\n  --header \"Authorization: Bearer $CLOUDFLARE_API_TOKEN\" \\\n  --json '{\n    \"enabled\": true\n  }'\njson\n{\n  \"errors\": [],\n  \"messages\": [],\n  \"result\": {\n    \"id\": <JOB_ID>,\n    \"dataset\": \"http_requests\",\n    \"enabled\": true,\n    \"name\": \"<DOMAIN_NAME>\",\n    \"output_options\": {\n      \"field_names\": [\"ClientIP\", \"ClientRequestHost\", \"ClientRequestMethod\", \"ClientRequestURI\", \"EdgeEndTimestamp\", \"EdgeResponseBytes\", \"EdgeResponseStatus\" ,\"EdgeStartTimestamp\", \"RayID\"],\n      \"timestamp_format\": \"rfc3339\"\n    },\n    \"destination_conf\": \"datadog://<DATADOG_ENDPOINT_URL>?header_DD-API-KEY=<DATADOG_API_KEY>\",\n    \"last_complete\": null,\n    \"last_error\": null,\n    \"error_message\": null\n  },\n  \"success\": true\n}\nbash\ncurl \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/logpush/jobs\" \\\n  --request POST \\\n  --header \"Authorization: Bearer $CLOUDFLARE_API_TOKEN\" \\\n  --json '{\n    \"name\": \"<PUBLIC_DOMAIN>\",\n    \"destination_conf\": \"https://<PUBLIC_DOMAIN>:<PUBLIC_PORT>?header_<SECRET_HEADER>=<SECRET_VALUE>\",\n    \"dataset\": \"http_requests\",\n    \"output_options\": {\n        \"field_names\": [\n            \"RayID\",\n            \"EdgeStartTimestamp\"\n        ],\n        \"timestamp_format\": \"rfc3339\"\n    }\n  }'\nbash\ncurl \"https://api.cloudflare.com/client/v4/zones/$ZONE_ID/settings/aegis\" \\\n  --request PATCH \\\n  --header \"Authorization: Bearer $CLOUDFLARE_API_TOKEN\" \\\n  --json '{\n    \"id\": \"aegis\",\n    \"value\": {\n        \"enabled\": true,\n        \"pool_id\": \"<YOUR_EGRESS_POOL_ID>\"\n    }\n  }'\ntxt\nhttps://logpush.yourdestinationendpoint.com?header_X-Logpush-Secret=YOUR_RANDOM_SECRET_TOKEN\ntxt\n  (http.host eq \"logpush.yourdestinationendpoint.com\" and all(http.request.headers[\"x-logpush-secret\"][*] ne \"YOUR_RANDOM_SECRET_TOKEN\"))\n  txt\n  (http.host eq \"logpush.yourdestinationendpoint.com\" and not ip.geoip.asnum in {13335 132892})\n  txt\nhttps://logpush.yourdestinationendpoint.com?header_CF-Access-Client-Id=YOUR_CLIENT_ID&header_CF-Access-Client-Secret=YOUR_CLIENT_SECRET\nbash\n$ curl https://logpush.yourdestinationendpoint.com",
  "code_samples": [
    {
      "code": "Refer to [Google Cloud's documentation](https://cloud.google.com/sql/docs/postgres/create-manage-users) for additional configuration options.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Materialize Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a Materialize streaming database.\nlastUpdated: 2025-08-20T20:59:04.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/materialize/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/materialize/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a [Materialize](https://materialize.com/) database. Materialize is a Postgres-compatible streaming database that can automatically compute real-time results against your streaming data sources.\n\n## 1. Allow Hyperdrive access\n\nTo allow Hyperdrive to connect to your database, you will need to ensure that Hyperdrive has valid user credentials and network access to your database.\n\n### Materialize Console\n\nNote\n\nRead the Materialize [Quickstart guide](https://materialize.com/docs/get-started/quickstart/) to set up your first database. The steps below assume you have an existing Materialize database ready to go.\n\nYou will need to create a new application user and password for Hyperdrive to connect with:\n\n1. Log in to the [Materialize Console](https://console.materialize.com/).\n2. Under the **App Passwords** section, select **Manage app passwords**.\n3. Select **New app password** and enter a name, for example, `hyperdrive-user`.\n4. Select **Create Password**.\n5. Copy the provided password: it will only be shown once.\n\nTo retrieve the hostname and database name of your Materialize configuration:\n\n1. Select **Connect** in the sidebar of the Materialize Console.\n2. Select **External tools**.\n3. Copy the **Host**, **Port** and **Database** settings.\n\nWith the username, app password, hostname, port and database name, you can now connect Hyperdrive to your Materialize database.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Neon Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a Neon Postgres database.\nlastUpdated: 2025-08-20T20:59:04.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/neon/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/neon/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a [Neon](https://neon.tech/) Postgres database.\n\n## 1. Allow Hyperdrive access\n\nYou can connect Hyperdrive to any existing Neon database by creating a new user and fetching your database connection string.\n\n### Neon Dashboard\n\n1. Go to the [**Neon dashboard**](https://console.neon.tech/app/projects) and select the project (database) you wish to connect to.\n2. Select **Roles** from the sidebar and select **New Role**. Enter `hyperdrive-user` as the name (or your preferred name) and **copy the password**. Note that the password will not be displayed again: you will have to reset it if you do not save it somewhere.\n3. Select **Dashboard** from the sidebar > go to the **Connection Details** pane > ensure you have selected the **branch**, **database** and **role** (for example,`hyperdrive-user`) that Hyperdrive will connect through.\n4. Select the `psql` and **uncheck the connection pooling** checkbox. Note down the connection string (starting with `postgres://hyperdrive-user@...`) from the text box.\n\nWith both the connection string and the password, you can now create a Hyperdrive database configuration.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nWhen connecting to a Neon database with Hyperdrive, you should use a driver like [node-postgres (pg)](https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/node-postgres/) or [Postgres.js](https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/postgres-js/) to connect directly to the underlying database instead of the [Neon serverless driver](https://neon.tech/docs/serverless/serverless-driver). Hyperdrive is optimized for database access for Workers and will perform global connection pooling and fast query routing by connecting directly to your database.\n\n## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Nile Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a Nile Postgres database instance.\nlastUpdated: 2025-08-20T20:59:04.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/nile/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/nile/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a [Nile](https://thenile.dev) PostgreSQL database instance.\n\nNile is PostgreSQL re-engineered for multi-tenant applications. Nile's virtual tenant databases provide you with isolation, placement, insight, and other features for your tenant's data and embedding. Refer to [Nile documentation](https://www.thenile.dev/docs/getting-started/whatisnile) to learn more.\n\n## 1. Allow Hyperdrive access\n\nYou can connect Cloudflare Hyperdrive to any Nile database in your workspace using its connection string - either with a new set of credentials, or using an existing set.\n\n### Nile console\n\nTo get a connection string from Nile console:\n\n1. Log in to [Nile console](https://console.thenile.dev), then select a database.\n2. On the left hand menu, click **Settings** (the bottom-most icon) and then select **Connection**.\n3. Select the PostgreSQL logo to show the connection string.\n4. Select \"Generate credentials\" to generate new credentials.\n5. Copy the connection string (without the \"psql\" part).\n\nYou will have obtained a connection string similar to the following:",
      "language": "unknown"
    },
    {
      "code": "With the connection string, you can now create a Hyperdrive database configuration.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: pgEdge Cloud Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a pgEdge Postgres database.\nlastUpdated: 2025-08-20T20:59:04.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/pgedge/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/pgedge/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a [pgEdge](https://pgedge.com/) Postgres database. pgEdge Cloud provides easy deployment of fully-managed, fully-distributed, and secure Postgres.\n\n## 1. Allow Hyperdrive access\n\nYou can connect Hyperdrive to any existing pgEdge database with the default user and password provided by pgEdge.\n\n### pgEdge dashboard\n\nTo retrieve your connection string from the pgEdge dashboard:\n\n1. Go to the [**pgEdge dashboard**](https://app.pgedge.com) and select the database you wish to connect to.\n2. From the **Connect to your database** section, note down the connection string (starting with `postgres://app@...`) from the **Connection String** text box.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: PlanetScale Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a PlanetScale PostgreSQL database.\nlastUpdated: 2025-09-05T14:33:32.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/planetscale-postgres/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/planetscale-postgres/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a [PlanetScale](https://planetscale.com/) PostgreSQL database.\n\n## 1. Allow Hyperdrive access\n\nYou can connect Hyperdrive to any existing PlanetScale PostgreSQL database by creating a new role (optional) and retrieving a connection string to your database.\n\n### PlanetScale Dashboard\n\n1. Go to the [**PlanetScale dashboard**](https://app.planetscale.com/) and select the database you wish to connect to.\n\n2. Click **Connect**.\n\n3. Create a new role for your Hyperdrive configuration (recommended):\n\n   1. Ensure the minimum required permissions for Hyperdrive to read and write data to your tables:\n\n      * **pg\\_read\\_all\\_data**: Read data from all tables, views, and sequences\n      * **pg\\_write\\_all\\_data**: Write data to all tables, views, and sequences\n\n   2. Click **Create role**.\n\n4. Note the user, the password, the database host, and the database name (or `postgres` as the default database). You will need these to create a database configuration in Hyperdrive.\n\nWith the host, database name, username and password, you can now create a Hyperdrive database configuration.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nWhen connecting to a PlanetScale PostgreSQL database with Hyperdrive, you should use a driver like [node-postgres (pg)](https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/node-postgres/) or [Postgres.js](https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/postgres-js/) to connect directly to the underlying database instead of the [PlanetScale serverless driver](https://planetscale.com/docs/tutorials/planetscale-serverless-driver). Hyperdrive is optimized for database access for Workers and will perform global connection pooling and fast query routing by connecting directly to your database.\n\n## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Prisma Postgres Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a Prisma Postgres database.\nlastUpdated: 2025-08-21T08:39:07.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/prisma-postgres/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/prisma-postgres/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a [Prisma Postgres](https://www.prisma.io/postgres) database.\n\n## 1. Allow Hyperdrive access\n\nYou can connect Hyperdrive to any existing Prisma Postgres database by using your existing database connection string.\n\n### Prisma Data Platform\n\n1. Go to the [**Prisma Data Platform Console**](https://console.prisma.io/) and select the project (database) you wish to connect to.\n2. Select **Connect to your database** > **Any client**.\n3. Select **Generate database credentials**. Copy the connection string for your Prisma Postgres database.\n4. Edit the connection string to make it compatible with Hyperdrive.\n\n* Add the database name after the port. You may remove any query parameters, such as `?sslmode=require`.\n* The final string will look like:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nAn alternative to the Prisma Data Platform is to use the [`create-db`](https://www.npmjs.com/package/create-db) package. This package will generate a quick temporary Prisma Postgres database for you to use.",
      "language": "unknown"
    },
    {
      "code": "With this connection string, you can now create a Hyperdrive database configuration.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "## 4. Configure Hyperdrive maximum connections\n\nPrisma Postgres has limits on the number of direct connections that can be made to the database using Hyperdrive. Refer to [Prisma Postgres limits](https://www.prisma.io/docs/postgres/database/direct-connections?utm_source=website\\&utm_medium=postgres-page#connection-limit).\n\nNote\n\nThere are two limits to consider here.\n\n* Origin database's connection limit, set by the origin database provider. This is the maximum number of direct database connections that can be made to the origin database.\n* Hyperdrive's origin connection limit, set by Hyperdrive. This is the maximum number of database connections that Hyperdrive can make to your origin database (in this case, Prisma Postgres).\n\nHyperdrive's origin connection limit should be lower than the Prisma Postgres connection limit, since Hyperdrive's origin connection limit is a soft limit, and Hyperdrive may create more connections if there are network disruptions that prevent existing connections from being used.\n\n* Dashboard\n\n  1. From the [Cloudflare Hyperdrive dashboard](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive), select your newly created Hyperdrive configuration.\n  2. Go to **Settings**.\n  3. In **Origin connection limit**, select **Edit Settings**, and set your maximum connections to a number that is lower than your Prisma connection limit.\n\n* Wrangler CLI\n\n  1. Edit your existing Hyperdrive configuration with the `--origin-connection-limit` parameter:",
      "language": "unknown"
    },
    {
      "code": "Replace `<HYPERDRIVE_ID>` with your Hyperdrive configuration ID and set the connection limit to a number that is less than your Prisma connection limit.\n\n  2. Verify the configuration change:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nWhen connecting to a Prisma Postgres database with Hyperdrive, you should use a driver like [node-postgres (pg)](https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/node-postgres/) or [Postgres.js](https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/postgres-js/) to connect directly to the underlying PostgreSQL database, instead of the Prisma Accelerate extension. Hyperdrive is optimized for database access for Workers, and connects directly to your database to perform global connection pooling and fast query routing.\n\n## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Supabase Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a Supabase Postgres database.\nlastUpdated: 2025-12-03T16:30:02.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/supabase/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/supabase/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a [Supabase](https://supabase.com/) Postgres database.\n\n## 1. Allow Hyperdrive access\n\nYou can connect Hyperdrive to any existing Supabase database as the Postgres user which is set up during project creation. Alternatively, to create a new user for Hyperdrive, run these commands in the [SQL Editor](https://supabase.com/dashboard/project/_/sql/new).",
      "language": "unknown"
    },
    {
      "code": "The database endpoint can be found in the [database settings page](https://supabase.com/dashboard/project/_/settings/database).\n\nWith a database user, password, database endpoint (hostname and port) and database name (default: postgres), you can now set up Hyperdrive.\n\nNote\n\nWhen connecting to Supabase from Hyperdrive, you should use the **Direct connection** connection string rather than the pooled connection strings. Hyperdrive will perform pooling of connections to ensure optimal access from Workers.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nWhen connecting to a Supabase database with Hyperdrive, you should use a driver like [node-postgres (pg)](https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/node-postgres/) or [Postgres.js](https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-drivers-and-libraries/postgres-js/) to connect directly to the underlying database instead of the [Supabase JavaScript client](https://github.com/supabase/supabase-js). Hyperdrive is optimized for database access for Workers and will perform global connection pooling and fast query routing by connecting directly to your database.\n\n## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Timescale Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a Timescale time-series database.\nlastUpdated: 2025-08-20T20:59:04.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/timescale/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/timescale/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a [Timescale](https://www.timescale.com/) time-series database. Timescale is built on PostgreSQL, and includes powerful time-series, event and analytics features.\n\nYou can learn more about Timescale by referring to their [Timescale services documentation](https://docs.timescale.com/getting-started/latest/services/).\n\n## 1. Allow Hyperdrive access\n\nYou can connect Hyperdrive to any existing Timescale database by creating a new user and fetching your database connection string.\n\n### Timescale Dashboard\n\nNote\n\nSimilar to most services, Timescale requires you to reset the password associated with your database user if you do not have it stored securely. You should ensure that you do not break any existing clients if when you reset the password.\n\nTo retrieve your credentials and database endpoint in the [Timescale Console](https://console.cloud.timescale.com/):\n\n1. Select the service (database) you want Hyperdrive to connect to.\n2. Expand **Connection info**.\n3. Copy the **Service URL**. The Service URL is the connection string that Hyperdrive will use to connect. This string includes the database hostname, port number and database name.\n\nIf you do not have your password stored, you will need to select **Forgot your password?** and set a new **SCRAM** password. Save this password, as Timescale will only display it once.\n\nYou will end up with a connection string resembling the below:",
      "language": "unknown"
    },
    {
      "code": "With the connection string, you can now create a Hyperdrive database configuration.\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Xata Â· Cloudflare Hyperdrive docs\ndescription: Connect Hyperdrive to a Xata database instance.\nlastUpdated: 2025-08-20T20:59:04.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/xata/\n  md: https://developers.cloudflare.com/hyperdrive/examples/connect-to-postgres/postgres-database-providers/xata/index.md\n---\n\nThis example shows you how to connect Hyperdrive to a Xata PostgreSQL database instance.\n\n## 1. Allow Hyperdrive access\n\nYou can connect Hyperdrive to any existing Xata PostgreSQL database with the connection string provided by Xata.\n\n### Xata dashboard\n\nTo retrieve your connection string from the Xata dashboard:\n\n1. Go to the [**Xata dashboard**](https://xata.io/).\n2. Select the database you want to connect to.\n3. Copy the `PostgreSQL` connection string.\n\nRefer to the full [Xata documentation](https://xata.io/documentation).\n\n## 2. Create a database configuration\n\nTo configure Hyperdrive, you will need:\n\n* The IP address (or hostname) and port of your database.\n* The database username (for example, `hyperdrive-demo`) you configured in a previous step.\n* The password associated with that username.\n* The name of the database you want Hyperdrive to connect to. For example, `postgres`.\n\nHyperdrive accepts the combination of these parameters in the common connection string format used by database drivers:",
      "language": "unknown"
    },
    {
      "code": "Most database providers will provide a connection string you can directly copy-and-paste directly into Hyperdrive.\n\n* Dashboard\n\n  To create a Hyperdrive configuration with the Cloudflare dashboard:\n\n  1. In the Cloudflare dashboard, go to the **Hyperdrive** page.\n\n     [Go to **Hyperdrive**](https://dash.cloudflare.com/?to=/:account/workers/hyperdrive)\n\n  2. Select **Create Configuration**.\n\n  3. Fill out the form, including the connection string.\n\n  4. Select **Create**.\n\n* Wrangler CLI\n\n  To create a Hyperdrive configuration with the [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/):\n\n  1. Open your terminal and run the following command. Replace `<NAME_OF_HYPERDRIVE_CONFIG>` with a name for your Hyperdrive configuration and paste the connection string provided from your database host, or replace `user`, `password`, `HOSTNAME_OR_IP_ADDRESS`, `port`, and `database_name` placeholders with those specific to your database:",
      "language": "unknown"
    },
    {
      "code": "2. This command outputs a binding for the [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/):\n\n     * wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Note\n\nHyperdrive will attempt to connect to your database with the provided credentials to verify they are correct before creating a configuration. If you encounter an error when attempting to connect, refer to Hyperdrive's [troubleshooting documentation](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug possible causes.\n\n## 3. Use Hyperdrive from your Worker\n\nInstall the `node-postgres` driver:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe minimum version of `node-postgres` required for Hyperdrive is `8.16.3`.\n\nIf using TypeScript, install the types package:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "Add the required Node.js compatibility flags and Hyperdrive binding to your `wrangler.jsonc` file:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Create a new `Client` instance and pass the Hyperdrive `connectionString`:",
      "language": "unknown"
    },
    {
      "code": "## Next steps\n\n* Learn more about [How Hyperdrive Works](https://developers.cloudflare.com/hyperdrive/concepts/how-hyperdrive-works/).\n* Refer to the [troubleshooting guide](https://developers.cloudflare.com/hyperdrive/observability/troubleshooting/) to debug common issues.\n* Understand more about other [storage options](https://developers.cloudflare.com/workers/platform/storage-options/) available to Cloudflare Workers.\n\n</page>\n\n<page>\n---\ntitle: Hash steering Â· Cloudflare Load Balancing docs\ndescription: Hash steering guides Cloudflare to send requests to endpoints based\n  on a combination of endpoint weights and previous requests from that IP\n  address. Ensures requests from the same IP address will hit the same endpoint,\n  but actual traffic distribution may differ from endpoint weights.\nlastUpdated: 2025-02-27T15:51:29.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/hash-origin-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/hash-origin-steering/index.md\n---\n\n**Hash steering** guides Cloudflare to send requests to endpoints based on a combination of [endpoint weights](https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/#weights) and previous requests from that IP address. Ensures requests from the same IP address will hit the same endpoint, but actual traffic distribution may differ from endpoint weights.\n\n## Limitation when using Workers\n\nHash Steering relies on the `x-forwarded-for` header to determine the originating IP address of a request. However, when a [Cloudflare Worker](https://developers.cloudflare.com/workers/) is used in front of a load balancer, this can affect how Hash Steering functions.\n\nWhen a request originates from a browser, it lacks an `x-forwarded-for` header, but if a Worker proxies the request to a load balancer, the header is populated with the Worker's IP instead of the original client IP. Since the Worker's IP â often a Cloudflare public IP â can change between requests, Hash Steering may direct the same client's requests to different endpoints, leading to inconsistent traffic routing.\n\n### Workaround\n\nTo ensure Hash Steering works correctly when using a Worker in front of a Load Balancer, manually set the `x-forwarded-for` header in the Worker to the client's original IP address. By manually setting `x-forwarded-for` to `CF-Connecting-IP`, Hash Steering will function as expected, ensuring traffic consistency for end users.\n\n</page>\n\n<page>\n---\ntitle: Least Outstanding Requests steering - Endpoint-level steering Â·\n  Cloudflare Load Balancing docs\ndescription: Least Outstanding Requests steering allows you to route traffic to\n  endpoints that currently have the lowest number of outstanding requests.\nlastUpdated: 2025-02-10T10:26:10.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/least-outstanding-requests-pools/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/least-outstanding-requests-pools/index.md\n---\n\n**Least Outstanding Requests steering** allows you to route traffic to endpoints that currently have the lowest number of outstanding requests.\n\nThis steering policy selects an endpoint by taking into consideration endpoint weights, as well as each endpoint's number of in-flight requests. Endpoints with more pending requests are weighted proportionately less in relation to others.\n\nLeast Outstanding Requests steering is best to use if your endpoints are easily overwhelmed by a spike in concurrent requests. It supports [adaptive routing](https://developers.cloudflare.com/load-balancing/understand-basics/adaptive-routing/) and [session affinity](https://developers.cloudflare.com/load-balancing/understand-basics/session-affinity/).\n\n## Configure via the API",
      "language": "unknown"
    },
    {
      "code": "Refer to the [API documentation](https://developers.cloudflare.com/api/resources/load_balancers/subresources/pools/methods/update/) for more information on the pool configuration.\n\nNote\n\nLeast Outstanding Requests steering can also be configured on a load balancer as a [global traffic steering policy](https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/least-outstanding-requests/), taking into account outstanding request counts and `random_steering` weights for pools on the load balancer.\n\n## Limitations\n\nLeast Outstanding Requests steering can be configured for pools that are part of [DNS-only load balancers](https://developers.cloudflare.com/load-balancing/understand-basics/proxy-modes/#dns-only-load-balancing), but is only supported in a no-operation form. When endpoint steering logic is applied for a pool on a DNS-only load balancer, all endpoint outstanding request counts are considered to be zero, meaning traffic is served solely based on endpoint weights.\n\nAlthough it is configurable, it is not recommended to associate pools that use Least Outstanding Requests steering with DNS-only load balancers due to its partial support.\n\n</page>\n\n<page>\n---\ntitle: Random steering Â· Cloudflare Load Balancing docs\ndescription: Random steering sends requests to endpoints purely based on\n  endpoint weights. Distributes traffic more accurately, but may cause requests\n  from the same IP to hit different endpoints.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/random-origin-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/random-origin-steering/index.md\n---\n\n**Random steering** sends requests to endpoints purely based on [endpoint weights](https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/#weights). Distributes traffic more accurately, but may cause requests from the same IP to hit different endpoints.\n\n</page>\n\n<page>\n---\ntitle: Dynamic steering Â· Cloudflare Load Balancing docs\ndescription: Dynamic steering uses health monitor data to identify the fastest\n  pool for a given Cloudflare Region or data center.\nlastUpdated: 2025-01-10T15:14:38.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/dynamic-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/dynamic-steering/index.md\n---\n\n**Dynamic steering** uses health monitor data to identify the fastest pool for a given Cloudflare Region or data center.\n\nDynamic steering creates Round Trip Time (RTT) profiles based on an exponential weighted moving average (EWMA) of RTT to determine the fastest pool. If there is no current RTT data for your pool in a region or colocation center, Cloudflare directs traffic to the pools in failover order.\n\nRTT values are collected each time a health probe request is made and based on the response from the endpoint to the monitor request. When a request is made, Cloudflare inspects the RTT data and uses it to sort pools by their RTT values.\n\nWhen enabling Dynamic steering the first time for a pool, allow 10 minutes for the change to take effect while Cloudflare builds an RTT profile for that pool.\n\nFor TCP health monitors, calculated latency may not reflect the true latency to the endpoint if you are terminating TCP at a cloud provider edge location.\n\nThe diagram below shows how Cloudflare would route traffic to the pool with the lowest EWMA among three regions: Eastern North America, Europe, and Australia. In this case, the ENAM pool is selected because it has the lowest RTT.\n\n![Dynamic steering routes traffic to the fastest available pool](https://developers.cloudflare.com/_astro/traffic-steering-2.CEeFHZfg_Z2bHxLO.webp)\n\nNote\n\nTo ensure dynamic steering works as expected, the [Health Monitor Region](https://developers.cloudflare.com/load-balancing/monitors/#health-monitor-regions) must be set to **All Regions**. The Enterprise-only **All Data Centers** option is also a viable alternative.\n\n</page>\n\n<page>\n---\ntitle: Geo steering Â· Cloudflare Load Balancing docs\ndescription: Geo steering directs traffic to pools tied to specific countries,\n  regions, or â for Enterprise customers only â data centers.\nlastUpdated: 2025-01-16T16:11:26.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/geo-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/geo-steering/index.md\n---\n\n**Geo steering** directs traffic to pools tied to specific countries, regions, or â for Enterprise customers only â data centers.\n\nThis option is extremely useful when you want site visitors to access the endpoint closest to them, which improves page-loading performance.\n\nNote\n\nCustom load balancing rules are incompatible with Geo steering. As a result, any custom rule applied to Geo-steered Load Balancers will not function as expected.\n\n## Pool assignment\n\nYou can assign multiple pools to the same area and the load balancer will use them in failover order. Any options not explicitly defined â whether in data centers, countries, or regions â will fall back to using default pools and failover.\n\n### Region steering\n\nCloudflare has [13 geographic regions](https://developers.cloudflare.com/load-balancing/reference/region-mapping-api/#list-of-load-balancer-regions) that span the world. The region of a client is determined by the region of the Cloudflare data center that answers the clientâs DNS query.\n\nWarning\n\nIf you add a pool to a region, you cannot [delete this pool](https://developers.cloudflare.com/load-balancing/pools/create-pool/#delete-a-pool) until you remove it from the **Geo steering** configuration. The configuration is **not** automatically removed when you change to a different **Traffic Steering** method.\n\n* Dashboard\n\n  When [creating or editing a load balancer](https://developers.cloudflare.com/load-balancing/load-balancers/create-load-balancer/):\n\n  1. Go to the **Traffic steering** step.\n  2. Select **Geo steering**.\n  3. For **Region**, select a region > **Add Region**.\n  4. Select **Edit**.\n  5. Select a pool > **Add Pool**.\n  6. If adding multiple pools, re-order them into your preferred failback order.\n  7. (optional) Add more regions if needed.\n\n* API\n\n  Use the `regions_pool` property of the [Update Load Balancers](https://developers.cloudflare.com/api/resources/load_balancers/methods/update/) command to specify an array of regions. Specify each region using the [appropriate region code](https://developers.cloudflare.com/load-balancing/reference/region-mapping-api/#list-of-load-balancer-regions) followed by a list of endpoints to use for that region.\n\n  In the example below, `WNAM` and `ENAM` represent the West and East Coasts of North America, respectively.",
      "language": "unknown"
    },
    {
      "code": "If you only define `WNAM`, then traffic from the East Coast will be routed to the `default_pools`. You can test this using a client in each of those locations.\n\n### Country steering\n\n* Dashboard\n\n  When [creating or editing a load balancer](https://developers.cloudflare.com/load-balancing/load-balancers/create-load-balancer/):\n\n  1. Follow the [create a load balancer procedure](https://developers.cloudflare.com/load-balancing/load-balancers/create-load-balancer/#create-a-load-balancer) until you reach the **Traffic steering** step.\n  2. Select **Geo steering**.\n  3. For **Country**, select a country > **Add Region**.\n  4. Select **Edit**.\n  5. Select a pool > **Add Pool**.\n  6. If adding multiple pools, re-order them into your preferred failback order.\n  7. (optional) Add more countries if needed.\n\n* API\n\n  When creating a load balancer [via the API](https://developers.cloudflare.com/api/resources/load_balancers/methods/create/), include the `country_pools` object to map countries to a list of pool IDs (ordered by their failover priority).\n\n  To get a list of country codes, use the [Region API](https://developers.cloudflare.com/load-balancing/reference/region-mapping-api/).\n\n  Any country not explicitly defined will fall back to using the corresponding `region_pool` mapping (if it exists), then to the associated default pools.\n\n### PoP steering\n\nWhen creating a load balancer [via the API](https://developers.cloudflare.com/api/resources/load_balancers/methods/create/), include the `pop_pools` object to map Cloudflare data centers to a list of pool IDs (ordered by their failover priority).\n\nFor help finding data center identifiers, refer to [this community thread](https://community.cloudflare.com/t/is-there-a-way-to-retrieve-cloudflare-pops-list-and-locations-programmatically/234643).\n\nAny data center not explicitly defined will fall back to using the corresponding `country_pool`, then `region_pool` mapping (if it exists), and finally to associated default pools.\n\nNote\n\nPoP steering is only available to Enterprise customers and only accessible via the API.\n\n### Failover behavior\n\nA fallback pool will be used if there is only one pool in the same region and it is unavailable. If there are multiple pools in the same region, the order of the pools will be respected. For example, if the first pool is unavailable, the second pool will be used.\n\n</page>\n\n<page>\n---\ntitle: Least Outstanding Requests steering - Steering policies Â· Cloudflare Load\n  Balancing docs\ndescription: Least Outstanding Requests steering allows you to route traffic to\n  pools that currently have the lowest number of outstanding requests.\nlastUpdated: 2025-02-10T10:26:10.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/least-outstanding-requests/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/least-outstanding-requests/index.md\n---\n\n**Least Outstanding Requests steering** allows you to route traffic to pools that currently have the lowest number of outstanding requests.\n\nThis steering policy selects a pool by taking into consideration `random_steering` weights, as well as each pool's number of in-flight requests. Pools with more pending requests are weighted proportionately less in relation to others.\n\nLeast Outstanding Requests steering is best to use if your pools are easily overwhelmed by a spike in concurrent requests. This steering method lends itself to applications that value server health above latency, geographic alignment, or other metrics. It takes into account the [pool's health status](https://developers.cloudflare.com/load-balancing/understand-basics/health-details/#how-a-pool-becomes-unhealthy), [adaptive routing](https://developers.cloudflare.com/load-balancing/understand-basics/adaptive-routing/), and [session affinity](https://developers.cloudflare.com/load-balancing/understand-basics/session-affinity/).\n\n## Configure via the API",
      "language": "unknown"
    },
    {
      "code": "Refer to the [API documentation](https://developers.cloudflare.com/api/resources/load_balancers/methods/update/) for more information on the load balancer configuration.\n\nNote\n\nLeast Outstanding Requests steering can also be configured on a pool as a [local traffic steering policy](https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/origin-level-steering/least-outstanding-requests-pools/), taking into account outstanding request counts and weights for endpoints within the pool.\n\n## Limitations\n\nLeast Outstanding Requests steering can be configured for [DNS-only load balancers](https://developers.cloudflare.com/load-balancing/understand-basics/proxy-modes/#dns-only-load-balancing), but is only supported in a no-operation form. For DNS-only load balancers, all pool outstanding request counts are considered to be zero, meaning traffic is served solely based on `random_steering` weights.\n\nAlthough it is configurable, it is not recommended to use Least Outstanding Requests steering for DNS-only load balancers due to its partial support.\n\n</page>\n\n<page>\n---\ntitle: Proximity steering Â· Cloudflare Load Balancing docs\ndescription: Proximity steering routes visitors or internal services to the\n  closest physical data center.\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/proximity-steering/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/proximity-steering/index.md\n---\n\n**Proximity steering** routes visitors or internal services to the closest physical data center.\n\nTo use proximity steering on a load balancer, you first need to add GPS coordinates to each pool.\n\n## When to add proximity steering\n\n* For new pools, add GPS coordinates when you create a pool.\n* For existing pools, add GPS coordinates when [managing pools](https://developers.cloudflare.com/load-balancing/pools/create-pool/#edit-a-pool) or in the **Add Traffic steering** step of [creating a load balancer](https://developers.cloudflare.com/load-balancing/load-balancers/create-load-balancer/).\n\n## How to add proximity steering\n\nTo add coordinates when creating or editing a pool:\n\n1. Click the *Configure coordinates for Proximity Steering* dropdown.\n2. Enter the latitude and longitude or drag a marker on the map.\n3. Select **Save**.\n\nWarning:\n\nFor accurate proximity steering, add GPS coordinates to all pools within the same load balancer.\n\n</page>\n\n<page>\n---\ntitle: Standard traffic steering policies Â· Cloudflare Load Balancing docs\ndescription: Standard steering policies include Off - Failover and Random.\nlastUpdated: 2024-12-19T15:49:57.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/standard-options/\n  md: https://developers.cloudflare.com/load-balancing/understand-basics/traffic-steering/steering-policies/standard-options/index.md\n---\n\n**Standard steering** policies include **Off - Failover** and **Random**.\n\nThese are the only steering policies available to non-Enterprise customers who have not purchased **Traffic steering**.\n\n## Off - Failover\n\nFailover steering uses the pool order to determine failover priority (the failover order).\n\nFailover directs traffic from unhealthy pools â determined by [health monitors](https://developers.cloudflare.com/load-balancing/monitors/) and the **Health Threshold** â to the next healthy pool in the configuration. Customers commonly use this option to set up [active - passive failover](https://developers.cloudflare.com/load-balancing/load-balancers/common-configurations/#active---passive-failover).\n\nIf all pools are marked unhealthy, Load Balancing will direct traffic to the fallback pool. The default fallback pool is the last pool listed in the Load Balancing configuration.\n\nIf no monitors are attached to the load balancer, it will direct traffic to the primary pool exclusively.\n\n### Failback behavior\n\nIn an active/standby setup, with two origin pools:\n\n* Traffic always routes to Pool 1 (the primary pool) unless it becomes unhealthy.\n* If Pool 1 is marked unhealthy, traffic shifts to Pool 2 (the standby pool).\n* Once Pool 1 becomes healthy again, traffic automatically shifts back to Pool 1, assuming no [session affinity](https://developers.cloudflare.com/load-balancing/understand-basics/session-affinity/) or other settings require subsequent requests to stay at Pool 2.\n\nThis behavior is known as failback and ensures traffic resumes normal routing when the primary pool recovers.\n\n## Random steering\n\nChoose **Random** to route traffic to a healthy pool at random. Customers can use this option to set up [active - active failover](https://developers.cloudflare.com/load-balancing/load-balancers/common-configurations/#active---active-failover) (also known as round robin), where traffic is split equally between multiple pools.\n\nSimilar to setting Weights to direct the amount of traffic going to each endpoint, customers can also set Weights on pools via the [API's](https://developers.cloudflare.com/api/resources/load_balancers/methods/create/) `random_steering` object to determine the percentage of traffic sent to each pool.\n\n</page>\n\n<page>\n---\ntitle: Account-scoped datasets Â· Cloudflare Logs docs\nlastUpdated: 2025-07-25T16:42:51.000Z\nchatbotDeprioritize: true\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/index.md\n---\n\n* [Access requests](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/access_requests/)\n* [Audit Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/audit_logs/)\n* [Audit Logs V2](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/audit_logs_v2/)\n* [Browser Isolation User Actions](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/biso_user_actions/)\n* [CASB Findings](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/casb_findings/)\n* [Device posture results](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/device_posture_results/)\n* [DEX Application Tests](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/dex_application_tests/)\n* [DEX Device State Events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/dex_device_state_events/)\n* [DLP Forensic Copies](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/dlp_forensic_copies/)\n* [DNS Firewall Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/dns_firewall_logs/)\n* [Email security Alerts](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/email_security_alerts/)\n* [Gateway DNS](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/gateway_dns/)\n* [Gateway HTTP](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/gateway_http/)\n* [Gateway Network](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/gateway_network/)\n* [IPSec Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/ipsec_logs/)\n* [Magic IDS Detections](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/magic_ids_detections/)\n* [Network Analytics Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/network_analytics_logs/)\n* [Sinkhole HTTP Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/sinkhole_http_logs/)\n* [SSH Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/ssh_logs/)\n* [WARP Config Changes](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/warp_config_changes/)\n* [WARP Toggle Changes](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/warp_toggle_changes/)\n* [Workers Trace Events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/workers_trace_events/)\n* [Zero Trust Network Session Logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/zero_trust_network_sessions/)\n\n</page>\n\n<page>\n---\ntitle: CMB support by dataset Â· Cloudflare Logs docs\nlastUpdated: 2025-07-28T14:13:32.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/cmb/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/cmb/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Zone-scoped datasets Â· Cloudflare Logs docs\nlastUpdated: 2025-07-25T16:42:51.000Z\nchatbotDeprioritize: true\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/index.md\n---\n\n* [DNS logs](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/dns_logs/)\n* [Firewall events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/firewall_events/)\n* [HTTP requests](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/http_requests/)\n* [NEL reports](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/nel_reports/)\n* [Page Shield events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/page_shield_events/)\n* [Spectrum events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/spectrum_events/)\n* [Zaraz Events](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/zaraz_events/)\n\n</page>\n\n<page>\n---\ntitle: Enable Logpush to Amazon S3 Â· Cloudflare Logs docs\ndescription: Cloudflare Logpush supports pushing logs directly to Amazon S3 via\n  the Cloudflare dashboard or via API. Customers that use AWS GovCloud locations\n  should use our S3-compatible endpoint and not the Amazon S3 endpoint.\nlastUpdated: 2025-10-10T13:43:07.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/aws-s3/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/aws-s3/index.md\n---\n\nCloudflare Logpush supports pushing logs directly to Amazon S3 via the Cloudflare dashboard or via API. Customers that use AWS GovCloud locations should use our **S3-compatible endpoint** and not the **Amazon S3 endpoint**.\n\n## Manage via the Cloudflare dashboard\n\n1. In the Cloudflare dashboard, go to the **Logpush** page at the account or or domain (also known as zone) level.\n\n   For account: [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/logs)\n\n   For domain (also known as zone): [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/:zone/analytics/logs)\n\n2. Depending on your choice, you have access to [account-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/) and [zone-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/), respectively.\n\n3. Select **Create a Logpush job**.\n\n1) In **Select a destination**, choose **Amazon S3**.\n\n2) Enter or select the following destination information:\n\n   * **Bucket** - S3 bucket name\n   * **Path** - bucket location within the storage container\n   * **Organize logs into daily subfolders** (recommended)\n   * **Bucket region**\n   * If your policy requires [AWS SSE-S3 AES256 Server Side Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html).\n   * For **Grant Cloudflare access to upload files to your bucket**, make sure your bucket has a [policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-policies-s3.html#iam-policy-ex0) (if you did not add it already):\n     * Copy the JSON policy, then go to your bucket in the Amazon S3 console and paste the policy in **Permissions** > **Bucket Policy** and select **Save**.\n\nWhen you are done entering the destination details, select **Continue**.\n\n1. To prove ownership, Cloudflare will send a file to your designated destination. To find the token, select the **Open** button in the **Overview** tab of the ownership challenge file, then paste it into the Cloudflare dashboard to verify your access to the bucket. Enter the **Ownership Token** and select **Continue**.\n\n2. Select the dataset to push to the storage service.\n\n3. In the next step, you need to configure your logpush job:\n\n   * Enter the **Job name**.\n   * Under **If logs match**, you can select the events to include and/or remove from your logs. Refer to [Filters](https://developers.cloudflare.com/logs/logpush/logpush-job/filters/) for more information. Not all datasets have this option available.\n   * In **Send the following fields**, you can choose to either push all logs to your storage destination or selectively choose which logs you want to push.\n\n4. In **Advanced Options**, you can:\n\n   * Choose the format of timestamp fields in your logs (`RFC3339`(default),`Unix`, or `UnixNano`).\n   * Select a [sampling rate](https://developers.cloudflare.com/logs/logpush/logpush-job/api-configuration/#sampling-rate) for your logs or push a randomly-sampled percentage of logs.\n   * Enable redaction for `CVE-2021-44228`. This option will replace every occurrence of `${` with `x{`.\n\n5. Select **Submit** once you are done configuring your logpush job.\n\n## Create and get access to an S3 bucket\n\nCloudflare uses Amazon Identity and Access Management (IAM) to gain access to your S3 bucket. The Cloudflare IAM user needs `PutObject` permission for the bucket.\n\nLogs are written into that bucket as gzipped objects using the S3 Access Control List (ACL) `Bucket-owner-full-control` permission.\n\nFor illustrative purposes, imagine that you want to store logs in the bucket `burritobot`, in the `logs` directory. The S3 URL would then be `s3://burritobot/logs`.\n\nEnsure **Log Share** permissions are enabled, before attempting to read or configure a Logpush job. For more information refer to the [Roles section](https://developers.cloudflare.com/logs/logpush/permissions/#roles).\n\n\n\nTo enable Logpush to Amazon S3:\n\n1. Create an S3 bucket. Refer to [instructions from Amazon](https://docs.aws.amazon.com/AmazonS3/latest/gsg/CreatingABucket.html).\n\n   Note\n\n   Buckets in China regions (`cn-north-1`, `cn-northwest-1`) are currently not supported.\n\n2. Edit and paste the policy below into **S3** > **Bucket** > **Permissions** > **Bucket Policy**, replacing the `Resource` value with your own bucket path. The `AWS` `Principal` is owned by Cloudflare and should not be changed.",
      "language": "unknown"
    },
    {
      "code": "Note\n\nLogpush uses multipart upload for S3. Aborted uploads will result in incomplete files remaining in your bucket. To minimize your storage costs, Amazon recommends configuring a lifecycle rule using the `AbortIncompleteMultipartUpload` action. Refer to [Uploading and copying objects using multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html#mpu-abort-incomplete-mpu-lifecycle-config).\n\n</page>\n\n<page>\n---\ntitle: Enable Logpush to Microsoft Azure Â· Cloudflare Logs docs\ndescription: Cloudflare Logpush supports pushing logs directly to Microsoft\n  Azure via the Cloudflare dashboard or via API.\nlastUpdated: 2025-11-19T11:40:36.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/azure/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/azure/index.md\n---\n\nCloudflare Logpush supports pushing logs directly to Microsoft Azure via the Cloudflare dashboard or via API.\n\nNote\n\nThe [Microsoft Sentinel](https://developers.cloudflare.com/analytics/analytics-integrations/sentinel/) integration for Cloudflare is available in two connector versions.\n\n## Manage via the Cloudflare dashboard\n\n1. In the Cloudflare dashboard, go to the **Logpush** page at the account or or domain (also known as zone) level.\n\n   For account: [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/logs)\n\n   For domain (also known as zone): [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/:zone/analytics/logs)\n\n2. Depending on your choice, you have access to [account-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/) and [zone-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/), respectively.\n\n3. Select **Create a Logpush job**.\n\n1) In **Select a destination**, choose **Microsoft Azure**.\n\n2) Enter or select the following destination details:\n\n   * **SAS URL** - a pre-signed URL that grants access to Azure Storage resources. Refer to [Azure storage documentation](https://learn.microsoft.com/en-us/azure/storage/storage-explorer/vs-azure-tools-storage-manage-with-storage-explorer?tabs=macos#shared-access-signature-sas-url) for more information on generating a SAS URL using Azure Storage Explorer. The service must be set to Blob-only (`ss=b`), and the resource type must be set to Object-only (`srt=o`).\n   * **Path** - bucket location within the storage container\n   * **Organize logs into daily subfolders** (recommended)\n\nWhen you are done entering the destination details, select **Continue**.\n\n1. Select the dataset to push to the storage service.\n\n2. In the next step, you need to configure your logpush job:\n\n   * Enter the **Job name**.\n   * Under **If logs match**, you can select the events to include and/or remove from your logs. Refer to [Filters](https://developers.cloudflare.com/logs/logpush/logpush-job/filters/) for more information. Not all datasets have this option available.\n   * In **Send the following fields**, you can choose to either push all logs to your storage destination or selectively choose which logs you want to push.\n\n3. In **Advanced Options**, you can:\n\n   * Choose the format of timestamp fields in your logs (`RFC3339`(default),`Unix`, or `UnixNano`).\n   * Select a [sampling rate](https://developers.cloudflare.com/logs/logpush/logpush-job/api-configuration/#sampling-rate) for your logs or push a randomly-sampled percentage of logs.\n   * Enable redaction for `CVE-2021-44228`. This option will replace every occurrence of `${` with `x{`.\n\n4. Select **Submit** once you are done configuring your logpush job.\n\n## Create and get access to a Blob Storage container\n\nCloudflare uses a shared access signature (SAS) token to gain access to your Blob Storage container. You will need to provide `Write` permission and an expiration period of at least five years, which will allow you to not worry about the SAS token expiring.\n\nEnsure **Log Share** permissions are enabled, before attempting to read or configure a Logpush job. For more information refer to the [Roles section](https://developers.cloudflare.com/logs/logpush/permissions/#roles).\n\n\n\nTo enable Logpush to Azure:\n\n1. Create a Blob Storage container. Refer to [instructions from Azure](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal).\n\n2. Create a [shared access signature (SAS)](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview) to secure and restrict access to your blob storage container. Use [Storage Explorer](https://learn.microsoft.com/en-us/azure/storage/storage-explorer/vs-azure-tools-storage-manage-with-storage-explorer) to navigate to your container and right click to create a signature. Set the signature to expire at least five years from now and only provide write permission.\n\n3. Provide the SAS URL when prompted by the Logpush API or UI.\n\nNote\n\nLogpush will stop pushing logs if your SAS token expires, which is why an expiration period of at least five years is required. The renewal for your SAS token needs to be done via API, updating the `destination_conf` parameter in your Logpush job.\n\n</page>\n\n<page>\n---\ntitle: Enable Logpush to BigQuery Â· Cloudflare Logs docs\ndescription: Configure Logpush to send batches of Cloudflare logs to BigQuery.\nlastUpdated: 2025-07-21T15:07:21.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/bigquery/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/bigquery/index.md\n---\n\nConfigure Logpush to send batches of Cloudflare logs to BigQuery.\n\nBigQuery supports loading up to 1,500 jobs per table per day (including failures) with up to 10 million files in each load. That means you can load into BigQuery once per minute and include up to 10 million files in a load. For more information, refer to BigQuery's quotas for load jobs.\n\nLogpush delivers batches of logs as soon as possible, which means you could receive more than one batch of files per minute. Ensure your BigQuery job is configured to ingest files on a given time interval, like every minute, as opposed to when files are received. Ingesting files into BigQuery as each Logpush file is received could exhaust your BigQuery quota quickly.\n\nFor a community-supported example of how to set up a schedule job load with BigQuery, refer to [Cloudflare + Google Cloud | Integrations repository](https://github.com/cloudflare/cloudflare-gcp/tree/master/logpush-to-bigquery). Note that this repository is provided on a best-effort basis and is not maintained routinely.\n\n</page>\n\n<page>\n---\ntitle: Enable Logpush to Datadog Â· Cloudflare Logs docs\ndescription: Cloudflare Logpush supports pushing logs directly to Datadog via\n  the Cloudflare dashboard or via API.\nlastUpdated: 2025-10-10T13:43:07.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/datadog/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/datadog/index.md\n---\n\nCloudflare Logpush supports pushing logs directly to Datadog via the Cloudflare dashboard or via API.\n\n## Manage via the Cloudflare dashboard\n\n1. In the Cloudflare dashboard, go to the **Logpush** page at the account or or domain (also known as zone) level.\n\n   For account: [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/logs)\n\n   For domain (also known as zone): [Go to **Logpush**](https://dash.cloudflare.com/?to=/:account/:zone/analytics/logs)\n\n2. Depending on your choice, you have access to [account-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/account/) and [zone-scoped datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/zone/), respectively.\n\n3. Select **Create a Logpush job**.\n\n1) In **Select a destination**, choose **Datadog**.\n\n2) Enter or select the following destination information:\n\n   * **Datadog URL Endpoint**, which can be either one below. You can find the difference at [Datadog API reference](https://docs.datadoghq.com/api/latest/logs/).\n\n* v1\n\n  * `http-intake.logs.datadoghq.com/v1/input`\n\n* v2\n\n  * `http-intake.logs.datadoghq.com/api/v2/logs`\n\n- **Datadog API Key**, can be retrieved by following [these steps](https://docs.datadoghq.com/account_management/api-app-keys/#add-an-api-key-or-client-token).\n\n- **Service**, **Hostname**, **Datadog ddsource field**, and **ddtags** fields can be set as URL parameters. For more information, refer to the [Logs section](https://docs.datadoghq.com/api/latest/logs/) in Datadog's documentation. While these parameters are optional, they can be useful for indexing or processing logs. Note that the values of these parameters may contain special characters, which should be URL encoded.\n\nWhen you are done entering the destination details, select **Continue**.\n\n1. Select the dataset to push to the storage service.\n\n2. In the next step, you need to configure your logpush job:\n\n   * Enter the **Job name**.\n   * Under **If logs match**, you can select the events to include and/or remove from your logs. Refer to [Filters](https://developers.cloudflare.com/logs/logpush/logpush-job/filters/) for more information. Not all datasets have this option available.\n   * In **Send the following fields**, you can choose to either push all logs to your storage destination or selectively choose which logs you want to push.\n\n3. In **Advanced Options**, you can:\n\n   * Choose the format of timestamp fields in your logs (`RFC3339`(default),`Unix`, or `UnixNano`).\n   * Select a [sampling rate](https://developers.cloudflare.com/logs/logpush/logpush-job/api-configuration/#sampling-rate) for your logs or push a randomly-sampled percentage of logs.\n   * Enable redaction for `CVE-2021-44228`. This option will replace every occurrence of `${` with `x{`.\n\n4. Select **Submit** once you are done configuring your logpush job.\n\n## Manage via API\n\nTo set up a Datadog Logpush job:\n\n1. Create a job with the appropriate endpoint URL and authentication parameters.\n2. Enable the job to begin pushing logs.\n\nNote\n\nUnlike configuring Logpush jobs for AWS S3, GCS, or Azure, there is no ownership challenge when configuring Logpush to Datadog.\n\nEnsure **Log Share** permissions are enabled, before attempting to read or configure a Logpush job. For more information refer to the [Roles section](https://developers.cloudflare.com/logs/logpush/permissions/#roles).\n\n### 1. Create a job\n\nTo create a job, make a `POST` request to the Logpush jobs endpoint with the following fields:\n\n* **name** (optional) - Use your domain name as the job name.\n\n* **destination\\_conf** - A log destination consisting of an endpoint URL, authorization header, and zero or more optional parameters that Datadog supports in the string format below.\n\n  * `<DATADOG_ENDPOINT_URL>`: The Datadog HTTP logs intake endpoint, which can be either one below. You can find the difference at [Datadog API reference](https://docs.datadoghq.com/api/latest/logs/).\n\n  - v1\n\n    [https://http-intake.logs.datadoghq.com/v1/input\\`](https://http-intake.logs.datadoghq.com/v1/input%60)\n\n  - v2\n\n    `https://http-intake.logs.datadoghq.com/api/v2/logs`\n\n* `<DATADOG_API_KEY>`: The Datadog API token can be retrieved by following [these steps](https://docs.datadoghq.com/account_management/api-app-keys/#add-an-api-key-or-client-token). For example, `20e6d94e8c57924ad1be3c29bcaee0197d`.\n\n* `ddsource`: Set to `cloudflare`.\n\n* `service`, `host`, `ddtags`: Optional parameters allowed by Datadog.",
      "language": "unknown"
    },
    {
      "code": "* **dataset** - The category of logs you want to receive. Refer to [Datasets](https://developers.cloudflare.com/logs/logpush/logpush-job/datasets/) for the full list of supported datasets.\n* **output\\_options** (optional) - To configure fields, sample rate, and timestamp format, refer to [Log Output Options](https://developers.cloudflare.com/logs/logpush/logpush-job/log-output-options/).\n\nExample request using cURL:\n\nRequired API token permissions\n\nAt least one of the following [token permissions](https://developers.cloudflare.com/fundamentals/api/reference/permissions/) is required:\n\n* `Logs Write`",
      "language": "unknown"
    },
    {
      "code": "Response:",
      "language": "unknown"
    },
    {
      "code": "### 2. Enable (update) a job\n\nTo enable a job, make a `PUT` request to the Logpush jobs endpoint. You will use the job ID returned from the previous step in the URL and send `{\"enabled\": true}` in the request body.\n\nExample request using cURL:\n\nRequired API token permissions\n\nAt least one of the following [token permissions](https://developers.cloudflare.com/fundamentals/api/reference/permissions/) is required:\n\n* `Logs Write`",
      "language": "unknown"
    },
    {
      "code": "Response:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe Datadog destination is exclusive to new jobs and might not be backward compatible with older jobs. Create new jobs if you expect to send your logs directly to Datadog instead of modifying already existing ones. If you try to modify an existing job for another destination to push logs to Datadog, you may observe errors.\n\nNote\n\nTo analyze and visualize Cloudflare metrics using the Cloudflare Integration tile for Datadog, follow the steps in the [Datadog Analytics integration page](https://developers.cloudflare.com/analytics/analytics-integrations/datadog/).\n\n## Limitations\n\nNote the following Logpush sending limitations, as described in the [Datadog documentation](https://docs.datadoghq.com/api/latest/logs/).\n\nSend your logs to your Datadog platform over HTTP. Limits per HTTP request are the following:\n\n* Maximum content size per payload (uncompressed): 5 MB\n* Maximum size for a single log: 1 MB\n* Maximum array size if sending multiple logs in an array: 1,000 entries\n\nWarning\n\nThe above limits are hardcoded defaults. It is not possible to override these limitations using the Logpush configuration values, `max_upload_records` or `max_upload_bytes`.\n\nThese limitations may result in noticeable log ingestion delay within Datadog following high traffic events. Logpush will not drop unsent logs, so all logs will be uploaded to Datadog in due time.\n\n</page>\n\n<page>\n---\ntitle: Enable Logpush to Elastic Â· Cloudflare Logs docs\ndescription: Push your Cloudflare logs to Elastic for instant visibility and\n  insights. Enabling this integration with Elastic comes with a predefined\n  dashboard to view all of your Cloudflare observability and security data with\n  ease.\nlastUpdated: 2025-07-21T15:07:21.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/elastic/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/elastic/index.md\n---\n\nPush your Cloudflare logs to Elastic for instant visibility and insights. Enabling this integration with Elastic comes with a predefined dashboard to view all of your Cloudflare observability and security data with ease.\n\nThe Cloudflare Logpush integration can be used in three different modes to collect data:\n\n* **HTTP Endpoint mode** - Cloudflare pushes logs directly to an HTTP endpoint hosted by your Elastic Agent.\n* **AWS S3 polling mode** - Cloudflare writes data to S3, and the Elastic Agent polls the S3 bucket by listing its contents and reading new files.\n* **AWS S3 SQS mode** - Cloudflare writes data to S3, S3 pushes a new object notification to SQS, the Elastic Agent receives the notification from SQS, and then reads the S3 object. Multiple Agents can be used in this mode.\n\nNote\n\nElastic recommends the AWS S3 SQS mode.\n\n## Enable Logpush Job in Cloudflare\n\nDetermine which method you want to use, and configure the appropriate Logpush job in the Cloudflare dashboard or via the API.\n\nElastic supports the default JSON format.\n\nTo push logs to an object storage for short term storage and buffering before ingesting into Elastic (recommended), follow the instructions to configure a Logpush job to push logs to [AWS S3](https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/aws-s3/), [Google Cloud Storage](https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/google-cloud-storage/), or [Azure Blob Storage](https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/azure/).\n\nTo use the [HTTP Endpoint mode](https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/http/), use the API to push logs to an HTTP endpoint backed by your Elastic Agent.\n\nAdd the same custom header along with its value on both sides for additional security.\n\nFor example, while creating a job along with a header and value for a particular dataset:\n\nRequired API token permissions\n\nAt least one of the following [token permissions](https://developers.cloudflare.com/fundamentals/api/reference/permissions/) is required:\n\n* `Logs Write`",
      "language": "unknown"
    },
    {
      "code": "## Enable the Integration in Elastic\n\nOnce the Logpush job is configured, follow Elastics instructions for [setting up the Integration](https://docs.elastic.co/integrations/cloudflare_logpush) in the Elastic app.\n\n## View Dashboards\n\nLog in to your [Elastic account](https://www.elastic.co/) to view prebuilt dashboards and configure alerts.\n\n</page>\n\n<page>\n---\ntitle: Dedicated Egress IP for Logpush Â· Cloudflare Logs docs\ndescription: This guide covers Dedicated CDN Egress IPs and Logpush\n  configuration and testing instructions to enable log delivery with a fixed,\n  dedicated egress IP.\nlastUpdated: 2025-12-19T01:58:33.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/egress-ip/\n  md: https://developers.cloudflare.com/logs/logpush/logpush-job/enable-destinations/egress-ip/index.md\n---\n\nThis guide covers [Dedicated CDN Egress IPs](https://developers.cloudflare.com/smart-shield/configuration/dedicated-egress-ips/) and Logpush configuration and testing instructions to enable log delivery with a fixed, dedicated egress IP.\n\n## Prerequisites\n\nTo use Logpush with a dedicated egress IP, you will need to have [Smart Shield Advanced](https://developers.cloudflare.com/smart-shield/get-started/#smart-shield-advanced) with Dedicated CDN Egress IPs (formerly known as Aegis). Note that the Dedicated CDN Egress IPs pool is associated with a zone, not with an account. To use Logpush with dedicated IPs, traffic must be routed to a single zone.\n\nThe general approach is to have your Logpush job proxying Logpush data through a Cloudflare zone with Dedicated CDN Egress IPs enabled to send data to your desired destination. This way your destination will only need to allowlist the provisioned dedicated egress IPs of your proxy zone.\n\nAs a prerequisite, you need to create a dedicated zone or use an existing zone. If using an existing zone, be aware that the zone's egress will be restricted to Dedicated CDN Egress IPs. Make sure all services using that zone will not be impacted.\n\nIt is recommended to use a separate, dedicated zone as a proxy to avoid impacting production systems. If you choose to create a new zone, follow the [steps](https://developers.cloudflare.com/registrar/get-started/register-domain/) to register a new domain with Cloudflare.\n\nThe following example shows how to set up logpush and Dedicated CDN Egress IPs to proxy an HTTPS destination, but the proxying should work for any supported Logpush destination as all destinations use the HTTP protocol underneath.\n\n## 1. Provision dedicated egress IP Pool\n\n1. Work with your Cloudflare account team to purchase [Dedicated CDN Egress IPs](https://developers.cloudflare.com/smart-shield/configuration/dedicated-egress-ips/) for your zone.\n\n2. (Optional but recommended) Request two IPs â one in PDX-B and one in SJC-A â to ensure coverage across regions.\n\n3. Confirm Pool ID once provisioned.\n\n## 2. Configure a zone\n\n1. Register or use an existing zone for the dedicated egress IPs pool.\n2. Contact your account team to get the ID for your dedicated egress IPs pool.\n3. Make a `PATCH` request to the [Edit Zone Setting](https://developers.cloudflare.com/api/resources/zones/subresources/settings/methods/edit/) endpoint:\n\n* Specify `aegis` as the setting ID in the URL.\n* In the request body, set `enabled` to `true` and use the ID from the previous step as `pool_id`.\n\nRequired API token permissions\n\nAt least one of the following [token permissions](https://developers.cloudflare.com/fundamentals/api/reference/permissions/) is required:\n\n* `Zone Settings Write`",
      "language": "unknown"
    },
    {
      "code": "## 3. Proxy zone setup\n\n1. In your zone, add a DNS record (CNAME or A/AAAA) with **Target** as HTTP destination endpoint.\n\n![Create a DNS record in the Cloudflare dashboard to define the HTTP destination endpoint](https://developers.cloudflare.com/_astro/endpoint.DmFFJC-j_14N2lf.webp)\n\n1. If needed, configure [origin rules](https://developers.cloudflare.com/rules/origin-rules/) to specify a custom port. This is useful if your destination only accepts traffic on a non standard port, for example `12345`. You can configure `logpush.yourdestinationendpoint.com` (without specifying a port, as Cloudflare by default only proxies traffic on HTTP/HTTPS ports) to proxy to `yourdestinationendpoint.com:12345`.\n\n## 4. Configure Logpush\n\n1. Create a Logpush job with the following details:\n\n* Destination: HTTP\n* Endpoint: Use the domain/path set up (the Cloudflare dashboard will auto-validate the destination). Use the server name specified in the **Name** section in the DNS record. In this case, `logpush.yourdestionationendpoint.com`.\n\n![Enter destination details when creating a Logpush job in the Cloudflare dashboard](https://developers.cloudflare.com/_astro/destination-details.imLwZlEZ_1Y0Gk.webp)\n\n* Configuration: Select dataset, job name, filters, and fields. Refer to the [Logpush documentation](https://developers.cloudflare.com/logs/logpush/) for more details.\n\n1. Check destination to confirm if the logs are received.\n\n## 5. Secure your proxy zone endpoint\n\nThe proxy zone hostname is publicly resolvable, but traffic passes through Cloudflare's edge where you can apply security controls. Use the following best practices to protect your endpoint.\n\n### Add a secret header with WAF validation\n\nAdd a secret token as an HTTP header in your Logpush job, then create a WAF rule to block requests without it. This is the recommended approach for most deployments.\n\n**Configure Logpush with a secret header**\n\nAny URL parameter starting with `header_` becomes an HTTP header in the request. When creating or updating your Logpush job, add the secret header to your destination URL:",
      "language": "unknown"
    },
    {
      "code": "Generate a strong random token using `openssl rand -hex 32`.\n\n**Create a WAF custom rule**\n\nIn the proxy zone, go to **Security** > **WAF** > **Custom rules** and create a rule to block requests without the correct secret header.\n\n* **Expression:**",
      "language": "unknown"
    },
    {
      "code": "* **Action:** Block\n\n### Add ASN-based filtering\n\nFor defense in depth, add a rule to only allow traffic from Cloudflare's ASN. Logpush traffic originates from Cloudflare's network (ASN 13335 or 132892).\n\n* **Expression:**",
      "language": "unknown"
    },
    {
      "code": "* **Action:** Block\n\nNote\n\nASN filtering alone is insufficient because other Cloudflare customers' traffic also originates from these ASNs. Always combine with secret header validation.\n\n### Use Access Service Tokens for high-security environments\n\nFor stronger authentication, use [Cloudflare Access Service Tokens](https://developers.cloudflare.com/cloudflare-one/access-controls/service-credentials/service-tokens/) for machine-to-machine authentication. Create a Service Token in the Zero Trust dashboard, then configure Logpush with the Access headers:",
      "language": "unknown"
    },
    {
      "code": "### Verify your security configuration\n\nTest that your WAF rules are blocking unauthorized requests:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h3",
      "text": "Materialize Console",
      "id": "materialize-console"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h3",
      "text": "Neon Dashboard",
      "id": "neon-dashboard"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h3",
      "text": "Nile console",
      "id": "nile-console"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h3",
      "text": "pgEdge dashboard",
      "id": "pgedge-dashboard"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h3",
      "text": "PlanetScale Dashboard",
      "id": "planetscale-dashboard"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h3",
      "text": "Prisma Data Platform",
      "id": "prisma-data-platform"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "4. Configure Hyperdrive maximum connections",
      "id": "4.-configure-hyperdrive-maximum-connections"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h3",
      "text": "Timescale Dashboard",
      "id": "timescale-dashboard"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "1. Allow Hyperdrive access",
      "id": "1.-allow-hyperdrive-access"
    },
    {
      "level": "h3",
      "text": "Xata dashboard",
      "id": "xata-dashboard"
    },
    {
      "level": "h2",
      "text": "2. Create a database configuration",
      "id": "2.-create-a-database-configuration"
    },
    {
      "level": "h2",
      "text": "3. Use Hyperdrive from your Worker",
      "id": "3.-use-hyperdrive-from-your-worker"
    },
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "Limitation when using Workers",
      "id": "limitation-when-using-workers"
    },
    {
      "level": "h3",
      "text": "Workaround",
      "id": "workaround"
    },
    {
      "level": "h2",
      "text": "Configure via the API",
      "id": "configure-via-the-api"
    },
    {
      "level": "h2",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h2",
      "text": "Pool assignment",
      "id": "pool-assignment"
    },
    {
      "level": "h3",
      "text": "Region steering",
      "id": "region-steering"
    },
    {
      "level": "h3",
      "text": "Country steering",
      "id": "country-steering"
    },
    {
      "level": "h3",
      "text": "PoP steering",
      "id": "pop-steering"
    },
    {
      "level": "h3",
      "text": "Failover behavior",
      "id": "failover-behavior"
    },
    {
      "level": "h2",
      "text": "Configure via the API",
      "id": "configure-via-the-api"
    },
    {
      "level": "h2",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h2",
      "text": "When to add proximity steering",
      "id": "when-to-add-proximity-steering"
    },
    {
      "level": "h2",
      "text": "How to add proximity steering",
      "id": "how-to-add-proximity-steering"
    },
    {
      "level": "h2",
      "text": "Off - Failover",
      "id": "off---failover"
    },
    {
      "level": "h3",
      "text": "Failback behavior",
      "id": "failback-behavior"
    },
    {
      "level": "h2",
      "text": "Random steering",
      "id": "random-steering"
    },
    {
      "level": "h2",
      "text": "Manage via the Cloudflare dashboard",
      "id": "manage-via-the-cloudflare-dashboard"
    },
    {
      "level": "h2",
      "text": "Create and get access to an S3 bucket",
      "id": "create-and-get-access-to-an-s3-bucket"
    },
    {
      "level": "h2",
      "text": "Manage via the Cloudflare dashboard",
      "id": "manage-via-the-cloudflare-dashboard"
    },
    {
      "level": "h2",
      "text": "Create and get access to a Blob Storage container",
      "id": "create-and-get-access-to-a-blob-storage-container"
    },
    {
      "level": "h2",
      "text": "Manage via the Cloudflare dashboard",
      "id": "manage-via-the-cloudflare-dashboard"
    },
    {
      "level": "h2",
      "text": "Manage via API",
      "id": "manage-via-api"
    },
    {
      "level": "h3",
      "text": "1. Create a job",
      "id": "1.-create-a-job"
    },
    {
      "level": "h3",
      "text": "2. Enable (update) a job",
      "id": "2.-enable-(update)-a-job"
    },
    {
      "level": "h2",
      "text": "Limitations",
      "id": "limitations"
    },
    {
      "level": "h2",
      "text": "Enable Logpush Job in Cloudflare",
      "id": "enable-logpush-job-in-cloudflare"
    },
    {
      "level": "h2",
      "text": "Enable the Integration in Elastic",
      "id": "enable-the-integration-in-elastic"
    },
    {
      "level": "h2",
      "text": "View Dashboards",
      "id": "view-dashboards"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Provision dedicated egress IP Pool",
      "id": "1.-provision-dedicated-egress-ip-pool"
    },
    {
      "level": "h2",
      "text": "2. Configure a zone",
      "id": "2.-configure-a-zone"
    },
    {
      "level": "h2",
      "text": "3. Proxy zone setup",
      "id": "3.-proxy-zone-setup"
    },
    {
      "level": "h2",
      "text": "4. Configure Logpush",
      "id": "4.-configure-logpush"
    },
    {
      "level": "h2",
      "text": "5. Secure your proxy zone endpoint",
      "id": "5.-secure-your-proxy-zone-endpoint"
    },
    {
      "level": "h3",
      "text": "Add a secret header with WAF validation",
      "id": "add-a-secret-header-with-waf-validation"
    },
    {
      "level": "h3",
      "text": "Add ASN-based filtering",
      "id": "add-asn-based-filtering"
    },
    {
      "level": "h3",
      "text": "Use Access Service Tokens for high-security environments",
      "id": "use-access-service-tokens-for-high-security-environments"
    },
    {
      "level": "h3",
      "text": "Verify your security configuration",
      "id": "verify-your-security-configuration"
    }
  ],
  "url": "llms-txt#the-gcloud-cli-will-replace-any-existing-authorized-networks-with-the-list-you-provide-here.",
  "links": []
}