{
  "title": "Current Deployment ID: 80b72e19-da82-4465-83a2-c12fb11ccc72",
  "content": "json\n{\n  \"action\": { \"info\": \"CreateDatabase\", \"result\": true, \"type\": \"create\" },\n  \"actor\": {\n    \"email\": \"<ACTOR_EMAIL>\",\n    \"id\": \"b1ab1021a61b1b12612a51b128baa172\",\n    \"ip\": \"1b11:a1b2:12b1:12a::11a:1b\",\n    \"type\": \"user\"\n  },\n  \"id\": \"a123b12a-ab11-1212-ab1a-a1aa11a11abb\",\n  \"interface\": \"API\",\n  \"metadata\": {},\n  \"newValue\": \"\",\n  \"newValueJson\": { \"database_name\": \"my-db\" },\n  \"oldValue\": \"\",\n  \"oldValueJson\": {},\n  \"owner\": { \"id\": \"211b1a74121aa32a19121a88a712aa12\" },\n  \"resource\": {\n    \"id\": \"11a21122-1a11-12bb-11ab-1aa2aa1ab12a\",\n    \"type\": \"d1.database\"\n  },\n  \"when\": \"2024-08-09T04:53:55.752Z\"\n}\njs\ntry {\n    // This is an intentional misspelling\n    await db.exec(\"INSERTZ INTO my_table (name, employees) VALUES ()\");\n} catch (e: any) {\n    console.error({\n        message: e.message\n    });\n}\njson\n{\n  \"message\": \"D1_EXEC_ERROR: Error in line 1: INSERTZ INTO my_table (name, employees) VALUES (): sql error: near \\\"INSERTZ\\\": syntax error in INSERTZ INTO my_table (name, employees) VALUES () at offset 0\"\n}\ngraphql\nquery D1ObservabilitySampleQuery(\n  $accountTag: string!\n  $start: Date\n  $end: Date\n  $databaseId: string\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountTag }) {\n      d1AnalyticsAdaptiveGroups(\n        limit: 10000\n        filter: { date_geq: $start, date_leq: $end, databaseId: $databaseId }\n        orderBy: [date_DESC]\n      ) {\n        sum {\n          readQueries\n          writeQueries\n        }\n        dimensions {\n          date\n          databaseId\n        }\n      }\n    }\n  }\n}\ngraphql\nquery D1ObservabilitySampleQuery2(\n  $accountTag: string!\n  $start: Date\n  $end: Date\n  $databaseId: string\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountId }) {\n      d1AnalyticsAdaptiveGroups(\n        limit: 10000\n        filter: { date_geq: $start, date_leq: $end, databaseId: $databaseId }\n        orderBy: [date_DESC]\n      ) {\n        quantiles {\n          queryBatchTimeMsP90\n        }\n        dimensions {\n          date\n          databaseId\n        }\n      }\n    }\n  }\n}\ngraphql\nquery D1ObservabilitySampleQuery3(\n  $accountTag: string!\n  $start: Date\n  $end: Date\n  $databaseId: string\n) {\n  viewer {\n    accounts(filter: { accountTag: $accountTag }) {\n      d1AnalyticsAdaptiveGroups(\n        limit: 10000\n        filter: { date_geq: $start, date_leq: $end, databaseId: $databaseId }\n      ) {\n        sum {\n          readQueries\n          writeQueries\n        }\n      }\n    }\n  }\n}\nsh\nnpx wrangler d1 insights <database_name> --sort-type=sum --sort-by=count --limit=3\nsh\n â›…ï¸ wrangler 3.95.0\n-------------------\n\n-------------------\nğŸš§ `wrangler d1 insights` is an experimental command.\nğŸš§ Flags for this command, their descriptions, and output may change between wrangler versions.\n-------------------\n\n[\n  {\n    \"query\": \"SELECT tbl_name as name,\\n                   (SELECT ncol FROM pragma_table_list(tbl_name)) as num_columns\\n            FROM sqlite_master\\n            WHERE TYPE = \\\"table\\\"\\n              AND tbl_name NOT LIKE \\\"sqlite_%\\\"\\n              AND tbl_name NOT LIKE \\\"d1_%\\\"\\n              AND tbl_name NOT LIKE \\\"_cf_%\\\"\\n            ORDER BY tbl_name ASC;\",\n    \"avgRowsRead\": 2,\n    \"totalRowsRead\": 4,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 0.49505,\n    \"totalDurationMs\": 0.9901,\n    \"numberOfTimesRun\": 2,\n    \"queryEfficiency\": 0\n  },\n  {\n    \"query\": \"SELECT * FROM Customers\",\n    \"avgRowsRead\": 4,\n    \"totalRowsRead\": 4,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 0.1873,\n    \"totalDurationMs\": 0.1873,\n    \"numberOfTimesRun\": 1,\n    \"queryEfficiency\": 1\n  },\n  {\n    \"query\": \"SELECT * From Customers\",\n    \"avgRowsRead\": 0,\n    \"totalRowsRead\": 0,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 1.0225,\n    \"totalDurationMs\": 1.0225,\n    \"numberOfTimesRun\": 1,\n    \"queryEfficiency\": 0\n  }\n]\nsh\nnpx wrangler d1 insights <database_name> --sort-type=avg --sort-by=time --limit=3\nsh\nâ›…ï¸ wrangler 3.95.0\n-------------------\n\n-------------------\nğŸš§ `wrangler d1 insights` is an experimental command.\nğŸš§ Flags for this command, their descriptions, and output may change between wrangler versions.\n-------------------\n\n[\n  {\n    \"query\": \"SELECT * From Customers\",\n    \"avgRowsRead\": 0,\n    \"totalRowsRead\": 0,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 1.0225,\n    \"totalDurationMs\": 1.0225,\n    \"numberOfTimesRun\": 1,\n    \"queryEfficiency\": 0\n  },\n  {\n    \"query\": \"SELECT tbl_name as name,\\n                   (SELECT ncol FROM pragma_table_list(tbl_name)) as num_columns\\n            FROM sqlite_master\\n            WHERE TYPE = \\\"table\\\"\\n              AND tbl_name NOT LIKE \\\"sqlite_%\\\"\\n              AND tbl_name NOT LIKE \\\"d1_%\\\"\\n              AND tbl_name NOT LIKE \\\"_cf_%\\\"\\n            ORDER BY tbl_name ASC;\",\n    \"avgRowsRead\": 2,\n    \"totalRowsRead\": 4,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 0.49505,\n    \"totalDurationMs\": 0.9901,\n    \"numberOfTimesRun\": 2,\n    \"queryEfficiency\": 0\n  },\n  {\n    \"query\": \"SELECT * FROM Customers\",\n    \"avgRowsRead\": 4,\n    \"totalRowsRead\": 4,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 0.1873,\n    \"totalDurationMs\": 0.1873,\n    \"numberOfTimesRun\": 1,\n    \"queryEfficiency\": 1\n  }\n]\nsh\nnpx wrangler d1 insights <database_name> --sort-type=sum --sort-by=writes --limit=10 --timePeriod=7d\nsh\nâ›…ï¸ wrangler 3.95.0\n-------------------\n\n-------------------\nğŸš§ `wrangler d1 insights` is an experimental command.\nğŸš§ Flags for this command, their descriptions, and output may change between wrangler versions.\n-------------------\n\n[\n  {\n    \"query\": \"SELECT * FROM Customers\",\n    \"avgRowsRead\": 4,\n    \"totalRowsRead\": 4,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 0.1873,\n    \"totalDurationMs\": 0.1873,\n    \"numberOfTimesRun\": 1,\n    \"queryEfficiency\": 1\n  },\n  {\n    \"query\": \"SELECT * From Customers\",\n    \"avgRowsRead\": 0,\n    \"totalRowsRead\": 0,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 1.0225,\n    \"totalDurationMs\": 1.0225,\n    \"numberOfTimesRun\": 1,\n    \"queryEfficiency\": 0\n  },\n  {\n    \"query\": \"SELECT tbl_name as name,\\n                   (SELECT ncol FROM pragma_table_list(tbl_name)) as num_columns\\n            FROM sqlite_master\\n            WHERE TYPE = \\\"table\\\"\\n              AND tbl_name NOT LIKE \\\"sqlite_%\\\"\\n              AND tbl_name NOT LIKE \\\"d1_%\\\"\\n              AND tbl_name NOT LIKE \\\"_cf_%\\\"\\n            ORDER BY tbl_name ASC;\",\n    \"avgRowsRead\": 2,\n    \"totalRowsRead\": 4,\n    \"avgRowsWritten\": 0,\n    \"totalRowsWritten\": 0,\n    \"avgDurationMs\": 0.49505,\n    \"totalDurationMs\": 0.9901,\n    \"numberOfTimesRun\": 2,\n    \"queryEfficiency\": 0\n  }\n]\nsh\nnpx wrangler d1 info <database_name>\nplaintext\n...\nâ”‚ version           â”‚ alpha                                 â”‚\n...\nsh\nnpx wrangler d1 backup create <alpha_database_name>\nsh\nnpx wrangler d1 backup download <alpha_database_name> <backup_id> # See available backups with wrangler d1 backup list <database_name>\nsh\nsqlite3 db_dump.sqlite3 .dump > db.sql\nsql\n   CREATE TABLE _cf_KV (\n      key TEXT PRIMARY KEY,\n      value BLOB\n   ) WITHOUT ROWID;\n   sh\nnpx wrangler d1 create <new_database_name>\nsh\nnpx wrangler d1 execute <new_database_name> --remote --file=./db.sql\nsh\nnpx wrangler d1 delete <alpha_database_name>\njson\n\"meta\": {\n  \"duration\": 0.20472300052642825,\n  \"size_after\": 45137920,\n  \"rows_read\": 5000,\n  \"rows_written\": 0\n}\nts\n// retrieve bookmark from previous session stored in HTTP header\nconst bookmark = request.headers.get(\"x-d1-bookmark\") ?? \"first-unconstrained\";\n\nconst session = env.DB.withSession(bookmark);\nconst result = await session\n  .prepare(`SELECT * FROM Customers WHERE CompanyName = 'Bs Beverages'`)\n  .run();\n// store bookmark for a future session\nresponse.headers.set(\"x-d1-bookmark\", session.getBookmark() ?? \"\");\njson\n\"meta\": {\n  \"duration\": 0.20472300052642825,\n  \"size_after\": 45137920,\n  \"rows_read\": 5000,\n  \"rows_written\": 0\n}\nsh\n$ wrangler d1 create your-database --experimental-backend\nsh\nwrangler d1 backup list existing-db\nsh\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ created_at   â”‚ id                                   â”‚ num_tables â”‚ size    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1 hour ago   â”‚ 54a23309-db00-4c5c-92b1-c977633b937c â”‚ 1          â”‚ 95.3 kB â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ <...>        â”‚ <...>                                â”‚ <...>      â”‚ <...>   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 2 months ago â”‚ 8433a91e-86d0-41a3-b1a3-333b080bca16 â”‚ 1          â”‚ 65.5 kB â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜%\nsh\nwrangler d1 backup create example-db\nsh\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ created_at                  â”‚ id                                   â”‚ num_tables â”‚ size    â”‚ state â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 2023-02-04T15:49:36.113753Z â”‚ 123a81a2-ab91-4c2e-8ebc-64d69633faf1 â”‚ 1          â”‚ 65.5 kB â”‚ done  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\nsh\nwrangler d1 backup download example-db 123a81a2-ab91-4c2e-8ebc-64d69633faf1\nsh\nğŸŒ€ Downloading backup 123a81a2-ab91-4c2e-8ebc-64d69633faf1 from 'example-db'\nğŸŒ€ Saving to /Users/you/projects/example-db.123a81a2.sqlite3\nğŸŒ€ Done!\nsh\nwrangler d1 backup restore existing-db  6cceaf8c-ceab-4351-ac85-7f9e606973e3\nsh\nRestoring existing-db from backup 6cceaf8c-ceab-4351-ac85-7f9e606973e3....\nDone!\njson\n\"meta\": {\n  \"duration\": 0.20472300052642825,\n  \"size_after\": 45137920,\n  \"rows_read\": 5000,\n  \"rows_written\": 0\n}\nsql\nCREATE TABLE some_table (\n    -- other columns omitted\n    some_generated_column AS <function_that_generates_the_column_data>\n)\njson\n{\n    \"measurement\": {\n        \"temp_f\": \"77.4\",\n        \"aqi\": [21, 42, 58],\n        \"o3\": [18, 500],\n        \"wind_mph\": \"13\",\n        \"location\": \"US-NY\"\n    }\n}\nsql\nCREATE TABLE sensor_readings (\n    event_id INTEGER PRIMARY KEY,\n    timestamp INTEGER NOT NULL,\n    raw_data TEXT,\n    location as (json_extract(raw_data, '$.measurement.location')) STORED\n);\nsql\nALTER TABLE sensor_readings\nADD COLUMN location as (json_extract(raw_data, '$.measurement.location'));\nsql\nALTER TABLE your_table\n-- date(timestamp, 'unixepoch') converts a Unix timestamp to a YYYY-MM-dd formatted date\nADD COLUMN formatted_date AS (date(timestamp, 'unixepoch'))\nsql\n-- Filter out \"expired\" results based on your generated column:\n-- SELECT * FROM your_table WHERE current_date() > expires_at\nALTER TABLE your_table\n-- calculates a date (YYYY-MM-dd) 30 days from the timestamp.\nADD COLUMN expires_at AS (date(timestamp, '+30 days'));\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"d1_databases\": [\n      {\n        \"binding\": \"<BINDING_NAME>\",\n        \"database_name\": \"<DATABASE_NAME>\",\n        \"database_id\": \"<UUID>\",\n        \"preview_database_id\": \"<UUID>\",\n        \"migrations_table\": \"<d1_migrations>\",\n        \"migrations_dir\": \"<FOLDER_NAME>\"\n      }\n    ]\n  }\n  toml\n  [[ d1_databases ]]\n  binding = \"<BINDING_NAME>\" # i.e. if you set this to \"DB\", it will be available in your Worker at `env.DB`\n  database_name = \"<DATABASE_NAME>\"\n  database_id = \"<UUID>\"\n  preview_database_id = \"<UUID>\"\n  migrations_table = \"<d1_migrations>\" # Customize this value to change your applied migrations table name\n  migrations_dir = \"<FOLDER_NAME>\" # Specify your custom migration directory\n  sh\nwrangler d1 time-travel info YOUR_DATABASE\nsh\nğŸš§ Time Traveling...\nâš ï¸ The current bookmark is '00000085-0000024c-00004c6d-8e61117bf38d7adb71b934ebbf891683'\nâš¡ï¸ To restore to this specific bookmark, run:\n `wrangler d1 time-travel restore YOUR_DATABASE --bookmark=00000085-0000024c-00004c6d-8e61117bf38d7adb71b934ebbf891683`\nsh\nwrangler d1 time-travel info YOUR_DATABASE --timestamp=\"2023-07-09T17:31:11+00:00\"\nsh\nwrangler d1 time-travel restore YOUR_DATABASE --timestamp=UNIX_TIMESTAMP\nsh\nğŸš§ Restoring database YOUR_DATABASE from bookmark 00000080-ffffffff-00004c60-390376cb1c4dd679b74a19d19f5ca5be\n\nâš ï¸ This will overwrite all data in database YOUR_DATABASE.\nIn-flight queries and transactions will be cancelled.\n\nâœ” OK to proceed (y/N) â€¦ yes\nâš¡ï¸ Time travel in progress...\nâœ… Database YOUR_DATABASE restored back to bookmark 00000080-ffffffff-00004c60-390376cb1c4dd679b74a19d19f5ca5be\n\nâ†©ï¸ To undo this operation, you can restore to the previous bookmark: 00000085-ffffffff-00004c6d-2510c8b03a2eb2c48b2422bb3b33fad5\nsh\nwrangler d1 time-travel info YOUR_DATABASE\nsh\nğŸš§ Time Traveling...\nâš ï¸ The current bookmark is '00000085-0000024c-00004c6d-8e61117bf38d7adb71b934ebbf891683'\nâš¡ï¸ To restore to this specific bookmark, run:\n `wrangler d1 time-travel restore YOUR_DATABASE --bookmark=00000085-0000024c-00004c6d-8e61117bf38d7adb71b934ebbf891683`\nsql\n-- Defer foreign key enforcement in this transaction.\nPRAGMA defer_foreign_keys = on\n\n-- Run your CREATE TABLE or ALTER TABLE / COLUMN statements\nALTER TABLE users ...\n\n-- This is implicit if not set by the end of the transaction.\nPRAGMA defer_foreign_keys = off\nsql\nCREATE TABLE users (\n    user_id INTEGER PRIMARY KEY,\n    email_address TEXT,\n    name TEXT,\n    metadata TEXT\n)\n\nCREATE TABLE orders (\n    order_id INTEGER PRIMARY KEY,\n    status INTEGER,\n    item_desc TEXT,\n    shipped_date INTEGER,\n    user_who_ordered INTEGER,\n    FOREIGN KEY(user_who_ordered) REFERENCES users(user_id)\n)\nsql\nCREATE TABLE users (\n    user_id INTEGER PRIMARY KEY,\n    email_address TEXT,\n)\n\nCREATE TABLE scores (\n    score_id INTEGER PRIMARY KEY,\n    game TEXT,\n    score INTEGER,\n    player_id INTEGER,\n    FOREIGN KEY(player_id) REFERENCES users(user_id) ON DELETE CASCADE\n)\nsql\nSELECT json_extract('not valid JSON: just a string', '$')\ntxt\nERROR 9015: SQL engine error: query error: Error code 1: SQL error or missing database (malformed\n  JSON)`\nsql\nCREATE TABLE some_table (\n    -- other columns omitted\n    raw_data TEXT -- JSON: {\"measurement\":{\"aqi\":[21,42,58],\"wind_mph\":\"13\",\"location\":\"US-NY\"}}\n    location AS (json_extract(raw_data, '$.measurement.location')) STORED\n)\njson\n{\n    \"measurement\": {\n        \"temp_f\": \"77.4\",\n        \"aqi\": [21, 42, 58],\n        \"o3\": [18, 500],\n        \"wind_mph\": \"13\",\n        \"location\": \"US-NY\"\n    }\n}\nsql\n-- Extract the temperature value\njson_extract(sensor_reading, '$.measurement.temp_f')-- returns \"77.4\" as TEXT\nsql\n-- Extract the maximum PM2.5 air quality reading\nsensor_reading -> '$.measurement.aqi[3]' -- returns 58 as a JSON number\nsql\n-- Extract the o3 (ozone) array in full\nsensor_reading -\\-> '$.measurement.o3' -- returns '[18, 500]' as TEXT\njson\n{\n    \"user_id\": \"abc12345\",\n    \"previous_logins\": [\"2023-03-31T21:07:14-05:00\", \"2023-03-28T08:21:02-05:00\", \"2023-03-28T05:52:11-05:00\"]\n}\nsql\njson_array_length(login_history, '$.previous_logins') --> returns 3 as an INTEGER\njson\n{\"history\": [\"2023-05-13T15:13:02+00:00\", \"2023-05-14T07:11:22+00:00\", \"2023-05-15T15:03:51+00:00\"]}\nsql\nUPDATE users\nSET login_history = json_insert(login_history, '$.history[#]', '2023-05-15T20:33:06+00:00')\nWHERE user_id = 'aba0e360-1e04-41b3-91a0-1f2263e1e0fb'\nsql\nUPDATE users\nSET last_audited = '2023-05-16T11:24:08+00:00'\nWHERE id IN (SELECT value FROM json_each('[183183, 13913, 94944]'))\nsql\nkey|value|type|id|fullkey|path\n0|183183|integer|1|$[0]|$\n1|13913|integer|2|$[1]|$\n2|94944|integer|3|$[2]|$\nts\nconst stmt = context.env.DB\n    .prepare(\"UPDATE users SET last_audited = ? WHERE id IN (SELECT value FROM json_each(?1))\")\nconst resp = await stmt.bind(\n    \"2023-05-16T11:24:08+00:00\",\n    JSON.stringify([183183, 13913, 94944])\n    ).run()\nsql\nPRAGMA foreign_keys=off;\nDROP TABLE IF EXISTS \"Employee\";\nDROP TABLE IF EXISTS \"Category\";\nDROP TABLE IF EXISTS \"Customer\";\nDROP TABLE IF EXISTS \"Shipper\";\nDROP TABLE IF EXISTS \"Supplier\";\nDROP TABLE IF EXISTS \"Order\";\nDROP TABLE IF EXISTS \"Product\";\nDROP TABLE IF EXISTS \"OrderDetail\";\nDROP TABLE IF EXISTS \"CustomerCustomerDemo\";\nDROP TABLE IF EXISTS \"CustomerDemographic\";\nDROP TABLE IF EXISTS \"Region\";\nDROP TABLE IF EXISTS \"Territory\";\nDROP TABLE IF EXISTS \"EmployeeTerritory\";\nDROP VIEW IF EXISTS [ProductDetails_V];\nCREATE TABLE IF NOT EXISTS \"Employee\" ( \"Id\" INTEGER PRIMARY KEY, \"LastName\" VARCHAR(8000) NULL, \"FirstName\" VARCHAR(8000) NULL, \"Title\" VARCHAR(8000) NULL, \"TitleOfCourtesy\" VARCHAR(8000) NULL, \"BirthDate\" VARCHAR(8000) NULL, \"HireDate\" VARCHAR(8000) NULL, \"Address\" VARCHAR(8000) NULL, \"City\" VARCHAR(8000) NULL, \"Region\" VARCHAR(8000) NULL, \"PostalCode\" VARCHAR(8000) NULL, \"Country\" VARCHAR(8000) NULL, \"HomePhone\" VARCHAR(8000) NULL, \"Extension\" VARCHAR(8000) NULL, \"Photo\" BLOB NULL, \"Notes\" VARCHAR(8000) NULL, \"ReportsTo\" INTEGER NULL, \"PhotoPath\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"Category\" ( \"Id\" INTEGER PRIMARY KEY, \"CategoryName\" VARCHAR(8000) NULL, \"Description\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"Customer\" ( \"Id\" VARCHAR(8000) PRIMARY KEY, \"CompanyName\" VARCHAR(8000) NULL, \"ContactName\" VARCHAR(8000) NULL, \"ContactTitle\" VARCHAR(8000) NULL, \"Address\" VARCHAR(8000) NULL, \"City\" VARCHAR(8000) NULL, \"Region\" VARCHAR(8000) NULL, \"PostalCode\" VARCHAR(8000) NULL, \"Country\" VARCHAR(8000) NULL, \"Phone\" VARCHAR(8000) NULL, \"Fax\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"Shipper\" ( \"Id\" INTEGER PRIMARY KEY, \"CompanyName\" VARCHAR(8000) NULL, \"Phone\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"Supplier\" ( \"Id\" INTEGER PRIMARY KEY, \"CompanyName\" VARCHAR(8000) NULL, \"ContactName\" VARCHAR(8000) NULL, \"ContactTitle\" VARCHAR(8000) NULL, \"Address\" VARCHAR(8000) NULL, \"City\" VARCHAR(8000) NULL, \"Region\" VARCHAR(8000) NULL, \"PostalCode\" VARCHAR(8000) NULL, \"Country\" VARCHAR(8000) NULL, \"Phone\" VARCHAR(8000) NULL, \"Fax\" VARCHAR(8000) NULL, \"HomePage\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"Order\" ( \"Id\" INTEGER PRIMARY KEY, \"CustomerId\" VARCHAR(8000) NULL, \"EmployeeId\" INTEGER NOT NULL, \"OrderDate\" VARCHAR(8000) NULL, \"RequiredDate\" VARCHAR(8000) NULL, \"ShippedDate\" VARCHAR(8000) NULL, \"ShipVia\" INTEGER NULL, \"Freight\" DECIMAL NOT NULL, \"ShipName\" VARCHAR(8000) NULL, \"ShipAddress\" VARCHAR(8000) NULL, \"ShipCity\" VARCHAR(8000) NULL, \"ShipRegion\" VARCHAR(8000) NULL, \"ShipPostalCode\" VARCHAR(8000) NULL, \"ShipCountry\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"Product\" ( \"Id\" INTEGER PRIMARY KEY, \"ProductName\" VARCHAR(8000) NULL, \"SupplierId\" INTEGER NOT NULL, \"CategoryId\" INTEGER NOT NULL, \"QuantityPerUnit\" VARCHAR(8000) NULL, \"UnitPrice\" DECIMAL NOT NULL, \"UnitsInStock\" INTEGER NOT NULL, \"UnitsOnOrder\" INTEGER NOT NULL, \"ReorderLevel\" INTEGER NOT NULL, \"Discontinued\" INTEGER NOT NULL);\nCREATE TABLE IF NOT EXISTS \"OrderDetail\" ( \"Id\" VARCHAR(8000) PRIMARY KEY, \"OrderId\" INTEGER NOT NULL, \"ProductId\" INTEGER NOT NULL, \"UnitPrice\" DECIMAL NOT NULL, \"Quantity\" INTEGER NOT NULL, \"Discount\" DOUBLE NOT NULL);\nCREATE TABLE IF NOT EXISTS \"CustomerCustomerDemo\" ( \"Id\" VARCHAR(8000) PRIMARY KEY, \"CustomerTypeId\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"CustomerDemographic\" ( \"Id\" VARCHAR(8000) PRIMARY KEY, \"CustomerDesc\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"Region\" ( \"Id\" INTEGER PRIMARY KEY, \"RegionDescription\" VARCHAR(8000) NULL);\nCREATE TABLE IF NOT EXISTS \"Territory\" ( \"Id\" VARCHAR(8000) PRIMARY KEY, \"TerritoryDescription\" VARCHAR(8000) NULL, \"RegionId\" INTEGER NOT NULL);\nCREATE TABLE IF NOT EXISTS \"EmployeeTerritory\" ( \"Id\" VARCHAR(8000) PRIMARY KEY, \"EmployeeId\" INTEGER NOT NULL, \"TerritoryId\" VARCHAR(8000) NULL);\nCREATE VIEW [ProductDetails_V] as select p.*, c.CategoryName, c.Description as [CategoryDescription], s.CompanyName as [SupplierName], s.Region as [SupplierRegion] from [Product] p join [Category] c on p.CategoryId = c.id join [Supplier] s on s.id = p.SupplierId;\nsh\nnpx wrangler d1 execute [DATABASE_NAME] --command='PRAGMA table_list'\nsh\nğŸŒ€ Executing on remote database [DATABASE_NAME] (DATABASE_ID):\nğŸŒ€ To execute on your local development database, remove the --remote flag from your wrangler command.\nğŸš£ Executed 1 commands in 0.5874ms\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ schema â”‚ name                 â”‚ type  â”‚ ncol â”‚ wr â”‚ strict â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Territory            â”‚ table â”‚ 3    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ CustomerDemographic  â”‚ table â”‚ 2    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ OrderDetail          â”‚ table â”‚ 6    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ sqlite_schema        â”‚ table â”‚ 5    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Region               â”‚ table â”‚ 2    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ _cf_KV               â”‚ table â”‚ 2    â”‚ 1  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ ProductDetails_V     â”‚ view  â”‚ 14   â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ EmployeeTerritory    â”‚ table â”‚ 3    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Employee             â”‚ table â”‚ 18   â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Category             â”‚ table â”‚ 3    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Customer             â”‚ table â”‚ 11   â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Shipper              â”‚ table â”‚ 3    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Supplier             â”‚ table â”‚ 12   â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Order                â”‚ table â”‚ 14   â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ CustomerCustomerDemo â”‚ table â”‚ 2    â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ main   â”‚ Product              â”‚ table â”‚ 10   â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ temp   â”‚ sqlite_temp_schema   â”‚ table â”‚ 5    â”‚ 0  â”‚ 0      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nsh\nnpx wrangler d1 execute [DATABASE_NAME] --command='PRAGMA table_info(\"Order\")'\nsh\nğŸŒ€ Executing on remote database [DATABASE_NAME] (DATABASE_ID):\nğŸŒ€ To execute on your local development database, remove the --remote flag from your wrangler command.\nğŸš£ Executed 1 commands in 0.8502ms\nâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”\nâ”‚ cid â”‚ name           â”‚ type          â”‚ notnull â”‚ dflt_value â”‚ pk â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 0   â”‚ Id             â”‚ INTEGER       â”‚ 0       â”‚            â”‚ 1  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 1   â”‚ CustomerId     â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 2   â”‚ EmployeeId     â”‚ INTEGER       â”‚ 1       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 3   â”‚ OrderDate      â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 4   â”‚ RequiredDate   â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 5   â”‚ ShippedDate    â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 6   â”‚ ShipVia        â”‚ INTEGER       â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 7   â”‚ Freight        â”‚ DECIMAL       â”‚ 1       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 8   â”‚ ShipName       â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 9   â”‚ ShipAddress    â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 10  â”‚ ShipCity       â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 11  â”‚ ShipRegion     â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 12  â”‚ ShipPostalCode â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚ 13  â”‚ ShipCountry    â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚\nâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜\nsh\nnpx wrangler d1 execute [DATABASE_NAME] --command='PRAGMA table_xinfo(\"Order\")'\nsh\nğŸŒ€ Executing on remote database [DATABASE_NAME] (DATABASE_ID):\nğŸŒ€ To execute on your local development database, remove the --remote flag from your wrangler command.\nğŸš£ Executed 1 commands in 0.3854ms\nâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ cid â”‚ name           â”‚ type          â”‚ notnull â”‚ dflt_value â”‚ pk â”‚ hidden â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 0   â”‚ Id             â”‚ INTEGER       â”‚ 0       â”‚            â”‚ 1  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1   â”‚ CustomerId     â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 2   â”‚ EmployeeId     â”‚ INTEGER       â”‚ 1       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 3   â”‚ OrderDate      â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 4   â”‚ RequiredDate   â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 5   â”‚ ShippedDate    â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 6   â”‚ ShipVia        â”‚ INTEGER       â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 7   â”‚ Freight        â”‚ DECIMAL       â”‚ 1       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 8   â”‚ ShipName       â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 9   â”‚ ShipAddress    â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 10  â”‚ ShipCity       â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 11  â”‚ ShipRegion     â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 12  â”‚ ShipPostalCode â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 13  â”‚ ShipCountry    â”‚ VARCHAR(8000) â”‚ 0       â”‚            â”‚ 0  â”‚ 0      â”‚\nâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nsh\nnpx wrangler d1 execute [DATABASE_NAME] --command='PRAGMA index_list(\"Territory\")'\nsh\nğŸŒ€ Executing on remote database d1-pragma-db (DATABASE_ID):\nğŸŒ€ To execute on your local development database, remove the --remote flag from your wrangler command.\nğŸš£ Executed 1 commands in 0.2177ms\nâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ seq â”‚ name                         â”‚ unique â”‚ origin â”‚ partial â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 0   â”‚ sqlite_autoindex_Territory_1 â”‚ 1      â”‚ pk     â”‚ 0       â”‚\nâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nsh\nnpx wrangler d1 execute [DATABASE_NAME] --command='PRAGMA index_info(\"sqlite_autoindex_Territory_1\")'\nsh\nğŸŒ€ Executing on remote database d1-pragma-db (DATABASE_ID):\nğŸŒ€ To execute on your local development database, remove the --remote flag from your wrangler command.\nğŸš£ Executed 1 commands in 0.2523ms\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\nâ”‚ seqno â”‚ cid â”‚ name â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 0     â”‚ 0   â”‚ Id   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\nsh\nnpx wrangler d1 execute [DATABASE_NAME] --command='PRAGMA index_xinfo(\"sqlite_autoindex_Territory_1\")'\nsh\nğŸŒ€ Executing on remote database d1-pragma-db (DATABASE_ID):\nğŸŒ€ To execute on your local development database, remove the --remote flag from your wrangler command.\nğŸš£ Executed 1 commands in 0.6034ms\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\nâ”‚ seqno â”‚ cid â”‚ name â”‚ desc â”‚ coll   â”‚ key â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤\nâ”‚ 0     â”‚ 0   â”‚ Id   â”‚ 0    â”‚ BINARY â”‚ 1   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤\nâ”‚ 1     â”‚ -1  â”‚      â”‚ 0    â”‚ BINARY â”‚ 0   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\nsh\nnpx wrangler d1 execute [DATABASE_NAME] --command='PRAGMA quick_check'\nsh\nğŸŒ€ Executing on remote database [DATABASE_NAME] (DATABASE_ID):\nğŸŒ€ To execute on your local development database, remove the --remote flag from your wrangler command.\nğŸš£ Executed 1 commands in 1.4073ms\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ quick_check â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ ok          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nsql\n-- Defer foreign key enforcement in this transaction.\nPRAGMA defer_foreign_keys = on\n\n-- Run your CREATE TABLE or ALTER TABLE / COLUMN statements\nALTER TABLE users ...\n\n-- This is implicit if not set by the end of the transaction.\nPRAGMA defer_foreign_keys = off\nsql\nSELECT name, sql FROM sqlite_master\njson\n      {\n        \"name\": \"users\",\n        \"sql\": \"CREATE TABLE users ( user_id INTEGER PRIMARY KEY, email_address TEXT, created_at INTEGER, deleted INTEGER, settings TEXT)\"\n      },\n      {\n        \"name\": \"idx_ordered_users\",\n        \"sql\": \"CREATE INDEX idx_ordered_users ON users(created_at DESC)\"\n      },\n      {\n        \"name\": \"Order\",\n        \"sql\": \"CREATE TABLE \\\"Order\\\" ( \\\"Id\\\" INTEGER PRIMARY KEY, \\\"CustomerId\\\" VARCHAR(8000) NULL, \\\"EmployeeId\\\" INTEGER NOT NULL, \\\"OrderDate\\\" VARCHAR(8000) NULL, \\\"RequiredDate\\\" VARCHAR(8000) NULL, \\\"ShippedDate\\\" VARCHAR(8000) NULL, \\\"ShipVia\\\" INTEGER NULL, \\\"Freight\\\" DECIMAL NOT NULL, \\\"ShipName\\\" VARCHAR(8000) NULL, \\\"ShipAddress\\\" VARCHAR(8000) NULL, \\\"ShipCity\\\" VARCHAR(8000) NULL, \\\"ShipRegion\\\" VARCHAR(8000) NULL, \\\"ShipPostalCode\\\" VARCHAR(8000) NULL, \\\"ShipCountry\\\" VARCHAR(8000) NULL)\"\n      },\n      {\n        \"name\": \"Product\",\n        \"sql\": \"CREATE TABLE \\\"Product\\\" ( \\\"Id\\\" INTEGER PRIMARY KEY, \\\"ProductName\\\" VARCHAR(8000) NULL, \\\"SupplierId\\\" INTEGER NOT NULL, \\\"CategoryId\\\" INTEGER NOT NULL, \\\"QuantityPerUnit\\\" VARCHAR(8000) NULL, \\\"UnitPrice\\\" DECIMAL NOT NULL, \\\"UnitsInStock\\\" INTEGER NOT NULL, \\\"UnitsOnOrder\\\" INTEGER NOT NULL, \\\"ReorderLevel\\\" INTEGER NOT NULL, \\\"Discontinued\\\" INTEGER NOT NULL)\"\n      }\njs\nconst { results } = await env.DB.prepare(\n  \"SELECT * FROM Customers WHERE CompanyName LIKE ?\",\n)\n  .bind(\"%eve%\")\n  .run();\nconsole.log(\"results: \", results);\njs\nresults:  [...]\nsh\n  npm create cloudflare@latest -- d1-example\n  sh\n  yarn create cloudflare d1-example\n  sh\n  pnpm create cloudflare@latest d1-example\n  sh\ncd d1-example\nsh\n  npm i hono\n  sh\n  yarn add hono\n  sh\n  pnpm add hono\n  js\nimport { Hono } from \"hono\";\n\nconst app = new Hono();\n\napp.get(\"/api/posts/:slug/comments\", async (c) => {\n  // Do something and return an HTTP response\n  // Optionally, do something with `c.req.param(\"slug\")`\n});\n\napp.post(\"/api/posts/:slug/comments\", async (c) => {\n  // Do something and return an HTTP response\n  // Optionally, do something with `c.req.param(\"slug\")`\n});\n\nexport default app;\nsh\nnpx wrangler d1 create d1-example\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"d1_databases\": [\n      {\n        \"binding\": \"DB\",\n        \"database_name\": \"d1-example\",\n        \"database_id\": \"4e1c28a9-90e4-41da-8b4b-6cf36e5abb29\"\n      }\n    ]\n  }\n  toml\n  [[ d1_databases ]]\n  binding = \"DB\" # available in your Worker on `env.DB`\n  database_name = \"d1-example\"\n  database_id = \"4e1c28a9-90e4-41da-8b4b-6cf36e5abb29\"\n  sh\nnpx wrangler d1 execute d1-example --remote --command \"SELECT name FROM sqlite_schema WHERE type ='table'\"\nsh\nExecuting on d1-example:\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ name  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ d1_kv â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”˜\nsql\nDROP TABLE IF EXISTS comments;\nCREATE TABLE IF NOT EXISTS comments (\n  id integer PRIMARY KEY AUTOINCREMENT,\n  author text NOT NULL,\n  body text NOT NULL,\n  post_slug text NOT NULL\n);\nCREATE INDEX idx_comments_post_slug ON comments (post_slug);\n\n-- Optionally, uncomment the below query to create data\n\n-- INSERT INTO COMMENTS (author, body, post_slug) VALUES ('Kristian', 'Great post!', 'hello-world');\nsh\nnpx wrangler d1 execute d1-example --remote --file schemas/schema.sql\njs\napp.get(\"/api/posts/:slug/comments\", async (c) => {\n  const { slug } = c.req.param();\n  const { results } = await c.env.DB.prepare(\n    `\n    select * from comments where post_slug = ?\n  `,\n  )\n    .bind(slug)\n    .run();\n  return c.json(results);\n});\njs\napp.post(\"/api/posts/:slug/comments\", async (c) => {\n  const { slug } = c.req.param();\n  const { author, body } = await c.req.json();\n\nif (!author) return c.text(\"Missing author value for new comment\");\n  if (!body) return c.text(\"Missing body value for new comment\");\n\nconst { success } = await c.env.DB.prepare(\n    `\n    insert into comments (author, body, post_slug) values (?, ?, ?)\n  `,\n  )\n    .bind(author, body, slug)\n    .run();\n\nif (success) {\n    c.status(201);\n    return c.text(\"Created\");\n  } else {\n    c.status(500);\n    return c.text(\"Something went wrong\");\n  }\n});\njsonc\n  {\n    \"$schema\": \"./node_modules/wrangler/config-schema.json\",\n    \"name\": \"d1-example\",\n    \"main\": \"src/worker.js\",\n    \"compatibility_date\": \"2022-07-15\",\n    \"d1_databases\": [\n      {\n        \"binding\": \"DB\",\n        \"database_name\": \"<YOUR_DATABASE_NAME>\",\n        \"database_id\": \"<YOUR_DATABASE_UUID>\"\n      }\n    ]\n  }\n  toml\n  name = \"d1-example\"\n  main = \"src/worker.js\"\n  compatibility_date = \"2022-07-15\"\n\n[[ d1_databases ]]\n  binding = \"DB\" # available in your Worker on env.DB\n  database_name = \"<YOUR_DATABASE_NAME>\"\n  database_id = \"<YOUR_DATABASE_UUID>\"\n  sh\nnpx wrangler deploy\nsh",
  "code_samples": [
    {
      "code": "Your Worker will be available at `https://python-and-d1.YOUR_SUBDOMAIN.workers.dev`.\n\nIf you receive an error deploying:\n\n* Make sure you have configured your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/) with the `database_id` and `database_name` of a valid D1 database.\n* Ensure `compatibility_flags = [\"python_workers\"]` is set in your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/), which is required for Python.\n* Review the [list of error codes](https://developers.cloudflare.com/workers/observability/errors/), and ensure your code does not throw an uncaught exception.\n\n## Next steps\n\n* Refer to [Workers Python documentation](https://developers.cloudflare.com/workers/languages/python/) to learn more about how to use Python in Workers.\n* Review the [D1 Workers Binding API](https://developers.cloudflare.com/d1/worker-api/) and how to query D1 databases.\n* Learn [how to import data](https://developers.cloudflare.com/d1/best-practices/import-export-data/) to your D1 database.\n\n</page>\n\n<page>\n---\ntitle: Audit Logs Â· Cloudflare D1 docs\ndescription: Audit logs provide a comprehensive summary of changes made within\n  your Cloudflare account, including those made to D1 databases. This\n  functionality is available on all plan types, free of charge, and is always\n  enabled.\nlastUpdated: 2025-09-03T16:40:54.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/observability/audit-logs/\n  md: https://developers.cloudflare.com/d1/observability/audit-logs/index.md\n---\n\n[Audit logs](https://developers.cloudflare.com/fundamentals/account/account-security/review-audit-logs/) provide a comprehensive summary of changes made within your Cloudflare account, including those made to D1 databases. This functionality is available on all plan types, free of charge, and is always enabled.\n\n## Viewing audit logs\n\nTo view audit logs for your D1 databases, go to the **Audit Logs** page.\n\n[Go to **Audit logs**](https://dash.cloudflare.com/?to=/:account/audit-log)\n\nFor more information on how to access and use audit logs, refer to [Review audit logs](https://developers.cloudflare.com/fundamentals/account/account-security/review-audit-logs/).\n\n## Logged operations\n\nThe following configuration actions are logged:\n\n| Operation | Description |\n| - | - |\n| CreateDatabase | Creation of a new database. |\n| DeleteDatabase | Deletion of an existing database. |\n| [TimeTravel](https://developers.cloudflare.com/d1/reference/time-travel) | Restoration of a past database version. |\n\n## Example log entry\n\nBelow is an example of an audit log entry showing the creation of a new database:",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Billing Â· Cloudflare D1 docs\ndescription: D1 exposes analytics to track billing metrics (rows read, rows\n  written, and total storage) across all databases in your account.\nlastUpdated: 2025-09-03T16:40:54.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/observability/billing/\n  md: https://developers.cloudflare.com/d1/observability/billing/index.md\n---\n\nD1 exposes analytics to track billing metrics (rows read, rows written, and total storage) across all databases in your account.\n\nThe metrics displayed in the [Cloudflare dashboard](https://dash.cloudflare.com/) are sourced from Cloudflare's [GraphQL Analytics API](https://developers.cloudflare.com/analytics/graphql-api/). You can access the metrics [programmatically](https://developers.cloudflare.com/d1/observability/metrics-analytics/#query-via-the-graphql-api) via GraphQL or HTTP client.\n\n## View metrics in the dashboard\n\nTotal account billable usage analytics for D1 are available in the Cloudflare dashboard. To view current and past metrics for an account:\n\n1. In the Cloudflare dashboard, go to the **Billing** page.\n\n   [Go to **Billing**](https://dash.cloudflare.com/?to=/:account/billing)\n\n2. Go to **Billable Usage**.\n\nFrom here you can view charts of your account's D1 usage on a daily or month-to-date timeframe.\n\nNote that billable usage history is stored for a maximum of 30 days.\n\n## Billing Notifications\n\nUsage-based billing notifications are available within the [Cloudflare dashboard](https://dash.cloudflare.com) for users looking to monitor their total account usage.\n\nNotifications on the following metrics are available:\n\n* Rows Read\n* Rows Written\n\n</page>\n\n<page>\n---\ntitle: Debug D1 Â· Cloudflare D1 docs\ndescription: D1 allows you to capture exceptions and log errors returned when\n  querying a database. To debug D1, you will use the same tools available when\n  debugging Workers.\nlastUpdated: 2025-09-17T08:55:05.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/observability/debug-d1/\n  md: https://developers.cloudflare.com/d1/observability/debug-d1/index.md\n---\n\nD1 allows you to capture exceptions and log errors returned when querying a database. To debug D1, you will use the same tools available when [debugging Workers](https://developers.cloudflare.com/workers/observability/).\n\nD1's [`stmt.`](https://developers.cloudflare.com/d1/worker-api/prepared-statements/) and [`db.`](https://developers.cloudflare.com/d1/worker-api/d1-database/) methods throw an [Error object](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error) whenever an error occurs. To capture exceptions, log the `e.message` value.\n\nFor example, the code below has a query with an invalid keyword - `INSERTZ` instead of `INSERT`:",
      "language": "unknown"
    },
    {
      "code": "The code above throws the following error message:",
      "language": "unknown"
    },
    {
      "code": "Note\n\nPrior to [`wrangler` 3.1.1](https://github.com/cloudflare/workers-sdk/releases/tag/wrangler%403.1.1), D1 JavaScript errors used the [cause property](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error/cause) for detailed error messages.\n\nTo inspect these errors when using older versions of `wrangler`, you should log `error?.cause?.message`.\n\n## Error list\n\nD1 returns the following error constants, in addition to the extended (detailed) error message:\n\n| Error message | Description | Recommended action |\n| - | - | - |\n| `D1_ERROR` | Prefix of a specific D1 error. | Refer to \"List of D1\\_ERRORs\" below for more detail about your specific error. |\n| `D1_EXEC_ERROR` | Exec error in line x: y error. | |\n| `D1_TYPE_ERROR` | Returned when there is a mismatch in the type between a column and a value. A common cause is supplying an `undefined` variable (unsupported) instead of `null`. | Ensure the type of the value and the column match. |\n| `D1_COLUMN_NOTFOUND` | Column not found. | Ensure you have selected a column which exists in the database. |\n\nThe following table lists specific instances of `D1_ERROR`.\n\nList of D1\\_ERRORs\n\nRetry operations\n\nWhile some D1 errors can be resolved by retrying the operation, retrying is only safe if your query is idempotent (produces the same result when executed multiple times).\n\nBefore retrying any failed operation:\n\n* Verify your query is idempotent (for example, read-only operations, or queries such as `CREATE TABLE IF NOT EXISTS`).\n* Consider [implementing application-level checks](https://developers.cloudflare.com/d1/best-practices/retry-queries/) to identify if the operation can be retried, and retrying only when it is safe and necessary.\n\n| `D1_ERROR` type | Description | Recommended action |\n| - | - | - |\n| `No SQL statements detected.` | The input query does not contain any SQL statements. | App action: Ensure the query contains at least one valid SQL statement. |\n| `Your account has exceeded D1's maximum account storage limit, please contact Cloudflare to raise your limit.` | The total storage across all D1 databases in the account has exceeded the [account storage limit](https://developers.cloudflare.com/d1/platform/limits/). | App action: Delete unused databases, or upgrade your account to a paid plan. |\n| `Exceeded maximum DB size.` | The D1 database has exceeded its [storage limit](https://developers.cloudflare.com/d1/platform/limits/). | App action: Delete data rows from the database, or shard your data into multiple databases. |\n| `D1 DB reset because its code was updated.` | Cloudflare has updated the code for D1 (or the underlying Durable Object), and the Durable Object which contains the D1 database is restarting. | Retry the operation. |\n| `Internal error while starting up D1 DB storage caused object to be reset.` | The Durable Object containing the D1 database is failing to start. | Retry the operation. |\n| `Network connection lost.` | A network error. | Retry the operation. Refer to the \"Retry operation\" note above. |\n| `Internal error in D1 DB storage caused object to be reset.` | An error has caused the D1 database to restart. | Retry the operation. |\n| `Cannot resolve D1 DB due to transient issue on remote node.` | The query cannot reach the Durable Object containing the D1 database. | Retry the operation. Refer to the \"Retry operation\" note above. |\n| `Can't read from request stream because client disconnected.` | A query request was made (e.g. uploading a SQL query), but the connection was closed during the query was fully executed. | App action: Retry the operation, and ensure the connection stays open. |\n| `D1 DB storage operation exceeded timeout which caused object to be reset.` | A query is trying to write a large amount of information (e.g. GBs), and is taking too long. | App action: Optimize the queries (so that each query takes less time), send fewer requests by spreading the load over time, or shard the queries. |\n| `D1 DB is overloaded. Requests queued for too long.` | The requests to the D1 database are queued for too long, either because there are too many requests, or the queued requests are taking too long. | App action: Optimize the queries (so that each query takes less time), send fewer requests by spreading the load over time, or shard the queries. |\n| `D1 DB is overloaded. Too many requests queued.` | The request queue to the D1 database is too long, either because there are too many requests, or the queued requests are taking too long. | App action: Optimize the queries (so that each query takes less time), send fewer requests by spreading the load over time, or shard the queries. |\n| `D1 DB's isolate exceeded its memory limit and was reset.` | A query loaded too much into memory, causing the D1 database to crash. | App action: Optimize the queries (so that each query takes less time), send fewer requests by spreading the load over time, or shard the queries. |\n| `D1 DB exceeded its CPU time limit and was reset.` | A query is taking up a lot of CPU time (e.g. scanning over 9 GB table, or attempting a large import/export). | App action: Split the query into smaller shards. |\n\n## Automatic retries\n\nD1 detects read-only queries and automatically attempts up to two retries to execute those queries in the event of failures with retryable errors.\n\nD1 ensures that any retry attempt does not cause database writes, making the automatic retries safe from side-effects, even if a query causing modifications slips through the read-only detection. D1 achieves this by checking for modifications after every query execution, and if any write occurred due to a retry attempt, the query is rolled back.\n\nNote\n\nOnly read-only queries (queries containing only the following SQLite keywords: `SELECT`, `EXPLAIN`, `WITH`) are retried. Queries containing any [SQLite keyword](https://sqlite.org/lang_keywords.html) that leads to database writes are not retried.\n\n## View logs\n\nView a stream of live logs from your Worker by using [`wrangler tail`](https://developers.cloudflare.com/workers/observability/logs/real-time-logs#view-logs-using-wrangler-tail) or via the [Cloudflare dashboard](https://developers.cloudflare.com/workers/observability/logs/real-time-logs#view-logs-from-the-dashboard).\n\n## Report issues\n\n* To report bugs or request features, go to the [Cloudflare Community Forums](https://community.cloudflare.com/c/developers/d1/85).\n* To give feedback, go to the [D1 Discord channel](https://discord.com/invite/cloudflaredev).\n* If you are having issues with Wrangler, report issues in the [Wrangler GitHub repository](https://github.com/cloudflare/workers-sdk/issues/new/choose).\n\nYou should include as much of the following in any bug report:\n\n* The ID of your database. Use `wrangler d1 list` to match a database name to its ID.\n* The query (or queries) you ran when you encountered an issue. Ensure you redact any personally identifying information (PII).\n* The Worker code that makes the query, including any calls to `bind()` using the [Workers Binding API](https://developers.cloudflare.com/d1/worker-api/).\n* The full error text, including the content of [`error.cause.message`](#handle-errors).\n\n## Related resources\n\n* Learn [how to debug Workers](https://developers.cloudflare.com/workers/observability/).\n* Understand how to [access logs](https://developers.cloudflare.com/workers/observability/logs/) generated from your Worker and D1.\n* Use [`wrangler dev`](https://developers.cloudflare.com/workers/wrangler/commands/#dev) to run your Worker and D1 locally and [debug issues before deploying](https://developers.cloudflare.com/workers/development-testing/).\n\n</page>\n\n<page>\n---\ntitle: Metrics and analytics Â· Cloudflare D1 docs\ndescription: D1 exposes database analytics that allow you to inspect query\n  volume, query latency, and storage size across all and/or each database in\n  your account.\nlastUpdated: 2025-09-03T16:40:54.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/observability/metrics-analytics/\n  md: https://developers.cloudflare.com/d1/observability/metrics-analytics/index.md\n---\n\nD1 exposes database analytics that allow you to inspect query volume, query latency, and storage size across all and/or each database in your account.\n\nThe metrics displayed in the [Cloudflare dashboard](https://dash.cloudflare.com/) charts are queried from Cloudflareâ€™s [GraphQL Analytics API](https://developers.cloudflare.com/analytics/graphql-api/). You can access the metrics [programmatically](#query-via-the-graphql-api) via GraphQL or HTTP client.\n\n## Metrics\n\nD1 currently exports the below metrics:\n\n| Metric | GraphQL Field Name | Description |\n| - | - | - |\n| Read Queries (qps) | `readQueries` | The number of read queries issued against a database. This is the raw number of read queries, and is not used for billing. |\n| Write Queries (qps) | `writeQueries` | The number of write queries issued against a database. This is the raw number of write queries, and is not used for billing. |\n| Rows read (count) | `rowsRead` | The number of rows read (scanned) across your queries. See [Pricing](https://developers.cloudflare.com/d1/platform/pricing/) for more details on how rows are counted. |\n| Rows written (count) | `rowsWritten` | The number of rows written across your queries. |\n| Query Response (bytes) | `queryBatchResponseBytes` | The total response size of the serialized query response, including any/all column names, rows and metadata. Reported in bytes. |\n| Query Latency (ms) | `queryBatchTimeMs` | The total query response time, including response serialization, on the server-side. Reported in milliseconds. |\n| Storage (Bytes) | `databaseSizeBytes` | Maximum size of a database. Reported in bytes. |\n\nMetrics can be queried (and are retained) for the past 31 days.\n\n### Row counts\n\nD1 returns the number of rows read, rows written (or both) in response to each individual query via [the Workers Binding API](https://developers.cloudflare.com/d1/worker-api/return-object/).\n\nRow counts are a precise count of how many rows were read (scanned) or written by that query. Inspect row counts to understand the performance and cost of a given query, including whether you can reduce the rows read [using indexes](https://developers.cloudflare.com/d1/best-practices/use-indexes/). Use query counts to understand the total volume of traffic against your databases and to discern which databases are actively in-use.\n\nRefer to the [Pricing documentation](https://developers.cloudflare.com/d1/platform/pricing/) for more details on how rows are counted.\n\n## View metrics in the dashboard\n\nPer-database analytics for D1 are available in the Cloudflare dashboard. To view current and historical metrics for a database:\n\n1. In the Cloudflare dashboard, go to the **D1** page.\n\n   [Go to **D1 SQL database**](https://dash.cloudflare.com/?to=/:account/workers/d1)\n\n2. Select an existing D1 database.\n\n3. Select the **Metrics** tab.\n\nYou can optionally select a time window to query. This defaults to the last 24 hours.\n\n## Query via the GraphQL API\n\nYou can programmatically query analytics for your D1 databases via the [GraphQL Analytics API](https://developers.cloudflare.com/analytics/graphql-api/). This API queries the same datasets as the Cloudflare dashboard, and supports GraphQL [introspection](https://developers.cloudflare.com/analytics/graphql-api/features/discovery/introspection/).\n\nD1's GraphQL datasets require an `accountTag` filter with your Cloudflare account ID and include:\n\n* `d1AnalyticsAdaptiveGroups`\n* `d1StorageAdaptiveGroups`\n* `d1QueriesAdaptiveGroups`\n\n### Examples\n\nTo query the sum of `readQueries`, `writeQueries` for a given `$databaseId`, grouping by `databaseId` and `date`:",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAIgRgPICMDOkBuBDFBLAGzwBcoBlbAWwAcCwBFcaACgCgYYASbAYx4HsQAO2IAVbAHMAXDDTEIeIRICE7LnOwRiMuNmJg1nMEIAmOvQY6cTe3NgwBJM7PmKJrAJQwA3msx4wAHdIHzUOXgFhYjRmADNCfQgZbxgIwRFxaS40qMyYAF8vXw4SmBMEAEEhbAIoYjweNAqbanrMMABxCEFqGLDSmCJKEhkEAAYJsf7S+IJE5LKLAH0JMGAZTg0tABpF-SW6da5jE12bYjtHZ2tbFHswJwLpkv4IE0gAISgZAG1zsCWcAAomQAMIAXWeRWeHDQIEooQGAwgYGwJkYkACaBhJUCCn0GIUYGxSI4+RxJjwlGMaDw-CEaERpI4-xxLNu9ycOPJSJ5JT55PyQA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBnRMAJ0SxACYAGbgKwBaAIzchAZl4MQAU3gATLn0GjxEkTIVhWAIzBNZVJdgBKAUQAKAGXzmKAdSrIAEtToBfIA)\n\nTo query both the average `queryBatchTimeMs` and the 90th percentile `queryBatchTimeMs` per database:",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAIgRgPICMDOkBuBDFBLAGzwBcoBlbAWwAcCwBFcaAJgAoAoGGAEmwGM+AexAA7YgBVsAcwBcMNMQh4RUgISceC7BGJy42YmA3cwIgCZ6DRrtzMHc2DAEkL8xcqnsAlDADeGzDwwAHdIPw0ufiFRYjRWADNCQwg5Xxgo4TFJWR4MmJcYAF8ffy4ymDMEAEERbAIoYjw+NCq7akbMMABxCGFqOIjymCJKEjkEAAYpicHyxIJk1IqrAH0pMGA5bi0dABplwxW6TZ5TM327YgdnV1t7FEcwAsLZssEIM0gAISg5AG1LmAVnAAKJkADCAF1XiVXlxQNgxIQwGhwkMhqBIFAvgY+AALcR4ShgACyaAACgBOGborgvWkVImmNB4QQiVGlBkHaxcy7XJ5mOFFV70sqil6FIA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBnRMAJ0SxACYAGbgKwBaAIzchAZl4MQAU3gATLn0GjxEkTIVhWAIzBNZVJdgBKAUQAKAGXzmKAdSrIAEtToBfIA)\n\nTo query your account-wide `readQueries` and `writeQueries`:",
      "language": "unknown"
    },
    {
      "code": "[Run in GraphQL API Explorer](https://graphql.cloudflare.com/explorer?query=I4VwpgTgngBAIgRgPICMDOkBuBDFBLAGzwBcoBlbAWwAcCwBFcaAZgAoAoGGAEmwGM+AexAA7YgBVsAcwBcMNMQh4RUgISceC7BGJy42YmA3cwIgCZ6DRrtzMHc2DAEkL8xcqnsAlDADeGzDwwAHdIPw0ufiFRYjRWADNCQwg5Xxgo4TFJWR4MmOyYAF8ffy4ymDMEAEERbAIoYjw+NCq7akbMMABxCGFqOIjymCJKEjkEAAYpicHyxIJk1IqrAH0pMGA5bi0dABplwxW6TZ5TM327YgdnV1t7FEcwFyLZmBLXrjQQSnChoYgwNgzIxIEE0B8ysElIYQUowOC-lxCq9keVUS9CkA\\&variables=N4IghgxhD2CuB2AXAKmA5iAXCAggYTwHkBVAOWQH0BJAERABoQBnRMAJ0SxACYAGbgKwBaAIzchAZl4MQAU3gATLn0GjxEkTIVhWAIzBNZVJdgBKAUQAKAGXzmKAdSrIAEtToBfIA)\n\n## Query `insights`\n\nD1 provides metrics that let you understand and debug query performance. You can access these via GraphQL's `d1QueriesAdaptiveGroups` or `wrangler d1 insights` command.\n\nD1 captures your query strings to make it easier to analyze metrics across query executions. [Bound parameters](https://developers.cloudflare.com/d1/worker-api/prepared-statements/#guidance) are not captured to remove any sensitive information.\n\nNote\n\n`wrangler d1 insights` is an experimental Wrangler command. Its options and output may change.\n\nRun `wrangler d1 insights --help` to view current options.\n\n| Option | Description |\n| - | - |\n| `--timePeriod` | Fetch data from now to the provided time period (default: `1d`). |\n| `--sort-type` | The operation you want to sort insights by. Select between `sum` and `avg` (default: `sum`). |\n| `--sort-by` | The field you want to sort insights by. Select between `time`, `reads`, `writes`, and `count` (default: `time`). |\n| `--sort-direction` | The sort direction. Select between `ASC` and `DESC` (default: `DESC`). |\n| `--json` | A boolean value to specify whether to return the result as clean JSON (default: `false`). |\n| `--limit` | The maximum number of queries to be fetched. |\n\nTo find top 3 queries by execution count:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "To find top 3 queries by average execution time:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "To find top 10 queries by rows written in last 7 days:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "Note\n\nThe quantity `queryEfficiency` measures how efficient your query was. It is calculated as: the number of rows returned divided by the number of rows read.\n\nGenerally, you should try to get `queryEfficiency` as close to `1` as possible. Refer to [Use indexes](https://developers.cloudflare.com/d1/best-practices/use-indexes/) for more information on efficient querying.\n\n</page>\n\n<page>\n---\ntitle: Alpha database migration guide Â· Cloudflare D1 docs\ndescription: D1's open beta launched in October 2023, and newly created\n  databases use a different underlying architecture that is significantly more\n  reliable and performant, with increased database sizes, improved query\n  throughput, and reduced latency.\nlastUpdated: 2025-07-23T15:37:48.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/platform/alpha-migration/\n  md: https://developers.cloudflare.com/d1/platform/alpha-migration/index.md\n---\n\nWarning\n\nD1 alpha databases stopped accepting live SQL queries on August 22, 2024.\n\nD1's open beta launched in October 2023, and newly created databases use a different underlying architecture that is significantly more reliable and performant, with increased database sizes, improved query throughput, and reduced latency.\n\nThis guide will instruct you to recreate alpha D1 databases on our production-ready system.\n\n## Prerequisites\n\n1. You have the [`wrangler` command-line tool](https://developers.cloudflare.com/workers/wrangler/install-and-update/) installed\n2. You are using `wrangler` version `3.33.0` or later (released March 2024) as earlier versions do not have the [`--remote` flag](https://developers.cloudflare.com/d1/platform/release-notes/#2024-03-12) required as part of this guide\n3. An 'alpha' D1 database. All databases created before July 27th, 2023 ([release notes](https://developers.cloudflare.com/d1/platform/release-notes/#2024-03-12)) use the alpha storage backend, which is no longer supported and was not recommended for production.\n\n## 1. Verify that a database is alpha",
      "language": "unknown"
    },
    {
      "code": "If the database is alpha, the output of the command will include `version` set to `alpha`:",
      "language": "unknown"
    },
    {
      "code": "## 2. Create a manual backup",
      "language": "unknown"
    },
    {
      "code": "## 3. Download the manual backup\n\nThe command below will download the manual backup of the alpha database as `.sqlite3` file:",
      "language": "unknown"
    },
    {
      "code": "## 4. Convert the manual backup into SQL statements\n\nThe command below will convert the manual backup of the alpha database from the downloaded `.sqlite3` file into SQL statements which can then be imported into the new database:",
      "language": "unknown"
    },
    {
      "code": "Once you have run the above command, you will need to edit the output SQL file to be compatible with D1:\n\n1. Remove `BEGIN TRANSACTION` and `COMMIT;` from the file.\n\n2. Remove the following table creation statement:",
      "language": "unknown"
    },
    {
      "code": "## 5. Create a new D1 database\n\nAll new D1 databases use the updated architecture by default.\n\nRun the following command to create a new database:",
      "language": "unknown"
    },
    {
      "code": "## 6. Run SQL statements against the new D1 database",
      "language": "unknown"
    },
    {
      "code": "## 7. Delete your alpha database\n\nTo delete your previous alpha database, run:",
      "language": "unknown"
    },
    {
      "code": "</page>\n\n<page>\n---\ntitle: Limits Â· Cloudflare D1 docs\ndescription: Cloudflare also offers other storage solutions such as Workers KV,\n  Durable Objects, and R2. Each product has different advantages and limits.\n  Refer to Choose a data or storage product to review which storage option is\n  right for your use case.\nlastUpdated: 2025-08-05T10:57:02.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/platform/limits/\n  md: https://developers.cloudflare.com/d1/platform/limits/index.md\n---\n\n| Feature | Limit |\n| - | - |\n| Databases | 50,000 (Workers Paid)[1](#user-content-fn-1) / 10 (Free) |\n| Maximum database size | 10 GB (Workers Paid) / 500 MB (Free) |\n| Maximum storage per account | 1 TB (Workers Paid)[2](#user-content-fn-2) / 5 GB (Free) |\n| [Time Travel](https://developers.cloudflare.com/d1/reference/time-travel/) duration (point-in-time recovery) | 30 days (Workers Paid) / 7 days (Free) |\n| Maximum Time Travel restore operations | 10 restores per 10 minute (per database) |\n| Queries per Worker invocation (read [subrequest limits](https://developers.cloudflare.com/workers/platform/limits/#how-many-subrequests-can-i-make)) | 1000 (Workers Paid) / 50 (Free) |\n| Maximum number of columns per table | 100 |\n| Maximum number of rows per table | Unlimited (excluding per-database storage limits) |\n| Maximum string, `BLOB` or table row size | 2,000,000 bytes (2 MB) |\n| Maximum SQL statement length | 100,000 bytes (100 KB) |\n| Maximum bound parameters per query | 100 |\n| Maximum arguments per SQL function | 32 |\n| Maximum characters (bytes) in a `LIKE` or `GLOB` pattern | 50 bytes |\n| Maximum bindings per Workers script | Approximately 5,000 [3](#user-content-fn-3) |\n| Maximum SQL query duration | 30 seconds [4](#user-content-fn-4) |\n| Maximum file import (`d1 execute`) size | 5 GB [5](#user-content-fn-5) |\n\nBatch limits\n\nLimits for individual queries (listed above) apply to each individual statement contained within a batch statement. For example, the maximum SQL statement length of 100 KB applies to each statement inside a `db.batch()`.\n\nFootnotes\n\n1: The maximum number of databases per account can be increased by request on Workers Paid and Enterprise plans, with support for millions to tens-of-millions of databases (or more) per account. Refer to the guidance on limit increases on this page to request an increase.\n\n2: The maximum storage per account can be increased by request on Workers Paid and Enterprise plans. Refer to the guidance on limit increases on this page to request an increase.\n\n3: A single Worker script can have up to 1 MB of script metadata. A binding is defined as a binding to a resource, such as a D1 database, KV namespace, [environmental variable](https://developers.cloudflare.com/workers/configuration/environment-variables/), or secret. Each resource binding is approximately 150 bytes, however environmental variables and secrets are controlled by the size of the value you provide. Excluding environmental variables, you can bind up to \\~5,000 D1 databases to a single Worker script.\n\n4: Requests to Cloudflare API must resolve in 30 seconds. Therefore, this duration limit also applies to the entire batch call.\n\n5: The imported file is uploaded to R2. Refer to [R2 upload limit](https://developers.cloudflare.com/r2/platform/limits).\n\nCloudflare also offers other storage solutions such as [Workers KV](https://developers.cloudflare.com/kv/api/), [Durable Objects](https://developers.cloudflare.com/durable-objects/), and [R2](https://developers.cloudflare.com/r2/get-started/). Each product has different advantages and limits. Refer to [Choose a data or storage product](https://developers.cloudflare.com/workers/platform/storage-options/) to review which storage option is right for your use case.\n\nNeed a higher limit?\n\nTo request an adjustment to a limit, complete the [Limit Increase Request Form](https://forms.gle/ukpeZVLWLnKeixDu7). If the limit can be increased, Cloudflare will contact you with next steps.\n\n## Frequently Asked Questions\n\nFrequently asked questions related to D1 limits:\n\n### How much work can a D1 database do?\n\nD1 is designed for horizontal scale out across multiple, smaller (10 GB) databases, such as per-user, per-tenant or per-entity databases. D1 allows you to build applications with thousands of databases at no extra cost, as the pricing is based only on query and storage costs.\n\n#### Storage\n\nEach D1 database can store up to 10 GB of data.\n\nWarning\n\nNote that the 10 GB limit of a D1 database cannot be further increased.\n\n#### Concurrency and throughput\n\nEach individual D1 database is inherently single-threaded, and processes queries one at a time.\n\nYour maximum throughput is directly related to the duration of your queries.\n\n* If your average query takes 1 ms, you can run approximately 1,000 queries per second.\n* If your average query takes 100 ms, you can run 10 queries per second.\n\nA database that receives too many concurrent requests will first attempt to queue them. If the queue becomes full, the database will return an [\"overloaded\" error](https://developers.cloudflare.com/d1/observability/debug-d1/#error-list).\n\nEach individual D1 database is backed by a single [Durable Object](https://developers.cloudflare.com/durable-objects/concepts/what-are-durable-objects/). When using [D1 read replication](https://developers.cloudflare.com/d1/best-practices/read-replication/#primary-database-instance-vs-read-replicas) each replica instance is a different Durable Object and the guidelines apply to each replica instance independently.\n\n#### Query performance\n\nQuery performance is the most important factor for throughput. As a rough guideline:\n\n* Read queries like `SELECT name FROM users WHERE id = ?` with an appropriate index on `id` will take less than a millisecond for SQL duration.\n* Write queries like `INSERT` or `UPDATE` can take several milliseconds for SQL duration, and depend on the number of rows written. Writes need to be durably persisted across several locations - learn more on [how D1 persists data under the hood](https://blog.cloudflare.com/d1-read-replication-beta/#under-the-hood-how-d1-read-replication-is-implemented).\n* Data migrations like a large `UPDATE` or `DELETE` affecting millions of rows must be run in batches. A single query that attempts to modify hundreds of thousands of rows or hundreds of MBs of data at once will exceed execution limits. Break the work into smaller chunks (e.g., processing 1,000 rows at a time) to stay within platform limits.\n\nTo ensure your queries are fast and efficient, [use appropriate indexes in your SQL schema](https://developers.cloudflare.com/d1/best-practices/use-indexes/).\n\n#### CPU and memory\n\nOperations on a D1 database, including query execution and result serialization, run within the [Workers platform CPU and memory limits](https://developers.cloudflare.com/workers/platform/limits/#memory).\n\nExceeding these limits, or hitting other platform limits, will generate errors. Refer to the [D1 error list for more details](https://developers.cloudflare.com/d1/observability/debug-d1/#error-list).\n\n### How many simultaneous connections can a Worker open to D1?\n\nYou can open up to six connections (to D1) simultaneously for each invocation of your Worker.\n\nFor more information on a Worker's simultaneous connections, refer to [Simultaneous open connections](https://developers.cloudflare.com/workers/platform/limits/#simultaneous-open-connections).\n\n## Footnotes\n\n1. The maximum number of databases per account can be increased by request on Workers Paid and Enterprise plans, with support for millions to tens-of-millions of databases (or more) per account. Refer to the guidance on limit increases on this page to request an increase. [â†©](#user-content-fnref-1)\n\n2. The maximum storage per account can be increased by request on Workers Paid and Enterprise plans. Refer to the guidance on limit increases on this page to request an increase. [â†©](#user-content-fnref-2)\n\n3. A single Worker script can have up to 1 MB of script metadata. A binding is defined as a binding to a resource, such as a D1 database, KV namespace, [environmental variable](https://developers.cloudflare.com/workers/configuration/environment-variables/), or secret. Each resource binding is approximately 150-bytes, however environmental variables and secrets are controlled by the size of the value you provide. Excluding environmental variables, you can bind up to \\~5,000 D1 databases to a single Worker script. [â†©](#user-content-fnref-3)\n\n4. Requests to Cloudflare API must resolve in 30 seconds. Therefore, this duration limit also applies to the entire batch call. [â†©](#user-content-fnref-4)\n\n5. The imported file is uploaded to R2. Refer to [R2 upload limit](https://developers.cloudflare.com/r2/platform/limits). [â†©](#user-content-fnref-5)\n\n</page>\n\n<page>\n---\ntitle: Pricing Â· Cloudflare D1 docs\ndescription: \"D1 bills based on:\"\nlastUpdated: 2025-07-23T15:37:48.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/platform/pricing/\n  md: https://developers.cloudflare.com/d1/platform/pricing/index.md\n---\n\nD1 bills based on:\n\n* **Usage**: Queries you run against D1 will count as rows read, rows written, or both (for transactions or batches).\n* **Scale-to-zero**: You are not billed for hours or capacity units. If you are not running queries against your database, you are not billed for compute.\n* **Storage**: You are only billed for storage above the included [limits](https://developers.cloudflare.com/d1/platform/limits/) of your plan.\n\n## Billing metrics\n\n| | [Workers Free](https://developers.cloudflare.com/workers/platform/pricing/#workers) | [Workers Paid](https://developers.cloudflare.com/workers/platform/pricing/#workers) |\n| - | - | - |\n| Rows read | 5 million / day | First 25 billion / month included + $0.001 / million rows |\n| Rows written | 100,000 / day | First 50 million / month included + $1.00 / million rows |\n| Storage (per GB stored) | 5 GB (total) | First 5 GB included + $0.75 / GB-mo |\n\nTrack your D1 usage\n\nTo accurately track your usage, use the [meta object](https://developers.cloudflare.com/d1/worker-api/return-object/), [GraphQL Analytics API](https://developers.cloudflare.com/d1/observability/metrics-analytics/#query-via-the-graphql-api), or the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/workers/d1/). Select your D1 database, then view: Metrics > Row Metrics.\n\n### Definitions\n\n1. Rows read measure how many rows a query reads (scans), regardless of the size of each row. For example, if you have a table with 5000 rows and run a `SELECT * FROM table` as a full table scan, this would count as 5,000 rows read. A query that filters on an [unindexed column](https://developers.cloudflare.com/d1/best-practices/use-indexes/) may return fewer rows to your Worker, but is still required to read (scan) more rows to determine which subset to return.\n2. Rows written measure how many rows were written to D1 database. Write operations include `INSERT`, `UPDATE`, and `DELETE`. Each of these operations contribute towards rows written. A query that `INSERT` 10 rows into a `users` table would count as 10 rows written.\n3. DDL operations (for example, `CREATE`, `ALTER`, and `DROP`) are used to define or modify the structure of a database. They may contribute to a mix of read rows and write rows. Ensure you are accurately tracking your usage through the available tools ([meta object](https://developers.cloudflare.com/d1/worker-api/return-object/), [GraphQL Analytics API](https://developers.cloudflare.com/d1/observability/metrics-analytics/#query-via-the-graphql-api), or the [Cloudflare dashboard](https://dash.cloudflare.com/?to=/:account/workers/d1/)).\n4. Row size or the number of columns in a row does not impact how rows are counted. A row that is 1 KB and a row that is 100 KB both count as one row.\n5. Defining [indexes](https://developers.cloudflare.com/d1/best-practices/use-indexes/) on your table(s) reduces the number of rows read by a query when filtering on that indexed field. For example, if the `users` table has an index on a timestamp column `created_at`, the query `SELECT * FROM users WHERE created_at > ?1` would only need to read a subset of the table.\n6. Indexes will add an additional written row when writes include the indexed column, as there are two rows written: one to the table itself, and one to the index. The performance benefit of an index and reduction in rows read will, in nearly all cases, offset this additional write.\n7. Storage is based on gigabytes stored per month, and is based on the sum of all databases in your account. Tables and indexes both count towards storage consumed.\n8. Free limits reset daily at 00:00 UTC. Monthly included limits reset based on your monthly subscription renewal date, which is determined by the day you first subscribed.\n9. There are no data transfer (egress) or throughput (bandwidth) charges for data accessed from D1.\n10. [Read replication](https://developers.cloudflare.com/d1/best-practices/read-replication/) does not charge extra for read replicas. You incur the same usage billing based on `rows_read` and `rows_written` by your queries.\n\n## Frequently Asked Questions\n\nFrequently asked questions related to D1 pricing:\n\n### Will D1 always have a Free plan?\n\nYes, the [Workers Free plan](https://developers.cloudflare.com/workers/platform/pricing/#workers) will always include the ability to prototype and experiment with D1 for free.\n\n### What happens if I exceed the daily limits on reads and writes, or the total storage limit, on the Free plan?\n\nWhen your account hits the daily read and/or write limits, you will not be able to run queries against D1. D1 API will return errors to your client indicating that your daily limits have been exceeded. Once you have reached your included storage limit, you will need to delete unused databases or clean up stale data before you can insert new data, create or alter tables or create indexes and triggers.\n\nUpgrading to the Workers Paid plan will remove these limits, typically within minutes.\n\n### What happens if I exceed the monthly included reads, writes and/or storage on the paid tier?\n\nYou will be billed for the additional reads, writes and storage according to [D1's pricing metrics](#billing-metrics).\n\n### How can I estimate my (eventual) bill?\n\nEvery query returns a `meta` object that contains a total count of the rows read (`rows_read`) and rows written (`rows_written`) by that query. For example, a query that performs a full table scan (for instance, `SELECT * FROM users`) from a table with 5000 rows would return a `rows_read` value of `5000`:",
      "language": "unknown"
    },
    {
      "code": "These are also included in the D1 [Cloudflare dashboard](https://dash.cloudflare.com) and the [analytics API](https://developers.cloudflare.com/d1/observability/metrics-analytics/), allowing you to attribute read and write volumes to specific databases, time periods, or both.\n\n### Does D1 charge for data transfer / egress?\n\nNo.\n\n### Does D1 charge additional for additional compute?\n\nD1 itself does not charge for additional compute. Workers querying D1 and computing results: for example, serializing results into JSON and/or running queries, are billed per [Workers pricing](https://developers.cloudflare.com/workers/platform/pricing/#workers), in addition to your D1 specific usage.\n\n### Do queries I run from the dashboard or Wrangler (the CLI) count as billable usage?\n\nYes, any queries you run against your database, including inserting (`INSERT`) existing data into a new database, table scans (`SELECT * FROM table`), or creating indexes count as either reads or writes.\n\n### Can I use an index to reduce the number of rows read by a query?\n\nYes, you can use an index to reduce the number of rows read by a query. [Creating indexes](https://developers.cloudflare.com/d1/best-practices/use-indexes/) for your most queried tables and filtered columns reduces how much data is scanned and improves query performance at the same time. If you have a read-heavy workload (most common), this can be particularly advantageous. Writing to columns referenced in an index will add at least one (1) additional row written to account for updating the index, but this is typically offset by the reduction in rows read due to the benefits of an index.\n\n### Does a freshly created database, and/or an empty table with no rows, contribute to my storage?\n\nYes, although minimal. An empty table consumes at least a few kilobytes, based on the number of columns (table width) in the table. An empty database consumes approximately 12 KB of storage.\n\n</page>\n\n<page>\n---\ntitle: Release notes Â· Cloudflare D1 docs\ndescription: Subscribe to RSS\nlastUpdated: 2025-07-23T15:37:48.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/platform/release-notes/\n  md: https://developers.cloudflare.com/d1/platform/release-notes/index.md\n---\n\n[Subscribe to RSS](https://developers.cloudflare.com/d1/platform/release-notes/index.xml)\n\n## 2025-11-05\n\n**D1 can configure jurisdictions for data localization**\n\nYou can now set a [jurisdiction](https://developers.cloudflare.com/d1/configuration/data-location/) when creating a D1 database to guarantee where your database runs and stores data.\n\n## 2025-09-11\n\n**D1 automatically retries read-only queries**\n\nD1 now detects read-only queries and automatically attempts up to two retries to execute those queries in the event of failures with retryable errors. You can access the number of execution attempts in the returned [response metadata](https://developers.cloudflare.com/d1/worker-api/return-object/#d1result) property `total_attempts`.\n\nAt the moment, only read-only queries are retried, that is, queries containing only the following SQLite keywords: `SELECT`, `EXPLAIN`, `WITH`. Queries containing any [SQLite keyword](https://sqlite.org/lang_keywords.html) that leads to database writes are not retried.\n\nThe retry success ratio among read-only retryable errors varies from 5% all the way up to 95%, depending on the underlying error and its duration (like network errors or other internal errors).\n\nThe retry success ratio among all retryable errors is lower, indicating that there are write-queries that could be retried. Therefore, we recommend D1 users to continue applying [retries in their own code](https://developers.cloudflare.com/d1/best-practices/retry-queries/) for queries that are not read-only but are idempotent according to the business logic of the application.\n\n![D1 automatically query retries success ratio](https://developers.cloudflare.com/_astro/d1-auto-retry-success-ratio.yPw8B0tB_Z1kzKe0.webp)\n\nD1 ensures that any retry attempt does not cause database writes, making the automatic retries safe from side-effects, even if a query causing changes slips through the read-only detection. D1 achieves this by checking for modifications after every query execution, and if any write occurred due to a retry attempt, the query is rolled back.\n\nThe read-only query detection heuristics are simple for now, and there is room for improvement to capture more cases of queries that can be retried, so this is just the beginning.\n\n## 2025-07-01\n\n**Maximum D1 storage per account for the Workers paid plan is now 1 TB**\n\nThe maximum D1 storage per account for users on the Workers paid plan has been increased from 250 GB to 1 TB.\n\n## 2025-07-01\n\n**D1 alpha database backup access removed**\n\nFollowing the removal of query access to D1 alpha databases on [2024-08-23](https://developers.cloudflare.com/d1/platform/release-notes/#2024-08-23), D1 alpha database backups can no longer be accessed or created with [`wrangler d1 backup`](https://developers.cloudflare.com/d1/reference/backups/), available with wrangler v3.\n\nIf you want to retain a backup of your D1 alpha database, please use `wrangler d1 backup` before 2025-07-01. A D1 alpha backup can be used to [migrate](https://developers.cloudflare.com/d1/platform/alpha-migration/#5-create-a-new-d1-database) to a newly created D1 database in its generally available state.\n\n## 2025-05-30\n\n**50-500ms Faster D1 REST API Requests**\n\nUsers using Cloudflare's [REST API](https://developers.cloudflare.com/api/resources/d1/) to query their D1 database can see lower end-to-end request latency now that D1 authentication is performed at the closest Cloudflare network data center that received the request. Previously, authentication required D1 REST API requests to proxy to Cloudflare's core, centralized data centers, which added network round trips and latency.\n\nLatency improvements range from 50-500 ms depending on request location and [database location](https://developers.cloudflare.com/d1/configuration/data-location/) and only apply to the REST API. REST API requests and databases outside the United States see a bigger benefit since Cloudflare's primary core data centers reside in the United States.\n\nD1 query endpoints like `/query` and `/raw` have the most noticeable improvements since they no longer access Cloudflare's core data centers. D1 control plane endpoints such as those to create and delete databases see smaller improvements, since they still require access to Cloudflare's core data centers for other control plane metadata.\n\n## 2025-05-02\n\n**D1 HTTP API permissions bug fix**\n\nA permissions bug that allowed Cloudflare account and user [API tokens](https://developers.cloudflare.com/fundamentals/api/get-started/account-owned-tokens/) with `D1:Read` permission and `Edit` permission on another Cloudflare product to perform D1 database writes is fixed. `D1:Edit` permission is required for any database writes via HTTP API.\n\nIf you were using an existing API token without `D1:Edit` permission to make edits to a D1 database via the HTTP API, then you will need to [create or edit API tokens](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/) to explicitly include `D1:Edit` permission.\n\n## 2025-04-10\n\n**D1 Read Replication Public Beta**\n\nD1 read replication is available in public beta to help lower average latency and increase overall throughput for read-heavy applications like e-commerce websites or content management tools.\n\nWorkers can leverage read-only database copies, called read replicas, by using D1 [Sessions API](https://developers.cloudflare.com/d1/best-practices/read-replication). A session encapsulates all the queries from one logical session for your application. For example, a session may correspond to all queries coming from a particular web browser session. With Sessions API, D1 queries in a session are guaranteed to be [sequentially consistent](https://developers.cloudflare.com/d1/best-practices/read-replication/#replica-lag-and-consistency-model) to avoid data consistency pitfalls. D1 [bookmarks](https://developers.cloudflare.com/d1/reference/time-travel/#bookmarks) can be used from a previous session to ensure logical consistency between sessions.",
      "language": "unknown"
    },
    {
      "code": "Read replicas are automatically created by Cloudflare (currently one in each supported [D1 region](https://developers.cloudflare.com/d1/best-practices/read-replication/#read-replica-locations)), are active/inactive based on query traffic, and are transparently routed to by Cloudflare at no additional cost.\n\nTo checkout D1 read replication, deploy the following Worker code using Sessions API, which will prompt you to create a D1 database and enable read replication on said database.\n\n[![Deploy to Cloudflare](https://deploy.workers.cloudflare.com/button)](https://deploy.workers.cloudflare.com/?url=https://github.com/cloudflare/templates/tree/main/d1-starter-sessions-api)\n\nTo learn more about how read replication was implemented, go to our [blog post](https://blog.cloudflare.com/d1-read-replication-beta).\n\n## 2025-02-19\n\n**D1 supports \\`PRAGMA optimize\\`**\n\nD1 now supports `PRAGMA optimize` command, which can improve database query performance. It is recommended to run this command after a schema change (for example, after creating an index). Refer to [`PRAGMA optimize`](https://developers.cloudflare.com/d1/sql-api/sql-statements/#pragma-optimize) for more information.\n\n## 2025-02-04\n\n**Fixed bug with D1 read-only access via UI and /query REST API.**\n\nFixed a bug with D1 permissions which allowed users with read-only roles via the UI and users with read-only API tokens via the `/query` [REST API](https://developers.cloudflare.com/api/resources/d1/subresources/database/methods/query/) to execute queries that modified databases. UI actions via the `Tables` tab, such as creating and deleting tables, were incorrectly allowed with read-only access. However, UI actions via the `Console` tab were not affected by this bug and correctly required write access.\n\nWrite queries with read-only access will now fail. If you relied on the previous incorrect behavior, please assign the correct roles to users or permissions to API tokens to perform D1 write queries.\n\n## 2025-01-13\n\n**D1 will begin enforcing its free tier limits from the 10th of February 2025.**\n\nD1 will begin enforcing the daily [free tier limits](https://developers.cloudflare.com/d1/platform/limits) from 2025-02-10. These limits only apply to accounts on the Workers Free plan.\n\nFrom 2025-02-10, if you do not take any action and exceed the daily free tier limits, queries to D1 databases via the Workers API and/or REST API will return errors until limits reset daily at 00:00 UTC.\n\nTo ensure uninterrupted service, upgrade your account to the [Workers Paid plan](https://developers.cloudflare.com/workers/platform/pricing/) from the [plans page](https://dash.cloudflare.com/?account=/workers/plans). The minimum monthly billing amount is $5. Refer to [Workers Paid plan](https://developers.cloudflare.com/workers/platform/pricing/) and [D1 limits](https://developers.cloudflare.com/d1/platform/limits/).\n\nFor better insight into your current usage, refer to your [billing metrics](https://developers.cloudflare.com/d1/observability/billing/) for rows read and rows written, which can be found on the [D1 dashboard](https://dash.cloudflare.com/?account=/workers/d1) or GraphQL API.\n\n## 2025-01-07\n\n**D1 Worker API request latency decreases by 40-60%.**\n\nD1 lowered end-to-end Worker API request latency by 40-60% by eliminating redundant network round trips for each request.\n\n![D1 Worker API latency](https://developers.cloudflare.com/images/d1/faster-d1-worker-api.png)\n\n*p50, p90, and p95 request latency aggregated across entire D1 service. These latencies are a reference point and should not be viewed as your exact workload improvement.*\n\nFor each request to a D1 database, at least two network round trips were eliminated. One round trip was due to a bug that is now fixed. The remaining removed round trips are due to avoiding creating a new TCP connection for each request when reaching out to the datacenter hosting the database.\n\nThe removal of redundant network round trips also applies to D1's [REST API](https://developers.cloudflare.com/api/resources/d1/subresources/database/methods/query/). However, the REST API still depends on Cloudflare's centralized datacenters for authentication, which reduces the relative performance improvement.\n\n## 2024-08-23\n\n**D1 alpha databases have stopped accepting SQL queries**\n\nFollowing the [deprecation warning](https://developers.cloudflare.com/d1/platform/release-notes/#2024-04-30) on 2024-04-30, D1 alpha databases have stopped accepting queries (you are still able to create and retrieve backups).\n\nRequests to D1 alpha databases now respond with a HTTP 400 error, containing the following text:\n\n`You can no longer query a D1 alpha database. Please follow https://developers.cloudflare.com/d1/platform/alpha-migration/ to migrate your alpha database and resume querying.`\n\nYou can upgrade to the new, generally available version of D1 by following the [alpha database migration guide](https://developers.cloudflare.com/d1/platform/alpha-migration/).\n\n## 2024-07-26\n\n**Fixed bug in TypeScript typings for run() API**\n\nThe `run()` method as part of the [D1 Client API](https://developers.cloudflare.com/d1/worker-api/) had an incorrect (outdated) type definition, which has now been addressed as of [`@cloudflare/workers-types`](https://www.npmjs.com/package/@cloudflare/workers-types) version `4.20240725.0`.\n\nThe correct type definition is `stmt.run<T>(): D1Result`, as `run()` returns the result rows of the query. The previously *incorrect* type definition was `stmt.run(): D1Response`, which only returns query metadata and no results.\n\n## 2024-06-17\n\n**HTTP API now returns a HTTP 429 error for overloaded D1 databases**\n\nPreviously, D1's [HTTP API](https://developers.cloudflare.com/api/resources/d1/subresources/database/methods/query/) returned a HTTP `500 Internal Server` error for queries that came in while a D1 database was overloaded. These requests now correctly return a `HTTP 429 Too Many Requests` error.\n\nD1's [Workers API](https://developers.cloudflare.com/d1/worker-api/) is unaffected by this change.\n\n## 2024-04-30\n\n**D1 alpha databases will stop accepting live SQL queries on August 15, 2024**\n\nPreviously [deprecated alpha](https://developers.cloudflare.com/d1/platform/release-notes/#2024-04-05) D1 databases need to be migrated by August 15, 2024 to accept new queries.\n\nRefer to [alpha database migration guide](https://developers.cloudflare.com/d1/platform/alpha-migration/) to migrate to the new, generally available, database architecture.\n\n## 2024-04-12\n\n**HTTP API now returns a HTTP 400 error for invalid queries**\n\nPreviously, D1's [HTTP API](https://developers.cloudflare.com/api/resources/d1/subresources/database/methods/query/) returned a HTTP `500 Internal Server` error for an invalid query. An invalid SQL query now correctly returns a `HTTP 400 Bad Request` error.\n\nD1's [Workers API](https://developers.cloudflare.com/d1/worker-api/) is unaffected by this change.\n\n## 2024-04-05\n\n**D1 alpha databases are deprecated**\n\nNow that D1 is generally available and production ready, alpha D1 databases are deprecated and should be migrated for better performance, reliability, and ongoing support.\n\nRefer to [alpha database migration guide](https://developers.cloudflare.com/d1/platform/alpha-migration/) to migrate to the new, generally available, database architecture.\n\n## 2024-04-01\n\n**D1 is generally available**\n\nD1 is now generally available and production ready. Read the [blog post](https://blog.cloudflare.com/building-d1-a-global-database/) for more details on new features in GA and to learn more about the upcoming D1 read replication API.\n\n* Developers with a Workers Paid plan now have a 10GB GB per-database limit (up from 2GB), which can be combined with existing limit of 50,000 databases per account.\n* Developers with a Workers Free plan retain the 500 MB per-database limit and can create up to 10 databases per account.\n* D1 databases can be [exported](https://developers.cloudflare.com/d1/best-practices/import-export-data/#export-an-existing-d1-database) as a SQL file.\n\n## 2024-03-12\n\n**Change in \\`wrangler d1 execute\\` default**\n\nAs of `wrangler@3.33.0`, `wrangler d1 execute` and `wrangler d1 migrations apply` now default to using a local database, to match the default behavior of `wrangler dev`.\n\nIt is also now possible to specify one of `--local` or `--remote` to explicitly tell wrangler which environment you wish to run your commands against.\n\n## 2024-03-05\n\n**Billing for D1 usage**\n\nAs of 2024-03-05, D1 usage will start to be counted and may incur charges for an account's future billing cycle.\n\nDevelopers on the Workers Paid plan with D1 usage beyond [included limits](https://developers.cloudflare.com/d1/platform/pricing/#billing-metrics) will incur charges according to [D1's pricing](https://developers.cloudflare.com/d1/platform/pricing).\n\nDevelopers on the Workers Free plan can use up to the included limits. Usage beyond the limits below requires signing up for the $5/month Workers Paid plan.\n\nAccount billable metrics are available in the [Cloudflare Dashboard](https://dash.cloudflare.com) and [GraphQL API](https://developers.cloudflare.com/d1/observability/metrics-analytics/#metrics).\n\n## 2024-02-16\n\n**API changes to \\`run()\\`**\n\nA previous change (made on 2024-02-13) to the `run()` [query statement method](https://developers.cloudflare.com/d1/worker-api/prepared-statements/#run) has been reverted.\n\n`run()` now returns a `D1Result`, including the result rows, matching its original behavior prior to the change on 2024-02-13.\n\nFuture change to `run()` to return a [`D1ExecResult`](https://developers.cloudflare.com/d1/worker-api/return-object/#d1execresult), as originally intended and documented, will be gated behind a [compatibility date](https://developers.cloudflare.com/workers/configuration/compatibility-dates/) as to avoid breaking existing Workers relying on the way `run()` currently works.\n\n## 2024-02-13\n\n**API changes to \\`raw()\\`, \\`all()\\` and \\`run()\\`**\n\nD1's `raw()`, `all()` and `run()` [query statement methods](https://developers.cloudflare.com/d1/worker-api/prepared-statements/) have been updated to reflect their intended behavior and improve compatibility with ORM libraries.\n\n`raw()` now correctly returns results as an array of arrays, allowing the correct handling of duplicate column names (such as when joining tables), as compared to `all()`, which is unchanged and returns an array of objects. To include an array of column names in the results when using `raw()`, use `raw({columnNames: true})`.\n\n`run()` no longer incorrectly returns a `D1Result` and instead returns a [`D1ExecResult`](https://developers.cloudflare.com/d1/worker-api/return-object/#d1execresult) as originally intended and documented.\n\nThis may be a breaking change for some applications that expected `raw()` to return an array of objects.\n\nRefer to [D1 client API](https://developers.cloudflare.com/d1/worker-api/) to review D1's query methods, return types and TypeScript support in detail.\n\n## 2024-01-18\n\n**Support for LIMIT on UPDATE and DELETE statements**\n\nD1 now supports adding a `LIMIT` clause to `UPDATE` and `DELETE` statements, which allows you to limit the impact of a potentially dangerous operation.\n\n## 2023-12-18\n\n**Legacy alpha automated backups disabled**\n\nDatabases using D1's legacy alpha backend will no longer run automated [hourly backups](https://developers.cloudflare.com/d1/reference/backups/). You may still choose to take manual backups of these databases.\n\nThe D1 team recommends moving to D1's new [production backend](https://developers.cloudflare.com/d1/platform/release-notes/#2023-09-28), which will require you to export and import your existing data. D1's production backend is faster than the original alpha backend. The new backend also supports [Time Travel](https://developers.cloudflare.com/d1/reference/time-travel/), which allows you to restore your database to any minute in the past 30 days without relying on hourly or manual snapshots.\n\n## 2023-10-03\n\n**Create up to 50,000 D1 databases**\n\nDevelopers using D1 on a Workers Paid plan can now create up to 50,000 databases as part of ongoing increases to D1's limits.\n\n* This further enables database-per-user use-cases and allows you to isolate data between customers.\n* Total storage per account is now 50 GB.\n* D1's [analytics and metrics](https://developers.cloudflare.com/d1/observability/metrics-analytics/) provide per-database usage data.\n\nIf you need to create more than 50,000 databases or need more per-account storage, [reach out](https://developers.cloudflare.com/d1/platform/limits/) to the D1 team to discuss.\n\n## 2023-09-28\n\n**The D1 public beta is here**\n\nD1 is now in public beta, and storage limits have been increased:\n\n* Developers with a Workers Paid plan now have a 2 GB per-database limit (up from 500 MB) and can create 25 databases per account (up from 10). These limits will continue to increase automatically during the public beta.\n* Developers with a Workers Free plan retain the 500 MB per-database limit and can create up to 10 databases per account.\n\nDatabases must be using D1's [new storage subsystem](https://developers.cloudflare.com/d1/platform/release-notes/#2023-07-27) to benefit from the increased database limits.\n\nRead the [announcement blog](https://blog.cloudflare.com/d1-open-beta-is-here/) for more details about what is new in the beta and what is coming in the future for D1.\n\n## 2023-08-19\n\n**Row count now returned per query**\n\nD1 now returns a count of `rows_written` and `rows_read` for every query executed, allowing you to assess the cost of query for both [pricing](https://developers.cloudflare.com/d1/platform/pricing/) and [index optimization](https://developers.cloudflare.com/d1/best-practices/use-indexes/) purposes.\n\nThe `meta` object returned in [D1's Client API](https://developers.cloudflare.com/d1/worker-api/return-object/#d1result) contains a total count of the rows read (`rows_read`) and rows written (`rows_written`) by that query. For example, a query that performs a full table scan (for example, `SELECT * FROM users`) from a table with 5000 rows would return a `rows_read` value of `5000`:",
      "language": "unknown"
    },
    {
      "code": "Refer to [D1 pricing documentation](https://developers.cloudflare.com/d1/platform/pricing/) to understand how reads and writes are measured. D1 remains free to use during the alpha period.\n\n## 2023-08-09\n\n**Bind D1 from the Cloudflare dashboard**\n\nYou can now [bind a D1 database](https://developers.cloudflare.com/d1/get-started/#3-bind-your-worker-to-your-d1-database) to your Workers directly in the [Cloudflare dashboard](https://dash.cloudflare.com). To bind D1 from the Cloudflare dashboard, select your Worker project -> **Settings** -> **Variables** -> and select **D1 Database Bindings**.\n\nNote: If you have previously deployed a Worker with a D1 database binding with a version of `wrangler` prior to `3.5.0`, you must upgrade to [`wrangler v3.5.0`](https://github.com/cloudflare/workers-sdk/releases/tag/wrangler%403.5.0) first before you can edit your D1 database bindings in the Cloudflare dashboard. New Workers projects do not have this limitation.\n\nLegacy D1 alpha users who had previously prefixed their database binding manually with `__D1_BETA__` should remove this as part of this upgrade. Your Worker scripts should call your D1 database via `env.BINDING_NAME` only. Refer to the latest [D1 getting started guide](https://developers.cloudflare.com/d1/get-started/#3-bind-your-worker-to-your-d1-database) for best practices.\n\nWe recommend all D1 alpha users begin using wrangler `3.5.0` (or later) to benefit from improved TypeScript types and future D1 API improvements.\n\n## 2023-08-01\n\n**Per-database limit now 500 MB**\n\nDatabases using D1's [new storage subsystem](https://developers.cloudflare.com/d1/platform/release-notes/#2023-07-27) can now grow to 500 MB each, up from the previous 100 MB limit. This applies to both existing and newly created databases.\n\nRefer to [Limits](https://developers.cloudflare.com/d1/platform/limits/) to learn about D1's limits.\n\n## 2023-07-27\n\n**New default storage subsystem**\n\nDatabases created via the Cloudflare dashboard and Wrangler (as of `v3.4.0`) now use D1's new storage subsystem by default. The new backend can [be 6 - 20x faster](https://blog.cloudflare.com/d1-turning-it-up-to-11/) than D1's original alpha backend.\n\nTo understand which storage subsystem your database uses, run `wrangler d1 info YOUR_DATABASE` and inspect the version field in the output.\n\nDatabases with `version: beta` use the new storage backend and support the [Time Travel](https://developers.cloudflare.com/d1/reference/time-travel/) API. Databases with `version: alpha` only use D1's older, legacy backend.\n\n## 2023-07-27\n\n**Time Travel**\n\n[Time Travel](https://developers.cloudflare.com/d1/reference/time-travel/) is now available. Time Travel allows you to restore a D1 database back to any minute within the last 30 days (Workers Paid plan) or 7 days (Workers Free plan), at no additional cost for storage or restore operations.\n\nRefer to the [Time Travel](https://developers.cloudflare.com/d1/reference/time-travel/) documentation to learn how to travel backwards in time.\n\nDatabases using D1's [new storage subsystem](https://blog.cloudflare.com/d1-turning-it-up-to-11/) can use Time Travel. Time Travel replaces the [snapshot-based backups](https://developers.cloudflare.com/d1/reference/backups/) used for legacy alpha databases.\n\n## 2023-06-28\n\n**Metrics and analytics**\n\nYou can now view [per-database metrics](https://developers.cloudflare.com/d1/observability/metrics-analytics/) via both the [Cloudflare dashboard](https://dash.cloudflare.com/) and the [GraphQL Analytics API](https://developers.cloudflare.com/analytics/graphql-api/).\n\nD1 currently exposes read & writes per second, query response size, and query latency percentiles.\n\n## 2023-06-16\n\n**Generated columns documentation**\n\nNew documentation has been published on how to use D1's support for [generated columns](https://developers.cloudflare.com/d1/reference/generated-columns/) to define columns that are dynamically generated on write (or read). Generated columns allow you to extract data from [JSON objects](https://developers.cloudflare.com/d1/sql-api/query-json/) or use the output of other SQL functions.\n\n## 2023-06-12\n\n**Deprecating Error.cause**\n\nAs of [`wrangler v3.1.1`](https://github.com/cloudflare/workers-sdk/releases/tag/wrangler%403.1.1) the [D1 client API](https://developers.cloudflare.com/d1/worker-api/) now returns [detailed error messages](https://developers.cloudflare.com/d1/observability/debug-d1/) within the top-level `Error.message` property, and no longer requires developers to inspect the `Error.cause.message` property.\n\nTo facilitate a transition from the previous `Error.cause` behaviour, detailed error messages will continue to be populated within `Error.cause` as well as the top-level `Error` object until approximately July 14th, 2023. Future versions of both `wrangler` and the D1 client API will no longer populate `Error.cause` after this date.\n\n## 2023-05-19\n\n**New experimental backend**\n\nD1 has a new experimental storage back end that dramatically improves query throughput, latency and reliability. The experimental back end will become the default back end in the near future. To create a database using the experimental backend, use `wrangler` and set the `--experimental-backend` flag when creating a database:",
      "language": "unknown"
    },
    {
      "code": "Read more about the experimental back end in the [announcement blog](https://blog.cloudflare.com/d1-turning-it-up-to-11/).\n\n## 2023-05-19\n\n**Location hints**\n\nYou can now provide a [location hint](https://developers.cloudflare.com/d1/configuration/data-location/) when creating a D1 database, which will influence where the leader (writer) is located. By default, D1 will automatically create your database in a location close to where you issued the request to create a database. In most cases this allows D1 to choose the optimal location for your database on your behalf.\n\n## 2023-05-17\n\n**Query JSON**\n\n[New documentation](https://developers.cloudflare.com/d1/sql-api/query-json/) has been published that covers D1's extensive JSON function support. JSON functions allow you to parse, query and modify JSON directly from your SQL queries, reducing the number of round trips to your database, or data queried.\n\n</page>\n\n<page>\n---\ntitle: Choose a data or storage product Â· Cloudflare D1 docs\nlastUpdated: 2025-07-23T15:37:48.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/platform/storage-options/\n  md: https://developers.cloudflare.com/d1/platform/storage-options/index.md\n---\n\n\n</page>\n\n<page>\n---\ntitle: Backups (Legacy) Â· Cloudflare D1 docs\ndescription: D1 has built-in support for creating and restoring backups of your\n  databases with wrangler v3, including support for scheduled automatic backups\n  and manual backup management.\nlastUpdated: 2025-06-20T15:14:49.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/reference/backups/\n  md: https://developers.cloudflare.com/d1/reference/backups/index.md\n---\n\nD1 has built-in support for creating and restoring backups of your databases with wrangler v3, including support for scheduled automatic backups and manual backup management.\n\nPlanned removal\n\nAccess to snapshot based backups for D1 alpha databases described in this documentation will be removed on [2025-07-01](https://developers.cloudflare.com/d1/platform/release-notes/#2025-07-01).\n\nTime Travel\n\nDatabases using D1's [production storage subsystem](https://blog.cloudflare.com/d1-turning-it-up-to-11/) can use Time Travel point-in-time recovery. [Time Travel](https://developers.cloudflare.com/d1/reference/time-travel/) replaces the snapshot based backups used for legacy alpha databases.\n\nTo understand which storage subsystem your database uses, run `wrangler d1 info YOUR_DATABASE` and check for the `version` field in the output.Databases with `version: alpha` only support the older, snapshot based backup API.\n\n## Automatic backups\n\nD1 automatically backs up your databases every hour on your behalf, and [retains backups for 24 hours](https://developers.cloudflare.com/d1/platform/limits/). Backups will block access to the DB while they are running. In most cases this should only be a second or two, and any requests that arrive during the backup will be queued.\n\nTo view and manage these backups, including any manual backups you have made, you can use the `d1 backup list <DATABASE_NAME>` command to list each backup.\n\nFor example, to list all of the backups of a D1 database named `existing-db`:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "The `id` of each backup allows you to download or restore a specific backup.\n\n## Manually back up a database\n\nCreating a manual backup of your database before making large schema changes, manually inserting or deleting data, or otherwise modifying a database you are actively using is a good practice to get into. D1 allows you to make a backup of a database at any time, and stores the backup on your behalf. You should also consider [using migrations](https://developers.cloudflare.com/d1/reference/migrations/) to simplify changes to an existing database.\n\nTo back up a D1 database, you must have:\n\n1. The Cloudflare [Wrangler CLI installed](https://developers.cloudflare.com/workers/wrangler/install-and-update/)\n2. An existing D1 database you want to back up.\n\nFor example, to create a manual backup of a D1 database named `example-db`, call `d1 backup create`.",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "Larger databases, especially those that are several megabytes (MB) in size with many tables, may take a few seconds to backup. The `state` column in the output will let you know when the backup is done.\n\n## Downloading a backup locally\n\nTo download a backup locally, call `wrangler d1 backup download <DATABASE_NAME> <BACKUP_ID>`. Use `wrangler d1 backup list <DATABASE_NAME>` to list the available backups, including their IDs, for a given D1 database.\n\nFor example, to download a specific backup for a database named `example-db`:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "The database backup will be download to the current working directory in native SQLite3 format. To import a local database, read [the documentation on importing data](https://developers.cloudflare.com/d1/best-practices/import-export-data/) to D1.\n\n## Restoring a backup\n\nWarning\n\nRestoring a backup will overwrite the existing version of your D1 database in-place. We recommend you make a manual backup before you restore a database, so that you have a backup to revert to if you accidentally restore the wrong backup or break your application.\n\nRestoring a backup will overwrite the current running version of a database with the backup. Database tables (and their data) that do not exist in the backup will no longer exist in the current version of the database, and queries that rely on them will fail.\n\nTo restore a previous backup of a D1 database named `existing-db`, pass the ID of that backup to `d1 backup restore`:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "Any queries against the database will immediately query the current (restored) version once the restore has completed.\n\n</page>\n\n<page>\n---\ntitle: Data security Â· Cloudflare D1 docs\ndescription: \"This page details the data security properties of D1, including:\"\nlastUpdated: 2024-08-13T19:56:56.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/reference/data-security/\n  md: https://developers.cloudflare.com/d1/reference/data-security/index.md\n---\n\nThis page details the data security properties of D1, including:\n\n* Encryption-at-rest (EAR).\n* Encryption-in-transit (EIT).\n* Cloudflare's compliance certifications.\n\n## Encryption at Rest\n\nAll objects stored in D1, including metadata, live databases, and inactive databases are encrypted at rest. Encryption and decryption are automatic, do not require user configuration to enable, and do not impact the effective performance of D1.\n\nEncryption keys are managed by Cloudflare and securely stored in the same key management systems we use for managing encrypted data across Cloudflare internally.\n\nObjects are encrypted using [AES-256](https://www.cloudflare.com/learning/ssl/what-is-encryption/), a widely tested, highly performant and industry-standard encryption algorithm. D1 uses GCM (Galois/Counter Mode) as its preferred mode.\n\n## Encryption in Transit\n\nData transfer between a Cloudflare Worker, and/or between nodes within the Cloudflare network and D1 is secured using the same [Transport Layer Security](https://www.cloudflare.com/learning/ssl/transport-layer-security-tls/) (TLS/SSL).\n\nAPI access via the HTTP API or using the [wrangler](https://developers.cloudflare.com/workers/wrangler/install-and-update/) command-line interface is also over TLS/SSL (HTTPS).\n\n## Compliance\n\nTo learn more about Cloudflare's adherence to industry-standard security compliance certifications, visit the Cloudflare [Trust Hub](https://www.cloudflare.com/trust-hub/compliance-resources/).\n\n</page>\n\n<page>\n---\ntitle: Community projects Â· Cloudflare D1 docs\ndescription: Members of the Cloudflare developer community and broader developer\n  ecosystem have built and/or contributed tooling â€” including ORMs (Object\n  Relational Mapper) libraries, query builders, and CLI tools â€” that build on\n  top of D1.\nlastUpdated: 2024-11-26T11:03:46.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/reference/community-projects/\n  md: https://developers.cloudflare.com/d1/reference/community-projects/index.md\n---\n\nMembers of the Cloudflare developer community and broader developer ecosystem have built and/or contributed tooling â€” including ORMs (Object Relational Mapper) libraries, query builders, and CLI tools â€” that build on top of D1.\n\nNote\n\nCommunity projects are not maintained by the Cloudflare D1 team. They are managed and updated by the project authors.\n\n## Projects\n\n### Sutando ORM\n\nSutando is an ORM designed for Node.js. With Sutando, each table in a database has a corresponding model that handles CRUD (Create, Read, Update, Delete) operations.\n\n* [GitHub](https://github.com/sutandojs/sutando)\n* [D1 with Sutando ORM Example](https://github.com/sutandojs/sutando-examples/tree/main/typescript/rest-hono-cf-d1)\n\n### knex-cloudflare-d1\n\nknex-cloudflare-d1 is the Cloudflare D1 dialect for Knex.js. Note that this is not an official dialect provided by Knex.js.\n\n* [GitHub](https://github.com/kiddyuchina/knex-cloudflare-d1)\n\n### Prisma ORM\n\n[Prisma ORM](https://www.prisma.io/orm) is a next-generation JavaScript and TypeScript ORM that unlocks a new level of developer experience when working with databases thanks to its intuitive data model, automated migrations, type-safety and auto-completion.\n\n* [Tutorial](https://developers.cloudflare.com/d1/tutorials/d1-and-prisma-orm/)\n* [Docs](https://www.prisma.io/docs/orm/prisma-client/deployment/edge/deploy-to-cloudflare#d1)\n\n### D1 adapter for Kysely ORM\n\nKysely is a type-safe and autocompletion-friendly typescript SQL query builder. With this adapter you can interact with D1 with the familiar Kysely interface.\n\n* [Kysely GitHub](https://github.com/koskimas/kysely)\n* [D1 adapter](https://github.com/aidenwallis/kysely-d1)\n\n### feathers-kysely\n\nThe `feathers-kysely` database adapter follows the FeathersJS Query Syntax standard and works with any framework. It is built on the D1 adapter for Kysely and supports passing queries directly from client applications. Since the FeathersJS query syntax is a subset of MongoDB's syntax, this is a great tool for MongoDB users to use Cloudflare D1 without previous SQL experience.\n\n* [feathers-kysely on npm](https://www.npmjs.com/package/feathers-kysely)\n* [feathers-kysely on GitHub](https://github.com/marshallswain/feathers-kysely)\n\n### Drizzle ORM\n\nDrizzle is a headless TypeScript ORM with a head which runs on Node, Bun and Deno. Drizzle ORM lives on the Edge and it is a JavaScript ORM too. It comes with a drizzle-kit CLI companion for automatic SQL migrations generation. Drizzle automatically generates your D1 schema based on types you define in TypeScript, and exposes an API that allows you to query your database directly.\n\n* [Docs](https://orm.drizzle.team/docs)\n* [GitHub](https://github.com/drizzle-team/drizzle-orm)\n* [D1 example](https://orm.drizzle.team/docs/connect-cloudflare-d1)\n\n### Flyweight\n\nFlyweight is an ORM designed specifically for databases related to SQLite. It has first-class D1 support that includes the ability to batch queries and integrate with the wrangler migration system.\n\n* [GitHub](https://github.com/thebinarysearchtree/flyweight)\n\n### d1-orm\n\nObject Relational Mapping (ORM) is a technique to query and manipulate data by using JavaScript. Created by a Cloudflare Discord Community Champion, the `d1-orm` seeks to provide a strictly typed experience while using D1.\n\n* [GitHub](https://github.com/Interactions-as-a-Service/d1-orm/issues)\n* [Documentation](https://docs.interactions.rest/d1-orm/)\n\n### workers-qb\n\n`workers-qb` is a zero-dependency query builder that provides a simple standardized interface while keeping the benefits and speed of using raw queries over a traditional ORM. While not intended to provide ORM-like functionality, `workers-qb` makes it easier to interact with your database from code for direct SQL access.\n\n* [GitHub](https://github.com/G4brym/workers-qb)\n* [Documentation](https://workers-qb.massadas.com/)\n\n### d1-console\n\nInstead of running the `wrangler d1 execute` command in your terminal every time you want to interact with your database, you can interact with D1 from within the `d1-console`. Created by a Discord Community Champion, this gives the benefit of executing multi-line queries, obtaining command history, and viewing a cleanly formatted table output.\n\n* [GitHub](https://github.com/isaac-mcfadyen/d1-console)\n\n### L1\n\n`L1` is a package that brings some Cloudflare Worker ecosystem bindings into PHP and Laravel via the Cloudflare API. It provides interaction with D1 via PDO, KV and Queues, with more services to add in the future, making PHP integration with Cloudflare a real breeze.\n\n* [GitHub](https://github.com/renoki-co/l1)\n* [Packagist](https://packagist.org/packages/renoki-co/l1)\n\n### Staff Directory - a D1-based demo\n\nStaff Directory is a demo project using D1, [HonoX](https://github.com/honojs/honox), and [Cloudflare Pages](https://developers.cloudflare.com/pages/). It uses D1 to store employee data, and is an example of a full-stack application built on top of D1.\n\n* [GitHub](https://github.com/lauragift21/staff-directory)\n* [D1 functionality](https://github.com/lauragift21/staff-directory/blob/main/app/db.ts)\n\n### NuxtHub\n\n`NuxtHub` is a Nuxt module that brings Cloudflare Worker bindings into your Nuxt application with no configuration. It leverages the [Wrangler Platform Proxy](https://developers.cloudflare.com/workers/wrangler/api/#getplatformproxy) in development and direct binding in production to interact with [D1](https://developers.cloudflare.com/d1/), [KV](https://developers.cloudflare.com/kv/) and [R2](https://developers.cloudflare.com/r2/) with server composables (`hubDatabase()`, `hubKV()` and `hubBlob()`).\n\n`NuxtHub` also provides a way to use your remote D1 database in development using the `npx nuxt dev --remote` command.\n\n* [GitHub](https://github.com/nuxt-hub/core)\n* [Documentation](https://hub.nuxt.com)\n* [Example](https://github.com/Atinux/nuxt-todos-edge)\n\n## Feedback\n\nTo report a bug or file feature requests for these community projects, create an issue directly on the project's repository.\n\n</page>\n\n<page>\n---\ntitle: FAQs Â· Cloudflare D1 docs\ndescription: Yes, the Workers Free plan will always include the ability to\n  prototype and experiment with D1 for free.\nlastUpdated: 2025-07-23T15:37:48.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/reference/faq/\n  md: https://developers.cloudflare.com/d1/reference/faq/index.md\n---\n\n## Pricing\n\n### Will D1 always have a Free plan?\n\nYes, the [Workers Free plan](https://developers.cloudflare.com/workers/platform/pricing/#workers) will always include the ability to prototype and experiment with D1 for free.\n\n### What happens if I exceed the daily limits on reads and writes, or the total storage limit, on the Free plan?\n\nWhen your account hits the daily read and/or write limits, you will not be able to run queries against D1. D1 API will return errors to your client indicating that your daily limits have been exceeded. Once you have reached your included storage limit, you will need to delete unused databases or clean up stale data before you can insert new data, create or alter tables or create indexes and triggers.\n\nUpgrading to the Workers Paid plan will remove these limits, typically within minutes.\n\n### What happens if I exceed the monthly included reads, writes and/or storage on the paid tier?\n\nYou will be billed for the additional reads, writes and storage according to [D1's pricing metrics](#billing-metrics).\n\n### How can I estimate my (eventual) bill?\n\nEvery query returns a `meta` object that contains a total count of the rows read (`rows_read`) and rows written (`rows_written`) by that query. For example, a query that performs a full table scan (for instance, `SELECT * FROM users`) from a table with 5000 rows would return a `rows_read` value of `5000`:",
      "language": "unknown"
    },
    {
      "code": "These are also included in the D1 [Cloudflare dashboard](https://dash.cloudflare.com) and the [analytics API](https://developers.cloudflare.com/d1/observability/metrics-analytics/), allowing you to attribute read and write volumes to specific databases, time periods, or both.\n\n### Does D1 charge for data transfer / egress?\n\nNo.\n\n### Does D1 charge additional for additional compute?\n\nD1 itself does not charge for additional compute. Workers querying D1 and computing results: for example, serializing results into JSON and/or running queries, are billed per [Workers pricing](https://developers.cloudflare.com/workers/platform/pricing/#workers), in addition to your D1 specific usage.\n\n### Do queries I run from the dashboard or Wrangler (the CLI) count as billable usage?\n\nYes, any queries you run against your database, including inserting (`INSERT`) existing data into a new database, table scans (`SELECT * FROM table`), or creating indexes count as either reads or writes.\n\n### Can I use an index to reduce the number of rows read by a query?\n\nYes, you can use an index to reduce the number of rows read by a query. [Creating indexes](https://developers.cloudflare.com/d1/best-practices/use-indexes/) for your most queried tables and filtered columns reduces how much data is scanned and improves query performance at the same time. If you have a read-heavy workload (most common), this can be particularly advantageous. Writing to columns referenced in an index will add at least one (1) additional row written to account for updating the index, but this is typically offset by the reduction in rows read due to the benefits of an index.\n\n### Does a freshly created database, and/or an empty table with no rows, contribute to my storage?\n\nYes, although minimal. An empty table consumes at least a few kilobytes, based on the number of columns (table width) in the table. An empty database consumes approximately 12 KB of storage.\n\n## Limits\n\n### How much work can a D1 database do?\n\nD1 is designed for horizontal scale out across multiple, smaller (10 GB) databases, such as per-user, per-tenant or per-entity databases. D1 allows you to build applications with thousands of databases at no extra cost, as the pricing is based only on query and storage costs.\n\n#### Storage\n\nEach D1 database can store up to 10 GB of data.\n\nWarning\n\nNote that the 10 GB limit of a D1 database cannot be further increased.\n\n#### Concurrency and throughput\n\nEach individual D1 database is inherently single-threaded, and processes queries one at a time.\n\nYour maximum throughput is directly related to the duration of your queries.\n\n* If your average query takes 1 ms, you can run approximately 1,000 queries per second.\n* If your average query takes 100 ms, you can run 10 queries per second.\n\nA database that receives too many concurrent requests will first attempt to queue them. If the queue becomes full, the database will return an [\"overloaded\" error](https://developers.cloudflare.com/d1/observability/debug-d1/#error-list).\n\nEach individual D1 database is backed by a single [Durable Object](https://developers.cloudflare.com/durable-objects/concepts/what-are-durable-objects/). When using [D1 read replication](https://developers.cloudflare.com/d1/best-practices/read-replication/#primary-database-instance-vs-read-replicas) each replica instance is a different Durable Object and the guidelines apply to each replica instance independently.\n\n#### Query performance\n\nQuery performance is the most important factor for throughput. As a rough guideline:\n\n* Read queries like `SELECT name FROM users WHERE id = ?` with an appropriate index on `id` will take less than a millisecond for SQL duration.\n* Write queries like `INSERT` or `UPDATE` can take several milliseconds for SQL duration, and depend on the number of rows written. Writes need to be durably persisted across several locations - learn more on [how D1 persists data under the hood](https://blog.cloudflare.com/d1-read-replication-beta/#under-the-hood-how-d1-read-replication-is-implemented).\n* Data migrations like a large `UPDATE` or `DELETE` affecting millions of rows must be run in batches. A single query that attempts to modify hundreds of thousands of rows or hundreds of MBs of data at once will exceed execution limits. Break the work into smaller chunks (e.g., processing 1,000 rows at a time) to stay within platform limits.\n\nTo ensure your queries are fast and efficient, [use appropriate indexes in your SQL schema](https://developers.cloudflare.com/d1/best-practices/use-indexes/).\n\n#### CPU and memory\n\nOperations on a D1 database, including query execution and result serialization, run within the [Workers platform CPU and memory limits](https://developers.cloudflare.com/workers/platform/limits/#memory).\n\nExceeding these limits, or hitting other platform limits, will generate errors. Refer to the [D1 error list for more details](https://developers.cloudflare.com/d1/observability/debug-d1/#error-list).\n\n### How many simultaneous connections can a Worker open to D1?\n\nYou can open up to six connections (to D1) simultaneously for each invocation of your Worker.\n\nFor more information on a Worker's simultaneous connections, refer to [Simultaneous open connections](https://developers.cloudflare.com/workers/platform/limits/#simultaneous-open-connections).\n\n</page>\n\n<page>\n---\ntitle: Generated columns Â· Cloudflare D1 docs\ndescription: D1 allows you to define generated columns based on the values of\n  one or more other columns, SQL functions, or even extracted JSON values.\nlastUpdated: 2024-12-11T09:43:45.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/reference/generated-columns/\n  md: https://developers.cloudflare.com/d1/reference/generated-columns/index.md\n---\n\nD1 allows you to define generated columns based on the values of one or more other columns, SQL functions, or even [extracted JSON values](https://developers.cloudflare.com/d1/sql-api/query-json/).\n\nThis allows you to normalize your data as you write to it or read it from a table, making it easier to query and reducing the need for complex application logic.\n\nGenerated columns can also have [indexes defined](https://developers.cloudflare.com/d1/best-practices/use-indexes/) against them, which can dramatically increase query performance over frequently queried fields.\n\n## Types of generated columns\n\nThere are two types of generated columns:\n\n* `VIRTUAL` (default): the column is generated when read. This has the benefit of not consuming storage, but can increase compute time (and thus reduce query performance), especially for larger queries.\n* `STORED`: the column is generated when the row is written. The column takes up storage space just as a regular column would, but the column does not need to be generated on every read, which can improve read query performance.\n\nWhen omitted from a generated column expression, generated columns default to the `VIRTUAL` type. The `STORED` type is recommended when the generated column is compute intensive. For example, when parsing large JSON structures.\n\n## Define a generated column\n\nGenerated columns can be defined during table creation in a `CREATE TABLE` statement or afterwards via the `ALTER TABLE` statement.\n\nTo create a table that defines a generated column, you use the `AS` keyword:",
      "language": "unknown"
    },
    {
      "code": "As a concrete example, to automatically extract the `location` value from the following JSON sensor data, you can define a generated column called `location` (of type `TEXT`), based on a `raw_data` column that stores the raw representation of our JSON data.",
      "language": "unknown"
    },
    {
      "code": "To define a generated column with the value of `$.measurement.location`, you can use the [`json_extract`](https://developers.cloudflare.com/d1/sql-api/query-json/#extract-values) function to extract the value from the `raw_data` column each time you write to that row:",
      "language": "unknown"
    },
    {
      "code": "Generated columns can optionally be specified with the `column_name GENERATED ALWAYS AS <function> [STORED|VIRTUAL]` syntax. The `GENERATED ALWAYS` syntax is optional and does not change the behavior of the generated column when omitted.\n\n## Add a generated column to an existing table\n\nA generated column can also be added to an existing table. If the `sensor_readings` table did not have the generated `location` column, you could add it by running an `ALTER TABLE` statement:",
      "language": "unknown"
    },
    {
      "code": "This defines a `VIRTUAL` generated column that runs `json_extract` on each read query.\n\nGenerated column definitions cannot be directly modified. To change how a generated column generates its data, you can use `ALTER TABLE table_name REMOVE COLUMN` and then `ADD COLUMN` to re-define the generated column, or `ALTER TABLE table_name RENAME COLUMN current_name TO new_name` to rename the existing column before calling `ADD COLUMN` with a new definition.\n\n## Examples\n\nGenerated columns are not just limited to JSON functions like `json_extract`: you can use almost any available function to define how a generated column is generated.\n\nFor example, you could generate a `date` column based on the `timestamp` column from the previous `sensor_reading` table, automatically converting a Unix timestamp into a `YYYY-MM-dd` format within your database:",
      "language": "unknown"
    },
    {
      "code": "Alternatively, you could define an `expires_at` column that calculates a future date, and filter on that date in your queries:",
      "language": "unknown"
    },
    {
      "code": "## Additional considerations\n\n* Tables must have at least one non-generated column. You cannot define a table with only generated column(s).\n* Expressions can only reference other columns in the same table and row, and must only use [deterministic functions](https://www.sqlite.org/deterministic.html). Functions like `random()`, sub-queries or aggregation functions cannot be used to define a generated column.\n* Columns added to an existing table via `ALTER TABLE ... ADD COLUMN` must be `VIRTUAL`. You cannot add a `STORED` column to an existing table.\n\n</page>\n\n<page>\n---\ntitle: Glossary Â· Cloudflare D1 docs\ndescription: Review the definitions for terms used across Cloudflare's D1 documentation.\nlastUpdated: 2025-02-24T09:30:25.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/reference/glossary/\n  md: https://developers.cloudflare.com/d1/reference/glossary/index.md\n---\n\nReview the definitions for terms used across Cloudflare's D1 documentation.\n\n| Term | Definition |\n| - | - |\n| bookmark | A bookmark represents the state of a database at a specific point in time.- Bookmarks are lexicographically sortable. Sorting orders a list of bookmarks from oldest-to-newest. |\n| primary database instance | The primary database instance is the original instance of a database. This database instance only exists in one location in the world. |\n| query planner | A component in a database management system which takes a user query and generates the most efficient plan of executing that query (the query plan). For example, the query planner decides which indices to use, or which table to access first. |\n| read replica | A read replica is an eventually-replicated copy of the primary database instance which only serve read requests. There may be multiple read replicas for a single primary database instance. |\n| replica lag | The time it takes for the primary database instance to replicate its changes to a specific read replica. |\n| session | A session encapsulates all the queries from one logical session for your application. For example, a session may correspond to all queries coming from a particular web browser session. |\n\n</page>\n\n<page>\n---\ntitle: Migrations Â· Cloudflare D1 docs\ndescription: Database migrations are a way of versioning your database. Each\n  migration is stored as an .sql file in your migrations folder. The migrations\n  folder is created in your project directory when you create your first\n  migration. This enables you to store and track changes throughout database\n  development.\nlastUpdated: 2025-04-09T22:35:27.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/reference/migrations/\n  md: https://developers.cloudflare.com/d1/reference/migrations/index.md\n---\n\nDatabase migrations are a way of versioning your database. Each migration is stored as an `.sql` file in your `migrations` folder. The `migrations` folder is created in your project directory when you create your first migration. This enables you to store and track changes throughout database development.\n\n## Features\n\nCurrently, the migrations system aims to be simple yet effective. With the current implementation, you can:\n\n* [Create](https://developers.cloudflare.com/workers/wrangler/commands/#d1-migrations-create) an empty migration file.\n* [List](https://developers.cloudflare.com/workers/wrangler/commands/#d1-migrations-list) unapplied migrations.\n* [Apply](https://developers.cloudflare.com/workers/wrangler/commands/#d1-migrations-apply) remaining migrations.\n\nEvery migration file in the `migrations` folder has a specified version number in the filename. Files are listed in sequential order. Every migration file is an SQL file where you can specify queries to be run.\n\nBinding name vs Database name\n\nWhen running a migration script, you can use either the binding name or the database name.\n\nHowever, the binding name can change, whereas the database name cannot. Therefore, to avoid accidentally running migrations on the wrong binding, you may wish to use the database name for D1 migrations.\n\n## Wrangler customizations\n\nBy default, migrations are created in the `migrations/` folder in your Worker project directory. Creating migrations will keep a record of applied migrations in the `d1_migrations` table found in your database.\n\nThis location and table name can be customized in your Wrangler file, inside the D1 binding.\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "## Foreign key constraints\n\nWhen applying a migration, you may need to temporarily disable [foreign key constraints](https://developers.cloudflare.com/d1/sql-api/foreign-keys/). To do so, call `PRAGMA defer_foreign_keys = true` before making changes that would violate foreign keys.\n\nRefer to the [foreign key documentation](https://developers.cloudflare.com/d1/sql-api/foreign-keys/) to learn more about how to work with foreign keys and D1.\n\n</page>\n\n<page>\n---\ntitle: Time Travel and backups Â· Cloudflare D1 docs\ndescription: Time Travel is D1's approach to backups and point-in-time-recovery,\n  and allows you to restore a database to any minute within the last 30 days.\nlastUpdated: 2025-07-07T12:53:47.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/reference/time-travel/\n  md: https://developers.cloudflare.com/d1/reference/time-travel/index.md\n---\n\nTime Travel is D1's approach to backups and point-in-time-recovery, and allows you to restore a database to any minute within the last 30 days.\n\n* You do not need to enable Time Travel. It is always on.\n* Database history and restoring a database incur no additional costs.\n* Time Travel automatically creates [bookmarks](#bookmarks) on your behalf. You do not need to manually trigger or remember to initiate a backup.\n\nBy not having to rely on scheduled backups and/or manually initiated backups, you can go back in time and restore a database prior to a failed migration or schema change, a `DELETE` or `UPDATE` statement without a specific `WHERE` clause, and in the future, fork/copy a production database directly.\n\nSupport for Time Travel\n\nDatabases using D1's [new storage subsystem](https://blog.cloudflare.com/d1-turning-it-up-to-11/) can use Time Travel. Time Travel replaces the [snapshot-based backups](https://developers.cloudflare.com/d1/reference/backups/) used for legacy alpha databases.\n\nTo understand which storage subsystem your database uses, run `wrangler d1 info YOUR_DATABASE` and inspect the `version` field in the output. Databases with `version: production` support the new Time Travel API. Databases with `version: alpha` only support the older, snapshot-based backup API.\n\n## Bookmarks\n\nTime Travel leverages D1's concept of a bookmark to restore to a point in time.\n\n* Bookmarks older than 30 days are invalid and cannot be used as a restore point.\n* Restoring a database to a specific bookmark does not remove or delete older bookmarks. For example, if you restore to a bookmark representing the state of your database 10 minutes ago, and determine that you needed to restore to an earlier point in time, you can still do so.\n* Bookmarks are lexicographically sortable. Sorting orders a list of bookmarks from oldest-to-newest.\n* Bookmarks can be derived from a [Unix timestamp](https://en.wikipedia.org/wiki/Unix_time) (seconds since Jan 1st, 1970), and conversion between a specific timestamp and a bookmark is deterministic (stable).\n\nBookmarks are also leveraged by [Sessions API](https://developers.cloudflare.com/d1/best-practices/read-replication/#sessions-api-examples) to ensure sequential consistency within a Session.\n\n## Timestamps\n\nTime Travel supports two timestamp formats:\n\n* [Unix timestamps](https://developer.mozilla.org/en-US/docs/Glossary/Unix_time), which correspond to seconds since January 1st, 1970 at midnight. This is always in UTC.\n* The [JavaScript date-time string format](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date#date_time_string_format), which is a simplified version of the ISO-8601 timestamp format. An valid date-time string for the July 27, 2023 at 11:18AM in Americas/New\\_York (EST) would look like `2023-07-27T11:18:53.000-04:00`.\n\n## Requirements\n\n* [`Wrangler`](https://developers.cloudflare.com/workers/wrangler/install-and-update/) `v3.4.0` or later installed to use Time Travel commands.\n* A database on D1's production backend. You can check whether a database is using this backend via `wrangler d1 info DB_NAME` - the output show `version: production`.\n\n## Retrieve a bookmark\n\nYou can retrieve a bookmark for the current timestamp by calling the `d1 info` command, which defaults to returning the current bookmark:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "To retrieve the bookmark for a timestamp in the past, pass the `--timestamp` flag with a valid Unix or RFC3339 timestamp:",
      "language": "unknown"
    },
    {
      "code": "## Restore a database\n\nTo restore a database to a specific point-in-time:\n\nWarning\n\nRestoring a database to a specific point-in-time is a *destructive* operation, and overwrites the database in place. In the future, D1 will support branching & cloning databases using Time Travel.",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "Note that:\n\n* Timestamps are converted to a deterministic, stable bookmark. The same timestamp will always represent the same bookmark.\n* Queries in flight will be cancelled, and an error returned to the client.\n* The restore operation will return a [bookmark](#bookmarks) that allows you to [undo](#undo-a-restore) and revert the database.\n\n## Undo a restore\n\nYou can undo a restore by:\n\n* Taking note of the previous bookmark returned as part of a `wrangler d1 time-travel restore` operation\n* Restoring directly to a bookmark in the past, prior to your last restore.\n\nTo fetch a bookmark from an earlier state:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## Export D1 into R2 using Workflows\n\nYou can automatically export your D1 database into R2 storage via REST API and Cloudflare Workflows. This may be useful if you wish to store a state of your D1 database for longer than 30 days.\n\nRefer to the guide [Export and save D1 database](https://developers.cloudflare.com/workflows/examples/backup-d1/).\n\n## Notes\n\n* You can quickly get the Unix timestamp from the command-line in macOS and Windows via `date +%s`.\n* Time Travel does not yet allow you to clone or fork an existing database to a new copy. In the future, Time Travel will allow you to fork (clone) an existing database into a new database, or overwrite an existing database.\n* You can restore a database back to a point in time up to 30 days in the past (Workers Paid plan) or 7 days (Workers Free plan). Refer to [Limits](https://developers.cloudflare.com/d1/platform/limits/) for details on Time Travel's limits.\n\n</page>\n\n<page>\n---\ntitle: Define foreign keys Â· Cloudflare D1 docs\ndescription: D1 supports defining and enforcing foreign key constraints across\n  tables in a database.\nlastUpdated: 2025-04-15T12:29:32.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/sql-api/foreign-keys/\n  md: https://developers.cloudflare.com/d1/sql-api/foreign-keys/index.md\n---\n\nD1 supports defining and enforcing foreign key constraints across tables in a database.\n\nForeign key constraints allow you to enforce relationships across tables. For example, you can use foreign keys to create a strict binding between a `user_id` in a `users` table and the `user_id` in an `orders` table, so that no order can be created against a user that does not exist.\n\nForeign key constraints can also prevent you from deleting rows that reference rows in other tables. For example, deleting rows from the `users` table when rows in the `orders` table refer to them.\n\nBy default, D1 enforces that foreign key constraints are valid within all queries and migrations. This is identical to the behaviour you would observe when setting `PRAGMA foreign_keys = on` in SQLite for every transaction.\n\n## Defer foreign key constraints\n\nWhen running a [query](https://developers.cloudflare.com/d1/worker-api/), [migration](https://developers.cloudflare.com/d1/reference/migrations/) or [importing data](https://developers.cloudflare.com/d1/best-practices/import-export-data/) against a D1 database, there may be situations in which you need to disable foreign key validation during table creation or changes to your schema.\n\nD1's foreign key enforcement is equivalent to SQLite's `PRAGMA foreign_keys = on` directive. Because D1 runs every query inside an implicit transaction, user queries cannot change this during a query or migration.\n\nInstead, D1 allows you to call `PRAGMA defer_foreign_keys = on` or `off`, which allows you to violate foreign key constraints temporarily (until the end of the current transaction).\n\nCalling `PRAGMA defer_foreign_keys = off` does not disable foreign key enforcement outside of the current transaction. If you have not resolved outstanding foreign key violations at the end of your transaction, it will fail with a `FOREIGN KEY constraint failed` error.\n\nTo defer foreign key enforcement, set `PRAGMA defer_foreign_keys = on` at the start of your transaction, or ahead of changes that would violate constraints:",
      "language": "unknown"
    },
    {
      "code": "You can also explicitly set `PRAGMA defer_foreign_keys = off` immediately after you have resolved outstanding foreign key constraints. If there are still outstanding foreign key constraints, you will receive a `FOREIGN KEY constraint failed` error and will need to resolve the violation.\n\n## Define a foreign key relationship\n\nA foreign key relationship can be defined when creating a table via `CREATE TABLE` or when adding a column to an existing table via an `ALTER TABLE` statement.\n\nTo illustrate this with an example based on an e-commerce website with two tables:\n\n* A `users` table that defines common properties about a user account, including a unique `user_id` identifier.\n* An `orders` table that maps an order back to a `user_id` in the user table.\n\nThis mapping is defined as `FOREIGN KEY`, which ensures that:\n\n* You cannot delete a row from the `users` table that would violate the foreign key constraint. This means that you cannot end up with orders that do not have a valid user to map back to.\n* `orders` are always defined against a valid `user_id`, mitigating the risk of creating orders that refer to invalid (or non-existent) users.",
      "language": "unknown"
    },
    {
      "code": "You can define multiple foreign key relationships per-table, and foreign key definitions can reference multiple tables within your overall database schema.\n\n## Foreign key actions\n\nYou can define *actions* as part of your foreign key definitions to either limit or propagate changes to a parent row (`REFERENCES table(column)`). Defining *actions* makes using foreign key constraints in your application easier to reason about, and help either clean up related data or prevent data from being islanded.\n\nThere are five actions you can set when defining the `ON UPDATE` and/or `ON DELETE` clauses as part of a foreign key relationship. You can also define different actions for `ON UPDATE` and `ON DELETE` depending on your requirements.\n\n* `CASCADE` - Updating or deleting a parent key deletes all child keys (rows) associated to it.\n* `RESTRICT` - A parent key cannot be updated or deleted when *any* child key refers to it. Unlike the default foreign key enforcement, relationships with `RESTRICT` applied return errors immediately, and not at the end of the transaction.\n* `SET DEFAULT` - Set the child column(s) referred to by the foreign key definition to the `DEFAULT` value defined in the schema. If no `DEFAULT` is set on the child columns, you cannot use this action.\n* `SET NULL` - Set the child column(s) referred to by the foreign key definition to SQL `NULL`.\n* `NO ACTION` - Take no action.\n\nCASCADE usage\n\nAlthough `CASCADE` can be the desired behavior in some cases, deleting child rows across tables can have undesirable effects and/or result in unintended side effects for your users.\n\nIn the following example, deleting a user from the `users` table will delete all related rows in the `scores` table as you have defined `ON DELETE CASCADE`. Delete all related rows in the `scores` table if you do not want to retain the scores for any users you have deleted entirely. This might mean that *other* users can no longer look up or refer to scores that were still valid.",
      "language": "unknown"
    },
    {
      "code": "## Next Steps\n\n* Read the SQLite [`FOREIGN KEY`](https://www.sqlite.org/foreignkeys.html) documentation.\n* Learn how to [use the D1 Workers Binding API](https://developers.cloudflare.com/d1/worker-api/) from within a Worker.\n* Understand how [database migrations work](https://developers.cloudflare.com/d1/reference/migrations/) with D1.\n\n</page>\n\n<page>\n---\ntitle: Query JSON Â· Cloudflare D1 docs\ndescription: \"D1 has built-in support for querying and parsing JSON data stored\n  within a database. This enables you to:\"\nlastUpdated: 2025-08-15T20:11:52.000Z\nchatbotDeprioritize: false\ntags: JSON\nsource_url:\n  html: https://developers.cloudflare.com/d1/sql-api/query-json/\n  md: https://developers.cloudflare.com/d1/sql-api/query-json/index.md\n---\n\nD1 has built-in support for querying and parsing JSON data stored within a database. This enables you to:\n\n* [Query paths](#extract-values) within a stored JSON object - for example, extracting the value of named key or array index directly, which is especially useful with larger JSON objects.\n* Insert and/or replace values within an object or array.\n* [Expand the contents of a JSON object](#expand-arrays-for-in-queries) or array into multiple rows - for example, for use as part of a `WHERE ... IN` predicate.\n* Create [generated columns](https://developers.cloudflare.com/d1/reference/generated-columns/) that are automatically populated with values from JSON objects you insert.\n\nOne of the biggest benefits to parsing JSON within D1 directly is that it can directly reduce the number of round-trips (queries) to your database. It reduces the cases where you have to read a JSON object into your application (1), parse it, and then write it back (2).\n\nThis allows you to more precisely query over data and reduce the result set your application needs to additionally parse and filter on.\n\n## Types\n\nJSON data is stored as a `TEXT` column in D1. JSON types follow the same [type conversion rules](https://developers.cloudflare.com/d1/worker-api/#type-conversion) as D1 in general, including:\n\n* A JSON null is treated as a D1 `NULL`.\n* A JSON number is treated as an `INTEGER` or `REAL`.\n* Booleans are treated as `INTEGER` values: `true` as `1` and `false` as `0`.\n* Object and array values as `TEXT`.\n\n## Supported functions\n\nThe following table outlines the JSON functions built into D1 and example usage.\n\n* The `json` argument placeholder can be a JSON object, array, string, number or a null value.\n* The `value` argument accepts string literals (only) and treats input as a string, even if it is well-formed JSON. The exception to this rule is when nesting `json_*` functions: the outer (wrapping) function will interpret the inner (wrapped) functions return value as JSON.\n* The `path` argument accepts path-style traversal syntax - for example, `$` to refer to the top-level object/array, `$.key1.key2` to refer to a nested object, and `$.key[2]` to index into an array.\n\n| Function | Description | Example |\n| - | - | - |\n| `json(json)` | Validates the provided string is JSON and returns a minified version of that JSON object. | `json('{\"hello\":[\"world\" ,\"there\"] }')` returns `{\"hello\":[\"world\",\"there\"]}` |\n| `json_array(value1, value2, value3, ...)` | Return a JSON array from the values. | `json_array(1, 2, 3)` returns `[1, 2, 3]` |\n| `json_array_length(json)` - `json_array_length(json, path)` | Return the length of the JSON array | `json_array_length('{\"data\":[\"x\", \"y\", \"z\"]}', '$.data')` returns `3` |\n| `json_extract(json, path)` | Extract the value(s) at the given path using `$.path.to.value` syntax. | `json_extract('{\"temp\":\"78.3\", \"sunset\":\"20:44\"}', '$.temp')` returns `\"78.3\"` |\n| `json -> path` | Extract the value(s) at the given path using path syntax and return it as JSON. | |\n| `json ->> path` | Extract the value(s) at the given path using path syntax and return it as a SQL type. | |\n| `json_insert(json, path, value)` | Insert a value at the given path. Does not overwrite an existing value. | |\n| `json_object(label1, value1, ...)` | Accepts pairs of (keys, values) and returns a JSON object. | `json_object('temp', 45, 'wind_speed_mph', 13)` returns `{\"temp\":45,\"wind_speed_mph\":13}` |\n| `json_patch(target, patch)` | Uses a JSON [MergePatch](https://tools.ietf.org/html/rfc7396) approach to merge the provided patch into the target JSON object. | |\n| `json_remove(json, path, ...)` | Remove the key and value at the specified path. | `json_remove('[60,70,80,90]', '$[0]')` returns `70,80,90]` |\n| `json_replace(json, path, value)` | Insert a value at the given path. Overwrites an existing value, but does not create a new key if it doesn't exist. | |\n| `json_set(json, path, value)` | Insert a value at the given path. Overwrites an existing value. | |\n| `json_type(json)` - `json_type(json, path)` | Return the type of the provided value or value at the specified path. Returns one of `null`, `true`, `false`, `integer`, `real`, `text`, `array`, or `object`. | `json_type('{\"temperatures\":[73.6, 77.8, 80.2]}', '$.temperatures')` returns `array` |\n| `json_valid(json)` | Returns 0 (false) for invalid JSON, and 1 (true) for valid JSON. | `json_valid({invalid:json})`returns`0\\` |\n| `json_quote(value)` | Converts the provided SQL value into its JSON representation. | `json_quote('[1, 2, 3]')` returns `[1,2,3]` |\n| `json_group_array(value)` | Returns the provided value(s) as a JSON array. | |\n| `json_each(value)` - `json_each(value, path)` | Returns each element within the object as an individual row. It will only traverse the top-level object. | |\n| `json_tree(value)` - `json_tree(value, path)` | Returns each element within the object as an individual row. It traverses the full object. | |\n\nThe SQLite [JSON extension](https://www.sqlite.org/json1.html), on which D1 builds on, has additional usage examples.\n\n## Error Handling\n\nJSON functions will return a `malformed JSON` error when operating over data that isn't JSON and/or is not valid JSON. D1 considers valid JSON to be [RFC 7159](https://www.rfc-editor.org/rfc/rfc7159.txt) conformant.\n\nIn the following example, calling `json_extract` over a string (not valid JSON) will cause the query to return a `malformed JSON` error:",
      "language": "unknown"
    },
    {
      "code": "This will return an error:",
      "language": "unknown"
    },
    {
      "code": "## Generated columns\n\nD1's support for [generated columns](https://developers.cloudflare.com/d1/reference/generated-columns/) allows you to create dynamic columns that are generated based on the values of other columns, including extracted or calculated values of JSON data.\n\nThese columns can be queried like any other column, and can have [indexes](https://developers.cloudflare.com/d1/best-practices/use-indexes/) defined on them. If you have JSON data that you frequently query and filter over, creating a generated column and an index can dramatically improve query performance.\n\nFor example, to define a column based on a value within a larger JSON object, use the `AS` keyword combined with a [JSON function](#supported-functions) to generate a typed column:",
      "language": "unknown"
    },
    {
      "code": "Refer to [Generated columns](https://developers.cloudflare.com/d1/reference/generated-columns/) to learn more about how to generate columns.\n\n## Example usage\n\n### Extract values\n\nThere are three ways to extract a value from a JSON object in D1:\n\n* The `json_extract()` function - for example, `json_extract(text_column_containing_json, '$.path.to.value)`.\n* The `->` operator, which returns a JSON representation of the value.\n* The `->>` operator, which returns an SQL representation of the value.\n\nThe `->` and `->>` operators functions both operate similarly to the same operators in PostgreSQL and MySQL/MariaDB.\n\nGiven the following JSON object in a column named `sensor_reading`, you can extract values from it directly.",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### Get the length of an array\n\nYou can get the length of a JSON array in two ways:\n\n1. By calling `json_array_length(value)` directly\n2. By calling `json_array_length(value, path)` to specify the path to an array within an object or outer array.\n\nFor example, given the following JSON object stored in a column called `login_history`, you could get a count of the last logins directly:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "You can also use `json_array_length` as a predicate in a more complex query - for example, `WHERE json_array_length(some_column, '$.path.to.value') >= 5`.\n\n### Insert a value into an existing object\n\nYou can insert a value into an existing JSON object or array using `json_insert()`. For example, if you have a `TEXT` column called `login_history` in a `users` table containing the following object:",
      "language": "unknown"
    },
    {
      "code": "To add a new timestamp to the `history` array within our `login_history` column, write a query resembling the following:",
      "language": "unknown"
    },
    {
      "code": "Provide three arguments to `json_insert`:\n\n1. The name of our column containing the JSON you want to modify.\n2. The path to the key within the object to modify.\n3. The JSON value to insert. Using `[#]` tells `json_insert` to append to the end of your array.\n\nTo replace an existing value, use `json_replace()`, which will overwrite an existing key-value pair if one already exists. To set a value regardless of whether it already exists, use `json_set()`.\n\n### Expand arrays for IN queries\n\nUse `json_each` to expand an array into multiple rows. This can be useful when composing a `WHERE column IN (?)` query over several values. For example, if you wanted to update a list of users by their integer `id`, use `json_each` to return a table with each value as a column called `value`:",
      "language": "unknown"
    },
    {
      "code": "This would extract only the `value` column from the table returned by `json_each`, with each row representing the user IDs you passed in as an array.\n\n`json_each` effectively returns a table with multiple columns, with the most relevant being:\n\n* `key` - the key (or index).\n* `value` - the literal value of each element parsed by `json_each`.\n* `type` - the type of the value: one of `null`, `true`, `false`, `integer`, `real`, `text`, `array`, or `object`.\n* `fullkey` - the full path to the element: e.g. `$[1]` for the second element in an array, or `$.path.to.key` for a nested object.\n* `path` - the top-level path - `$` as the path for an element with a `fullkey` of `$[0]`.\n\nIn this example, `SELECT * FROM json_each('[183183, 13913, 94944]')` would return a table resembling the below:",
      "language": "unknown"
    },
    {
      "code": "You can use `json_each` with [D1 Workers Binding API](https://developers.cloudflare.com/d1/worker-api/) in a Worker by creating a statement and using `JSON.stringify` to pass an array as a [bound parameter](https://developers.cloudflare.com/d1/worker-api/d1-database/#guidance):",
      "language": "unknown"
    },
    {
      "code": "This would only update rows in your `users` table where the `id` matches one of the three provided.\n\n</page>\n\n<page>\n---\ntitle: SQL statements Â· Cloudflare D1 docs\ndescription: D1 is compatible with most SQLite's SQL convention since it\n  leverages SQLite's query engine. D1 supports a number of database-level\n  statements that allow you to list tables, indexes, and inspect the schema for\n  a given table or index.\nlastUpdated: 2025-09-01T15:12:51.000Z\nchatbotDeprioritize: false\nsource_url:\n  html: https://developers.cloudflare.com/d1/sql-api/sql-statements/\n  md: https://developers.cloudflare.com/d1/sql-api/sql-statements/index.md\n---\n\nD1 is compatible with most SQLite's SQL convention since it leverages SQLite's query engine. D1 supports a number of database-level statements that allow you to list tables, indexes, and inspect the schema for a given table or index.\n\nYou can execute any of these statements via the D1 console in the Cloudflare dashboard, [`wrangler d1 execute`](https://developers.cloudflare.com/workers/wrangler/commands/#d1), or with the [D1 Worker Bindings API](https://developers.cloudflare.com/d1/worker-api/d1-database).\n\n## Supported SQLite extensions\n\nD1 supports a subset of SQLite extensions for added functionality, including:\n\n* [FTS5 module](https://www.sqlite.org/fts5.html) for full-text search (including `fts5vocab`).\n* [JSON extension](https://www.sqlite.org/json1.html) for JSON functions and operators.\n* [Math functions](https://sqlite.org/lang_mathfunc.html).\n\nRefer to the [source code](https://github.com/cloudflare/workerd/blob/4c42a4a9d3390c88e9bd977091c9d3395a6cd665/src/workerd/util/sqlite.c%2B%2B#L269) for the full list of supported functions.\n\n## Compatible PRAGMA statements\n\nD1 supports some [SQLite PRAGMA](https://www.sqlite.org/pragma.html) statements. The PRAGMA statement is an SQL extension for SQLite. PRAGMA commands can be used to:\n\n* Modify the behavior of certain SQLite operations.\n* Query the SQLite library for internal data about schemas or tables (but note that PRAGMA statements cannot query the contents of a table).\n* Control [environmental variables](https://developers.cloudflare.com/workers/configuration/environment-variables/).\n\nThe PRAGMA statement examples on this page use the following SQL.",
      "language": "unknown"
    },
    {
      "code": "Warning\n\nD1 PRAGMA statements only apply to the current transaction.\n\n### `PRAGMA table_list`\n\nLists the tables and views in the database. This includes the system tables maintained by D1.\n\n#### Return values\n\nOne row per each table. Each row contains:\n\n1. `Schema`: the schema in which the table appears (for example, `main` or `temp`)\n2. `name`: the name of the table\n3. `type`: the type of the object (one of `table`, `view`, `shadow`, `virtual`)\n4. `ncol`: the number of columns in the table, including generated or hidden columns\n5. `wr`: `1` if the table is a WITHOUT ROWID table, `0` otherwise\n6. `strict`: `1` if the table is a STRICT table, `0` otherwise\n\nExample of `PRAGMA table_list`",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### `PRAGMA table_info(\"TABLE_NAME\")`\n\nShows the schema (columns, types, null, default values) for the given `TABLE_NAME`.\n\n#### Return values\n\nOne row for each column in the specified table. Each row contains:\n\n1. `cid`: a row identifier\n2. `name`: the name of the column\n3. `type`: the data type (if provided), `''` otherwise\n4. `notnull`: `1` if the column can be NULL, `0` otherwise\n5. `dflt_value`: the default value of the column\n6. `pk`: `1` if the column is a primary key, `0` otherwise\n\nExample of `PRAGMA table_info`",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### `PRAGMA table_xinfo(\"TABLE_NAME\")`\n\nSimilar to `PRAGMA table_info(TABLE_NAME)` but also includes [generated columns](https://developers.cloudflare.com/d1/reference/generated-columns/).\n\nExample of `PRAGMA table_xinfo`",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### `PRAGMA index_list(\"TABLE_NAME\")`\n\nShow the indexes for the given `TABLE_NAME`.\n\n#### Return values\n\nOne row for each index associated with the specified table. Each row contains:\n\n1. `seq`: a sequence number for internal tracking\n2. `name`: the name of the index\n3. `unique`: `1` if the index is UNIQUE, `0` otherwise\n4. `origin`: the origin of the index (`c` if created by `CREATE INDEX` statement, `u` if created by UNIQUE constraint, `pk` if created by a PRIMARY KEY constraint)\n5. `partial`: `1` if the index is a partial index, `0` otherwise\n\nExample of `PRAGMA index_list`",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### `PRAGMA index_info(INDEX_NAME)`\n\nShow the indexed column(s) for the given `INDEX_NAME`.\n\n#### Return values\n\nOne row for each key column in the specified index. Each row contains:\n\n1. `seqno`: the rank of the column within the index\n2. `cid`: the rank of the column within the table being indexed\n3. `name`: the name of the column being indexed\n\nExample of `PRAGMA index_info`",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### `PRAGMA index_xinfo(\"INDEX_NAME\")`\n\nSimilar to `PRAGMA index_info(\"TABLE_NAME\")` but also includes hidden columns.\n\nExample of `PRAGMA index_xinfo`",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### `PRAGMA quick_check`\n\nChecks the formatting and consistency of the table, including:\n\n* Incorrectly formatted records\n* Missing pages\n* Sections of the database which are used multiple times, or are not used at all.\n\n#### Return values\n\n* **If there are no errors:** a single row with the value `OK`\n* **If there are errors:** a string which describes the issues flagged by the check\n\nExample of `PRAGMA quick_check`",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "### `PRAGMA foreign_key_check`\n\nChecks for invalid references of foreign keys in the selected table.\n\n### `PRAGMA foreign_key_list(\"TABLE_NAME\")`\n\nLists the foreign key constraints in the selected table.\n\n### `PRAGMA case_sensitive_like = (on|off)`\n\nToggles case sensitivity for LIKE operators. When `PRAGMA case_sensitive_like` is set to:\n\n* `ON`: 'a' LIKE 'A' is false\n* `OFF`: 'a' LIKE 'A' is true (this is the default behavior of the LIKE operator)\n\n### `PRAGMA ignore_check_constraints = (on|off)`\n\nToggles the enforcement of CHECK constraints. When `PRAGMA ignore_check_constraints` is set to:\n\n* `ON`: check constraints are ignored\n* `OFF`: check constraints are enforced (this is the default behavior)\n\n### `PRAGMA legacy_alter_table = (on|off)`\n\nToggles the ALTER TABLE RENAME command behavior before/after the legacy version of SQLite (3.24.0). When `PRAGMA legacy_alter_table` is set to:\n\n* `ON`: ALTER TABLE RENAME only rewrites the initial occurrence of the table name in its CREATE TABLE statement and any associated CREATE INDEX and CREATE TRIGGER statements. All other occurrences are unmodified.\n* `OFF`: ALTER TABLE RENAME rewrites all references to the table name in the schema (this is the default behavior).\n\n### `PRAGMA recursive_triggers = (on|off)`\n\nToggles the recursive trigger capability. When `PRAGMA recursive_triggers` is set to:\n\n* `ON`: triggers which fire can activate other triggers (a single trigger can fire multiple times over the same row)\n* `OFF`: triggers which fire cannot activate other triggers\n\n### `PRAGMA reverse_unordered_selects = (on|off)`\n\nToggles the order of the results of a SELECT statement without an ORDER BY clause. When `PRAGMA reverse_unordered_selects` is set to:\n\n* `ON`: reverses the order of results of a SELECT statement\n* `OFF`: returns the results of a SELECT statement in the usual order\n\n### `PRAGMA foreign_keys = (on|off)`\n\nToggles the foreign key constraint enforcement. When `PRAGMA foreign_keys` is set to:\n\n* `ON`: stops operations which violate foreign key constraints\n* `OFF`: allows operations which violate foreign key constraints\n\n### `PRAGMA defer_foreign_keys = (on|off)`\n\nAllows you to defer the enforcement of [foreign key constraints](https://developers.cloudflare.com/d1/sql-api/foreign-keys/) until the end of the current transaction. This can be useful during [database migrations](https://developers.cloudflare.com/d1/reference/migrations/), as schema changes may temporarily violate constraints depending on the order in which they are applied.\n\nThis does not disable foreign key enforcement outside of the current transaction. If you have not resolved outstanding foreign key violations at the end of your transaction, it will fail with a `FOREIGN KEY constraint failed` error.\n\nNote that setting `PRAGMA defer_foreign_keys = ON` does not prevent `ON DELETE CASCADE` actions from being executed. While foreign key constraint checks are deferred until the end of a transaction, `ON DELETE CASCADE` operations will remain active, consistent with SQLite's behavior.\n\nTo defer foreign key enforcement, set `PRAGMA defer_foreign_keys = on` at the start of your transaction, or ahead of changes that would violate constraints:",
      "language": "unknown"
    },
    {
      "code": "Refer to the [foreign key documentation](https://developers.cloudflare.com/d1/sql-api/foreign-keys/) to learn more about how to work with foreign keys.\n\n### `PRAGMA optimize`\n\nAttempts to optimize all schemas in a database by running the `ANALYZE` command for each table, if necessary. `ANALYZE` updates an internal table which contain statistics about tables and indices. These statistics helps the query planner to execute the input query more efficiently.\n\nWhen `PRAGMA optimize` runs `ANALYZE`, it sets a limit to ensure the command does not take too long to execute. Alternatively, `PRAGMA optimize` may deem it unnecessary to run `ANALYZE` (for example, if the schema has not changed significantly). In this scenario, no optimizations are made.\n\nWe recommend running this command after making any changes to the schema (for example, after [creating an index](https://developers.cloudflare.com/d1/best-practices/use-indexes/)).\n\nNote\n\nCurrently, D1 does not support `PRAGMA optimize(-1)`.\n\n`PRAGMA optimize(-1)` is a command which displays all optimizations that would have been performed without actually executing them.\n\nRefer to [SQLite PRAGMA optimize documentation](https://www.sqlite.org/pragma.html#pragma_optimize) for more information on how `PRAGMA optimize` optimizes a database.\n\n## Query `sqlite_master`\n\nYou can also query the `sqlite_master` table to show all tables, indexes, and the original SQL used to generate them:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## Search with LIKE\n\nYou can perform a search using SQL's `LIKE` operator:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "## Related resources\n\n* Learn [how to create indexes](https://developers.cloudflare.com/d1/best-practices/use-indexes/#list-indexes) in D1.\n* Use D1's [JSON functions](https://developers.cloudflare.com/d1/sql-api/query-json/) to query JSON data.\n* Use [`wrangler dev`](https://developers.cloudflare.com/workers/wrangler/commands/#dev) to run your Worker and D1 locally and debug issues before deploying.\n\n</page>\n\n<page>\n---\ntitle: Build a Comments API Â· Cloudflare D1 docs\ndescription: In this tutorial, you will learn how to use D1 to add comments to a\n  static blog site. You will construct a new D1 database, and build a JSON API\n  that allows the creation and retrieval of comments.\nlastUpdated: 2025-10-13T13:40:40.000Z\nchatbotDeprioritize: false\ntags: Hono,JavaScript,SQL\nsource_url:\n  html: https://developers.cloudflare.com/d1/tutorials/build-a-comments-api/\n  md: https://developers.cloudflare.com/d1/tutorials/build-a-comments-api/index.md\n---\n\nIn this tutorial, you will learn how to use D1 to add comments to a static blog site. To do this, you will construct a new D1 database, and build a JSON API that allows the creation and retrieval of comments.\n\n## Prerequisites\n\nUse [C3](https://developers.cloudflare.com/learning-paths/workers/get-started/c3-and-wrangler/#c3), the command-line tool for Cloudflare's developer products, to create a new directory and initialize a new Worker project:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "For setup, select the following options:\n\n* For *What would you like to start with?*, choose `Hello World example`.\n* For *Which template would you like to use?*, choose `Worker only`.\n* For *Which language do you want to use?*, choose `JavaScript`.\n* For *Do you want to use git for version control?*, choose `Yes`.\n* For *Do you want to deploy your application?*, choose `No` (we will be making some changes before deploying).\n\nTo start developing your Worker, `cd` into your new project directory:",
      "language": "unknown"
    },
    {
      "code": "## Video Tutorial\n\n## 1. Install Hono\n\nIn this tutorial, you will use [Hono](https://github.com/honojs/hono), an Express.js-style framework, to build your API. To use Hono in this project, install it using `npm`:\n\n* npm",
      "language": "unknown"
    },
    {
      "code": "* yarn",
      "language": "unknown"
    },
    {
      "code": "* pnpm",
      "language": "unknown"
    },
    {
      "code": "## 2. Initialize your Hono application\n\nIn `src/worker.js`, initialize a new Hono application, and define the following endpoints:\n\n* `GET /api/posts/:slug/comments`.\n* `POST /api/posts/:slug/comments`.",
      "language": "unknown"
    },
    {
      "code": "## 3. Create a database\n\nYou will now create a D1 database. In Wrangler, there is support for the `wrangler d1` subcommand, which allows you to create and query your D1 databases directly from the command line. Create a new database with `wrangler d1 create`:",
      "language": "unknown"
    },
    {
      "code": "Reference your created database in your Worker code by creating a [binding](https://developers.cloudflare.com/workers/runtime-apis/bindings/) inside of your [Wrangler configuration file](https://developers.cloudflare.com/workers/wrangler/configuration/). Bindings allow us to access Cloudflare resources, like D1 databases, KV namespaces, and R2 buckets, using a variable name in code. In the Wrangler configuration file, set up the binding `DB` and connect it to the `database_name` and `database_id`:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "With your binding configured in your Wrangler file, you can interact with your database from the command line, and inside your Workers function.\n\n## 4. Interact with D1\n\nInteract with D1 by issuing direct SQL commands using `wrangler d1 execute`:",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "You can also pass a SQL file - perfect for initial data seeding in a single command. Create `schemas/schema.sql`, which will create a new `comments` table for your project:",
      "language": "unknown"
    },
    {
      "code": "With the file created, execute the schema file against the D1 database by passing it with the flag `--file`:",
      "language": "unknown"
    },
    {
      "code": "## 5. Execute SQL\n\nIn earlier steps, you created a SQL database and populated it with initial data. Now, you will add a route to your Workers function to retrieve data from that database. Based on your Wrangler configuration in previous steps, your D1 database is now accessible via the `DB` binding. In your code, use the binding to prepare SQL statements and execute them, for example, to retrieve comments:",
      "language": "unknown"
    },
    {
      "code": "The above code makes use of the `prepare`, `bind`, and `run` functions on a D1 binding to prepare and execute a SQL statement. Refer to [D1 Workers Binding API](https://developers.cloudflare.com/d1/worker-api/) for a list of all methods available.\n\nIn this function, you accept a `slug` URL query parameter and set up a new SQL statement where you select all comments with a matching `post_slug` value to your query parameter. You can then return it as a JSON response.\n\n## 6. Insert data\n\nThe previous steps grant read-only access to your data. To create new comments by inserting data into the database, define another endpoint in `src/worker.js`:",
      "language": "unknown"
    },
    {
      "code": "## 7. Deploy your Hono application\n\nWith your application ready for deployment, use Wrangler to build and deploy your project to the Cloudflare network.\n\nBegin by running `wrangler whoami` to confirm that you are logged in to your Cloudflare account. If you are not logged in, Wrangler will prompt you to login, creating an API key that you can use to make authenticated requests automatically from your local machine.\n\nAfter you have logged in, confirm that your Wrangler file is configured similarly to what is seen below. You can change the `name` field to a project name of your choice:\n\n* wrangler.jsonc",
      "language": "unknown"
    },
    {
      "code": "* wrangler.toml",
      "language": "unknown"
    },
    {
      "code": "Now, run `npx wrangler deploy` to deploy your project to Cloudflare.",
      "language": "unknown"
    },
    {
      "code": "When it has successfully deployed, test the API by making a `GET` request to retrieve comments for an associated post. Since you have no posts yet, this response will be empty, but it will still make a request to the D1 database regardless, which you can use to confirm that the application has deployed correctly:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Next steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "Viewing audit logs",
      "id": "viewing-audit-logs"
    },
    {
      "level": "h2",
      "text": "Logged operations",
      "id": "logged-operations"
    },
    {
      "level": "h2",
      "text": "Example log entry",
      "id": "example-log-entry"
    },
    {
      "level": "h2",
      "text": "View metrics in the dashboard",
      "id": "view-metrics-in-the-dashboard"
    },
    {
      "level": "h2",
      "text": "Billing Notifications",
      "id": "billing-notifications"
    },
    {
      "level": "h2",
      "text": "Error list",
      "id": "error-list"
    },
    {
      "level": "h2",
      "text": "Automatic retries",
      "id": "automatic-retries"
    },
    {
      "level": "h2",
      "text": "View logs",
      "id": "view-logs"
    },
    {
      "level": "h2",
      "text": "Report issues",
      "id": "report-issues"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Metrics",
      "id": "metrics"
    },
    {
      "level": "h3",
      "text": "Row counts",
      "id": "row-counts"
    },
    {
      "level": "h2",
      "text": "View metrics in the dashboard",
      "id": "view-metrics-in-the-dashboard"
    },
    {
      "level": "h2",
      "text": "Query via the GraphQL API",
      "id": "query-via-the-graphql-api"
    },
    {
      "level": "h3",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "Query `insights`",
      "id": "query-`insights`"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "1. Verify that a database is alpha",
      "id": "1.-verify-that-a-database-is-alpha"
    },
    {
      "level": "h2",
      "text": "2. Create a manual backup",
      "id": "2.-create-a-manual-backup"
    },
    {
      "level": "h2",
      "text": "3. Download the manual backup",
      "id": "3.-download-the-manual-backup"
    },
    {
      "level": "h2",
      "text": "4. Convert the manual backup into SQL statements",
      "id": "4.-convert-the-manual-backup-into-sql-statements"
    },
    {
      "level": "h2",
      "text": "5. Create a new D1 database",
      "id": "5.-create-a-new-d1-database"
    },
    {
      "level": "h2",
      "text": "6. Run SQL statements against the new D1 database",
      "id": "6.-run-sql-statements-against-the-new-d1-database"
    },
    {
      "level": "h2",
      "text": "7. Delete your alpha database",
      "id": "7.-delete-your-alpha-database"
    },
    {
      "level": "h2",
      "text": "Frequently Asked Questions",
      "id": "frequently-asked-questions"
    },
    {
      "level": "h3",
      "text": "How much work can a D1 database do?",
      "id": "how-much-work-can-a-d1-database-do?"
    },
    {
      "level": "h3",
      "text": "How many simultaneous connections can a Worker open to D1?",
      "id": "how-many-simultaneous-connections-can-a-worker-open-to-d1?"
    },
    {
      "level": "h2",
      "text": "Footnotes",
      "id": "footnotes"
    },
    {
      "level": "h2",
      "text": "Billing metrics",
      "id": "billing-metrics"
    },
    {
      "level": "h3",
      "text": "Definitions",
      "id": "definitions"
    },
    {
      "level": "h2",
      "text": "Frequently Asked Questions",
      "id": "frequently-asked-questions"
    },
    {
      "level": "h3",
      "text": "Will D1 always have a Free plan?",
      "id": "will-d1-always-have-a-free-plan?"
    },
    {
      "level": "h3",
      "text": "What happens if I exceed the daily limits on reads and writes, or the total storage limit, on the Free plan?",
      "id": "what-happens-if-i-exceed-the-daily-limits-on-reads-and-writes,-or-the-total-storage-limit,-on-the-free-plan?"
    },
    {
      "level": "h3",
      "text": "What happens if I exceed the monthly included reads, writes and/or storage on the paid tier?",
      "id": "what-happens-if-i-exceed-the-monthly-included-reads,-writes-and/or-storage-on-the-paid-tier?"
    },
    {
      "level": "h3",
      "text": "How can I estimate my (eventual) bill?",
      "id": "how-can-i-estimate-my-(eventual)-bill?"
    },
    {
      "level": "h3",
      "text": "Does D1 charge for data transfer / egress?",
      "id": "does-d1-charge-for-data-transfer-/-egress?"
    },
    {
      "level": "h3",
      "text": "Does D1 charge additional for additional compute?",
      "id": "does-d1-charge-additional-for-additional-compute?"
    },
    {
      "level": "h3",
      "text": "Do queries I run from the dashboard or Wrangler (the CLI) count as billable usage?",
      "id": "do-queries-i-run-from-the-dashboard-or-wrangler-(the-cli)-count-as-billable-usage?"
    },
    {
      "level": "h3",
      "text": "Can I use an index to reduce the number of rows read by a query?",
      "id": "can-i-use-an-index-to-reduce-the-number-of-rows-read-by-a-query?"
    },
    {
      "level": "h3",
      "text": "Does a freshly created database, and/or an empty table with no rows, contribute to my storage?",
      "id": "does-a-freshly-created-database,-and/or-an-empty-table-with-no-rows,-contribute-to-my-storage?"
    },
    {
      "level": "h2",
      "text": "2025-11-05",
      "id": "2025-11-05"
    },
    {
      "level": "h2",
      "text": "2025-09-11",
      "id": "2025-09-11"
    },
    {
      "level": "h2",
      "text": "2025-07-01",
      "id": "2025-07-01"
    },
    {
      "level": "h2",
      "text": "2025-07-01",
      "id": "2025-07-01"
    },
    {
      "level": "h2",
      "text": "2025-05-30",
      "id": "2025-05-30"
    },
    {
      "level": "h2",
      "text": "2025-05-02",
      "id": "2025-05-02"
    },
    {
      "level": "h2",
      "text": "2025-04-10",
      "id": "2025-04-10"
    },
    {
      "level": "h2",
      "text": "2025-02-19",
      "id": "2025-02-19"
    },
    {
      "level": "h2",
      "text": "2025-02-04",
      "id": "2025-02-04"
    },
    {
      "level": "h2",
      "text": "2025-01-13",
      "id": "2025-01-13"
    },
    {
      "level": "h2",
      "text": "2025-01-07",
      "id": "2025-01-07"
    },
    {
      "level": "h2",
      "text": "2024-08-23",
      "id": "2024-08-23"
    },
    {
      "level": "h2",
      "text": "2024-07-26",
      "id": "2024-07-26"
    },
    {
      "level": "h2",
      "text": "2024-06-17",
      "id": "2024-06-17"
    },
    {
      "level": "h2",
      "text": "2024-04-30",
      "id": "2024-04-30"
    },
    {
      "level": "h2",
      "text": "2024-04-12",
      "id": "2024-04-12"
    },
    {
      "level": "h2",
      "text": "2024-04-05",
      "id": "2024-04-05"
    },
    {
      "level": "h2",
      "text": "2024-04-01",
      "id": "2024-04-01"
    },
    {
      "level": "h2",
      "text": "2024-03-12",
      "id": "2024-03-12"
    },
    {
      "level": "h2",
      "text": "2024-03-05",
      "id": "2024-03-05"
    },
    {
      "level": "h2",
      "text": "2024-02-16",
      "id": "2024-02-16"
    },
    {
      "level": "h2",
      "text": "2024-02-13",
      "id": "2024-02-13"
    },
    {
      "level": "h2",
      "text": "2024-01-18",
      "id": "2024-01-18"
    },
    {
      "level": "h2",
      "text": "2023-12-18",
      "id": "2023-12-18"
    },
    {
      "level": "h2",
      "text": "2023-10-03",
      "id": "2023-10-03"
    },
    {
      "level": "h2",
      "text": "2023-09-28",
      "id": "2023-09-28"
    },
    {
      "level": "h2",
      "text": "2023-08-19",
      "id": "2023-08-19"
    },
    {
      "level": "h2",
      "text": "2023-08-09",
      "id": "2023-08-09"
    },
    {
      "level": "h2",
      "text": "2023-08-01",
      "id": "2023-08-01"
    },
    {
      "level": "h2",
      "text": "2023-07-27",
      "id": "2023-07-27"
    },
    {
      "level": "h2",
      "text": "2023-07-27",
      "id": "2023-07-27"
    },
    {
      "level": "h2",
      "text": "2023-06-28",
      "id": "2023-06-28"
    },
    {
      "level": "h2",
      "text": "2023-06-16",
      "id": "2023-06-16"
    },
    {
      "level": "h2",
      "text": "2023-06-12",
      "id": "2023-06-12"
    },
    {
      "level": "h2",
      "text": "2023-05-19",
      "id": "2023-05-19"
    },
    {
      "level": "h2",
      "text": "2023-05-19",
      "id": "2023-05-19"
    },
    {
      "level": "h2",
      "text": "2023-05-17",
      "id": "2023-05-17"
    },
    {
      "level": "h2",
      "text": "Automatic backups",
      "id": "automatic-backups"
    },
    {
      "level": "h2",
      "text": "Manually back up a database",
      "id": "manually-back-up-a-database"
    },
    {
      "level": "h2",
      "text": "Downloading a backup locally",
      "id": "downloading-a-backup-locally"
    },
    {
      "level": "h2",
      "text": "Restoring a backup",
      "id": "restoring-a-backup"
    },
    {
      "level": "h2",
      "text": "Encryption at Rest",
      "id": "encryption-at-rest"
    },
    {
      "level": "h2",
      "text": "Encryption in Transit",
      "id": "encryption-in-transit"
    },
    {
      "level": "h2",
      "text": "Compliance",
      "id": "compliance"
    },
    {
      "level": "h2",
      "text": "Projects",
      "id": "projects"
    },
    {
      "level": "h3",
      "text": "Sutando ORM",
      "id": "sutando-orm"
    },
    {
      "level": "h3",
      "text": "knex-cloudflare-d1",
      "id": "knex-cloudflare-d1"
    },
    {
      "level": "h3",
      "text": "Prisma ORM",
      "id": "prisma-orm"
    },
    {
      "level": "h3",
      "text": "D1 adapter for Kysely ORM",
      "id": "d1-adapter-for-kysely-orm"
    },
    {
      "level": "h3",
      "text": "feathers-kysely",
      "id": "feathers-kysely"
    },
    {
      "level": "h3",
      "text": "Drizzle ORM",
      "id": "drizzle-orm"
    },
    {
      "level": "h3",
      "text": "Flyweight",
      "id": "flyweight"
    },
    {
      "level": "h3",
      "text": "d1-orm",
      "id": "d1-orm"
    },
    {
      "level": "h3",
      "text": "workers-qb",
      "id": "workers-qb"
    },
    {
      "level": "h3",
      "text": "d1-console",
      "id": "d1-console"
    },
    {
      "level": "h3",
      "text": "L1",
      "id": "l1"
    },
    {
      "level": "h3",
      "text": "Staff Directory - a D1-based demo",
      "id": "staff-directory---a-d1-based-demo"
    },
    {
      "level": "h3",
      "text": "NuxtHub",
      "id": "nuxthub"
    },
    {
      "level": "h2",
      "text": "Feedback",
      "id": "feedback"
    },
    {
      "level": "h2",
      "text": "Pricing",
      "id": "pricing"
    },
    {
      "level": "h3",
      "text": "Will D1 always have a Free plan?",
      "id": "will-d1-always-have-a-free-plan?"
    },
    {
      "level": "h3",
      "text": "What happens if I exceed the daily limits on reads and writes, or the total storage limit, on the Free plan?",
      "id": "what-happens-if-i-exceed-the-daily-limits-on-reads-and-writes,-or-the-total-storage-limit,-on-the-free-plan?"
    },
    {
      "level": "h3",
      "text": "What happens if I exceed the monthly included reads, writes and/or storage on the paid tier?",
      "id": "what-happens-if-i-exceed-the-monthly-included-reads,-writes-and/or-storage-on-the-paid-tier?"
    },
    {
      "level": "h3",
      "text": "How can I estimate my (eventual) bill?",
      "id": "how-can-i-estimate-my-(eventual)-bill?"
    },
    {
      "level": "h3",
      "text": "Does D1 charge for data transfer / egress?",
      "id": "does-d1-charge-for-data-transfer-/-egress?"
    },
    {
      "level": "h3",
      "text": "Does D1 charge additional for additional compute?",
      "id": "does-d1-charge-additional-for-additional-compute?"
    },
    {
      "level": "h3",
      "text": "Do queries I run from the dashboard or Wrangler (the CLI) count as billable usage?",
      "id": "do-queries-i-run-from-the-dashboard-or-wrangler-(the-cli)-count-as-billable-usage?"
    },
    {
      "level": "h3",
      "text": "Can I use an index to reduce the number of rows read by a query?",
      "id": "can-i-use-an-index-to-reduce-the-number-of-rows-read-by-a-query?"
    },
    {
      "level": "h3",
      "text": "Does a freshly created database, and/or an empty table with no rows, contribute to my storage?",
      "id": "does-a-freshly-created-database,-and/or-an-empty-table-with-no-rows,-contribute-to-my-storage?"
    },
    {
      "level": "h2",
      "text": "Limits",
      "id": "limits"
    },
    {
      "level": "h3",
      "text": "How much work can a D1 database do?",
      "id": "how-much-work-can-a-d1-database-do?"
    },
    {
      "level": "h3",
      "text": "How many simultaneous connections can a Worker open to D1?",
      "id": "how-many-simultaneous-connections-can-a-worker-open-to-d1?"
    },
    {
      "level": "h2",
      "text": "Types of generated columns",
      "id": "types-of-generated-columns"
    },
    {
      "level": "h2",
      "text": "Define a generated column",
      "id": "define-a-generated-column"
    },
    {
      "level": "h2",
      "text": "Add a generated column to an existing table",
      "id": "add-a-generated-column-to-an-existing-table"
    },
    {
      "level": "h2",
      "text": "Examples",
      "id": "examples"
    },
    {
      "level": "h2",
      "text": "Additional considerations",
      "id": "additional-considerations"
    },
    {
      "level": "h2",
      "text": "Features",
      "id": "features"
    },
    {
      "level": "h2",
      "text": "Wrangler customizations",
      "id": "wrangler-customizations"
    },
    {
      "level": "h2",
      "text": "Foreign key constraints",
      "id": "foreign-key-constraints"
    },
    {
      "level": "h2",
      "text": "Bookmarks",
      "id": "bookmarks"
    },
    {
      "level": "h2",
      "text": "Timestamps",
      "id": "timestamps"
    },
    {
      "level": "h2",
      "text": "Requirements",
      "id": "requirements"
    },
    {
      "level": "h2",
      "text": "Retrieve a bookmark",
      "id": "retrieve-a-bookmark"
    },
    {
      "level": "h2",
      "text": "Restore a database",
      "id": "restore-a-database"
    },
    {
      "level": "h2",
      "text": "Undo a restore",
      "id": "undo-a-restore"
    },
    {
      "level": "h2",
      "text": "Export D1 into R2 using Workflows",
      "id": "export-d1-into-r2-using-workflows"
    },
    {
      "level": "h2",
      "text": "Notes",
      "id": "notes"
    },
    {
      "level": "h2",
      "text": "Defer foreign key constraints",
      "id": "defer-foreign-key-constraints"
    },
    {
      "level": "h2",
      "text": "Define a foreign key relationship",
      "id": "define-a-foreign-key-relationship"
    },
    {
      "level": "h2",
      "text": "Foreign key actions",
      "id": "foreign-key-actions"
    },
    {
      "level": "h2",
      "text": "Next Steps",
      "id": "next-steps"
    },
    {
      "level": "h2",
      "text": "Types",
      "id": "types"
    },
    {
      "level": "h2",
      "text": "Supported functions",
      "id": "supported-functions"
    },
    {
      "level": "h2",
      "text": "Error Handling",
      "id": "error-handling"
    },
    {
      "level": "h2",
      "text": "Generated columns",
      "id": "generated-columns"
    },
    {
      "level": "h2",
      "text": "Example usage",
      "id": "example-usage"
    },
    {
      "level": "h3",
      "text": "Extract values",
      "id": "extract-values"
    },
    {
      "level": "h3",
      "text": "Get the length of an array",
      "id": "get-the-length-of-an-array"
    },
    {
      "level": "h3",
      "text": "Insert a value into an existing object",
      "id": "insert-a-value-into-an-existing-object"
    },
    {
      "level": "h3",
      "text": "Expand arrays for IN queries",
      "id": "expand-arrays-for-in-queries"
    },
    {
      "level": "h2",
      "text": "Supported SQLite extensions",
      "id": "supported-sqlite-extensions"
    },
    {
      "level": "h2",
      "text": "Compatible PRAGMA statements",
      "id": "compatible-pragma-statements"
    },
    {
      "level": "h3",
      "text": "`PRAGMA table_list`",
      "id": "`pragma-table_list`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA table_info(\"TABLE_NAME\")`",
      "id": "`pragma-table_info(\"table_name\")`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA table_xinfo(\"TABLE_NAME\")`",
      "id": "`pragma-table_xinfo(\"table_name\")`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA index_list(\"TABLE_NAME\")`",
      "id": "`pragma-index_list(\"table_name\")`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA index_info(INDEX_NAME)`",
      "id": "`pragma-index_info(index_name)`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA index_xinfo(\"INDEX_NAME\")`",
      "id": "`pragma-index_xinfo(\"index_name\")`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA quick_check`",
      "id": "`pragma-quick_check`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA foreign_key_check`",
      "id": "`pragma-foreign_key_check`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA foreign_key_list(\"TABLE_NAME\")`",
      "id": "`pragma-foreign_key_list(\"table_name\")`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA case_sensitive_like = (on|off)`",
      "id": "`pragma-case_sensitive_like-=-(on|off)`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA ignore_check_constraints = (on|off)`",
      "id": "`pragma-ignore_check_constraints-=-(on|off)`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA legacy_alter_table = (on|off)`",
      "id": "`pragma-legacy_alter_table-=-(on|off)`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA recursive_triggers = (on|off)`",
      "id": "`pragma-recursive_triggers-=-(on|off)`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA reverse_unordered_selects = (on|off)`",
      "id": "`pragma-reverse_unordered_selects-=-(on|off)`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA foreign_keys = (on|off)`",
      "id": "`pragma-foreign_keys-=-(on|off)`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA defer_foreign_keys = (on|off)`",
      "id": "`pragma-defer_foreign_keys-=-(on|off)`"
    },
    {
      "level": "h3",
      "text": "`PRAGMA optimize`",
      "id": "`pragma-optimize`"
    },
    {
      "level": "h2",
      "text": "Query `sqlite_master`",
      "id": "query-`sqlite_master`"
    },
    {
      "level": "h2",
      "text": "Search with LIKE",
      "id": "search-with-like"
    },
    {
      "level": "h2",
      "text": "Related resources",
      "id": "related-resources"
    },
    {
      "level": "h2",
      "text": "Prerequisites",
      "id": "prerequisites"
    },
    {
      "level": "h2",
      "text": "Video Tutorial",
      "id": "video-tutorial"
    },
    {
      "level": "h2",
      "text": "1. Install Hono",
      "id": "1.-install-hono"
    },
    {
      "level": "h2",
      "text": "2. Initialize your Hono application",
      "id": "2.-initialize-your-hono-application"
    },
    {
      "level": "h2",
      "text": "3. Create a database",
      "id": "3.-create-a-database"
    },
    {
      "level": "h2",
      "text": "4. Interact with D1",
      "id": "4.-interact-with-d1"
    },
    {
      "level": "h2",
      "text": "5. Execute SQL",
      "id": "5.-execute-sql"
    },
    {
      "level": "h2",
      "text": "6. Insert data",
      "id": "6.-insert-data"
    },
    {
      "level": "h2",
      "text": "7. Deploy your Hono application",
      "id": "7.-deploy-your-hono-application"
    }
  ],
  "url": "llms-txt#current-deployment-id:-80b72e19-da82-4465-83a2-c12fb11ccc72",
  "links": []
}